{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEAZYXvZU_XG"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:15:58.926817Z",
     "iopub.status.busy": "2024-03-09T12:15:58.926250Z",
     "iopub.status.idle": "2024-03-09T12:16:01.339341Z",
     "shell.execute_reply": "2024-03-09T12:16:01.338230Z"
    },
    "id": "zN4yVFK5-0Bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sever\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sever\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sever\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 206] Der Dateiname oder die Erweiterung ist zu lang: 'C:\\\\Users\\\\Sever\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\tensorflow_model_optimization\\\\python\\\\core\\\\api\\\\quantization\\\\keras\\\\experimental\\\\default_n_bit\\\\default_n_bit_transforms'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sever\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\Sever\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:01.343641Z",
     "iopub.status.busy": "2024-03-09T12:16:01.343326Z",
     "iopub.status.idle": "2024-03-09T12:16:04.249886Z",
     "shell.execute_reply": "2024-03-09T12:16:04.249009Z"
    },
    "id": "yJwIonXEVJo6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psViY5PRDurp"
   },
   "source": [
    "## Baseline implementation from TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:04.254609Z",
     "iopub.status.busy": "2024-03-09T12:16:04.253856Z",
     "iopub.status.idle": "2024-03-09T12:16:46.928264Z",
     "shell.execute_reply": "2024-03-09T12:16:46.927391Z"
    },
    "id": "pbY-KGMPvbW9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2991 - accuracy: 0.9168 - val_loss: 0.1287 - val_accuracy: 0.9658\n",
      "Epoch 2/4\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.1321 - accuracy: 0.9625 - val_loss: 0.0919 - val_accuracy: 0.9765\n",
      "Epoch 3/4\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0955 - accuracy: 0.9722 - val_loss: 0.0821 - val_accuracy: 0.9762\n",
      "Epoch 4/4\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0763 - accuracy: 0.9781 - val_loss: 0.0742 - val_accuracy: 0.9808\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
      " _3 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 26, 26, 12)       230       \n",
      " 3 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 13, 13, 12)       1         \n",
      " ling2d_3 (PruneLowMagnitude                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 2028)             1         \n",
      " _3 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_3  (None, 10)               40572     \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,805\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 20,395\n",
      "_________________________________________________________________\n",
      "Pruned test accuracy: 0.9778000116348267\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "conv2d_3, Conv2D, 120\n",
      "dense_3, Dense, 20290\n",
      "Total non-zero weights in the model: 20410\n",
      "Non-zero weights in the model: None\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=4,\n",
    "  validation_split=0.1,\n",
    ")\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()\n",
    "\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "print('Non-zero weights in the model:', non_zero_weights_summary(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1qKfXNqo9hk"
   },
   "source": [
    "## Implementation of first simple pruning techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def non_zero_weights_summary(model):\n",
    "    \"\"\"\n",
    "    Prints a summary of the model showing only non-zero weights for each layer.\n",
    "\n",
    "    Args:\n",
    "    model (tf.keras.Model): The model to summarize.\n",
    "    \"\"\"\n",
    "    print(\"Layer Name, Layer Type, Non-zero Weights\")\n",
    "    total_non_zero = 0\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'weights') and len(layer.weights) > 0:\n",
    "            non_zero_count = 0\n",
    "            for weight in layer.get_weights():\n",
    "                non_zero_count += np.count_nonzero(weight)\n",
    "            print(f\"{layer.name}, {type(layer).__name__}, {non_zero_count}\")\n",
    "            total_non_zero += non_zero_count\n",
    "    print(f\"Total non-zero weights in the model: {total_non_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def prune_weights_magnitude(original_model, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Creates a new model by cloning the architecture of the original model, \n",
    "    copying its weights, and then pruning the weights by setting weights with an \n",
    "    absolute value less than the threshold to zero.\n",
    "\n",
    "    Args:\n",
    "    original_model (tf.keras.Model): The trained Keras model to be pruned.\n",
    "    threshold (float): The magnitude threshold below which weights will be set to zero.\n",
    "\n",
    "    Returns:\n",
    "    tf.keras.Model: A new model with pruned weights.\n",
    "    \"\"\"\n",
    "    # Clone the model architecture\n",
    "    new_model = tf.keras.models.clone_model(original_model)\n",
    "    \n",
    "    # Compile the new model with dummy parameters (these can be set as needed later)\n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Copy the weights from the original model\n",
    "    new_model.set_weights(original_model.get_weights())\n",
    "    \n",
    "    # Prune the weights of the new model\n",
    "    for layer in new_model.layers:\n",
    "        if hasattr(layer, 'weights'):  # Check if the layer has weights\n",
    "            new_weights = []\n",
    "            for w in layer.get_weights():\n",
    "                # Apply the pruning condition\n",
    "                pruned_weights = tf.where(tf.abs(w) < threshold, tf.zeros_like(w), w)\n",
    "                new_weights.append(pruned_weights)\n",
    "            layer.set_weights(new_weights)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def prune_model_based_on_gradients(original_model, accumulators, threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Creates a new model by cloning the original model, computes gradients based on provided\n",
    "    data and labels, and prunes the weights based on these gradients where the absolute \n",
    "    gradient value is below the specified threshold.\n",
    "\n",
    "    Args:\n",
    "    original_model (tf.keras.Model): The trained model to be cloned and pruned.\n",
    "    data (np.array): Input data used to compute gradients.\n",
    "    labels (np.array): Corresponding labels for the data.\n",
    "    threshold (float): Gradient magnitude threshold below which weights will be set to zero.\n",
    "\n",
    "    Returns:\n",
    "    tf.keras.Model: A new model with pruned weights based on gradient magnitudes.\n",
    "    \"\"\"\n",
    "    # Clone the model architecture\n",
    "    new_model = tf.keras.models.clone_model(original_model)\n",
    "    \n",
    "    # Compile the new model with dummy parameters (these can be set as needed later)\n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Copy the weights from the original model\n",
    "    new_model.set_weights(original_model.get_weights())\n",
    "    # flat_accumulators = tf.concat([tf.reshape(acc, [-1]) for acc in accumulators], axis=0)\n",
    "    # gradients = tf.nn.softmax(flat_accumulators).numpy() \n",
    "    gradients = accumulators\n",
    "    \n",
    "    # Prune the weights based on gradients\n",
    "    idx = 0  # This should index the gradients list\n",
    "    for layer in new_model.layers:\n",
    "        if layer.trainable_weights:\n",
    "            new_weights = []\n",
    "            for w, g in zip(layer.weights, gradients[idx:idx + len(layer.weights)]):\n",
    "                if g is not None:\n",
    "                    mask = tf.abs(g) > threshold\n",
    "                    new_weight = w * tf.cast(mask, dtype=w.dtype)\n",
    "                    new_weights.append(new_weight.numpy())\n",
    "                else:\n",
    "                    new_weights.append(w.numpy())\n",
    "            layer.set_weights(new_weights)\n",
    "            idx += len(layer.weights)\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def train_with_gradient_accumulation(model, train_data, epochs):\n",
    "    # Define the loss function\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    \n",
    "    # Initialize gradient accumulators as tf.Variables\n",
    "    accumulators = [tf.Variable(tf.zeros_like(w), trainable=False) for w in model.trainable_weights]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            \n",
    "            # Debugging: Check max gradient\n",
    "            max_grad = max([tf.reduce_max(tf.abs(g)).numpy() for g in grads if g is not None])\n",
    "            print(f\"Max gradient at step {step}: {max_grad}\")\n",
    "            \n",
    "            # Update the gradient accumulators\n",
    "            for acc, g in zip(accumulators, grads):\n",
    "                acc.assign_add(tf.abs(g))\n",
    "                \n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(\"Training loss (for one batch) at step {}: {:.4f}\".format(step, float(loss_value)))\n",
    "\n",
    "    return accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal training with SGD optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def train_model(model, train_data, epochs, learning_rate=0.01):\n",
    "    # Define the loss function\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(\"Training loss (for one batch) at step {}: {:.4f}\".format(step, float(loss_value)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with SGD + pruning in each epoch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def train_model_pruning(model, train_data, epochs, learning_rate=0.01, threshold = 0.05):\n",
    "    # Define the loss function\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        if epoch != 0:\n",
    "            model = prune_weights_magnitude(model, threshold) \n",
    "            pruned_weights_mask = [tf.not_equal(w, 0.0) for w in model.get_weights()]\n",
    "            \n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "               \n",
    "            if step % 100 == 0:\n",
    "                print(\"Training loss (for one batch) at step {}: {:.4f}\".format(step, float(loss_value)))\n",
    "                \n",
    "        if epoch != 0:\n",
    "            # Apply mask to keep pruned weights at zero\n",
    "            for w, mask in zip(model.trainable_weights, pruned_weights_mask):\n",
    "                w.assign(w * tf.cast(mask, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(model, train_data, pruned_weights_mask, epochs, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Retrains a given model using the specified training data and parameters,\n",
    "    while applying a mask to keep pruned weights at zero.\n",
    "\n",
    "    Args:\n",
    "    model (tf.keras.Model): The model to be retrained.\n",
    "    train_data (tf.data.Dataset): Dataset to use for training.\n",
    "    pruned_weights_mask (list of np.array): Mask where True indicates weight should be kept at zero.\n",
    "    epochs (int): Number of epochs to train for.\n",
    "    learning_rate (float): Learning rate for the optimizer.\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "        for x_batch_train, y_batch_train in train_data:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            \n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            # Apply mask to keep pruned weights at zero\n",
    "            for w, mask in zip(model.trainable_weights, pruned_weights_mask):\n",
    "                w.assign(w * tf.cast(mask, tf.float32))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network as usual\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)  # Example batch size\n",
    "\n",
    "# Define the model architecture.\n",
    "model_test = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "epochs = 4\n",
    "train_model(model_test, train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9330999851226807\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "conv2d, Conv2D, 120\n",
      "dense, Dense, 20290\n",
      "Total non-zero weights in the model: 20410\n",
      "Non-zero weights in the model: None\n"
     ]
    }
   ],
   "source": [
    "model_test.compile(optimizer='sgd',  # Use the same optimizer as used in training\n",
    "                loss='sparse_categorical_crossentropy',  # Use the appropriate loss function\n",
    "                metrics=['accuracy']) \n",
    "_, baseline_model_test_accuracy = model_test.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_test_accuracy)\n",
    "print('Non-zero weights in the model:', non_zero_weights_summary(model_test))\n",
    "\n",
    "pruned_keras_file = r'C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\base.h5'\n",
    "keras.models.save_model(model_test, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network with Pruning\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)  # Example batch size\n",
    "\n",
    "# Define the model architecture.\n",
    "model_test_ = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "epochs = 4\n",
    "train_model_pruning(model_test_, train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9106000065803528\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "conv2d, Conv2D, 120\n",
      "dense, Dense, 20290\n",
      "Total non-zero weights in the model: 20410\n",
      "Non-zero weights in the model: None\n"
     ]
    }
   ],
   "source": [
    "model_test_.compile(optimizer='sgd',  # Use the same optimizer as used in training\n",
    "                loss='sparse_categorical_crossentropy',  # Use the appropriate loss function\n",
    "                metrics=['accuracy']) \n",
    "_, baseline_model_test_accuracy = model_test_.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_test_accuracy)\n",
    "print('Non-zero weights in the model:', non_zero_weights_summary(model_test))\n",
    "\n",
    "pruned_keras_file = r'C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\base.h5'\n",
    "keras.models.save_model(model_test, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prune based on magnitude\n",
    "model_mag = prune_weights_magnitude(model_test, threshold=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x219a0afb820>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_weights_mask = [tf.not_equal(w, 0.0) for w in model_mag.get_weights()]\n",
    "retrain_model(model_mag, train_dataset, pruned_weights_mask, epochs=2, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpf9w8y6wo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpf9w8y6wo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite model accuracy: 94.61\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_mag)\n",
    "tflite_mag = converter.convert()\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_mag)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "test_images = test_images.astype(\"float32\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate the TFLite model on all test data\n",
    "def evaluate_model(interpreter, test_images, test_labels):\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "    \n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for test_image in test_images:\n",
    "        # Preprocessing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Postprocessing: remove batch dimension and find the predicted digit\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy\n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_digits)):\n",
    "        if prediction_digits[index] == test_labels[index]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 100 / len(prediction_digits)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate_model(interpreter, test_images, test_labels)\n",
    "print('TensorFlow Lite model accuracy:', accuracy)\n",
    "\n",
    "with open('models/mag.tflite', 'wb') as f:\n",
    "    f.write(tflite_mag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9460999965667725\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "conv2d, Conv2D, 88\n",
      "dense, Dense, 812\n",
      "Total non-zero weights in the model: 900\n",
      "Non-zero weights in the model: None\n"
     ]
    }
   ],
   "source": [
    "model_mag.compile(optimizer='sgd',  # Use the same optimizer as used in training\n",
    "                loss='sparse_categorical_crossentropy',  # Use the appropriate loss function\n",
    "                metrics=['accuracy']) \n",
    "_, baseline_model_mag_accuracy = model_mag.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_mag_accuracy)\n",
    "print('Non-zero weights in the model:', non_zero_weights_summary(model_mag))\n",
    "pruned_keras_file = r'C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\mag.h5'\n",
    "keras.models.save_model(model_mag, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'mag'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_mag, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Max gradient at step 0: 0.10940287262201309\n",
      "Training loss (for one batch) at step 0: 2.2857\n",
      "Max gradient at step 1: 0.14899222552776337\n",
      "Max gradient at step 2: 0.14264975488185883\n",
      "Max gradient at step 3: 0.1266109198331833\n",
      "Max gradient at step 4: 0.1634834259748459\n",
      "Max gradient at step 5: 0.10963931679725647\n",
      "Max gradient at step 6: 0.1511652171611786\n",
      "Max gradient at step 7: 0.09271976351737976\n",
      "Max gradient at step 8: 0.11512503772974014\n",
      "Max gradient at step 9: 0.12011048197746277\n",
      "Max gradient at step 10: 0.11539633572101593\n",
      "Max gradient at step 11: 0.138559028506279\n",
      "Max gradient at step 12: 0.07779813557863235\n",
      "Max gradient at step 13: 0.1136104092001915\n",
      "Max gradient at step 14: 0.08423705399036407\n",
      "Max gradient at step 15: 0.07488560676574707\n",
      "Max gradient at step 16: 0.18862676620483398\n",
      "Max gradient at step 17: 0.0919169932603836\n",
      "Max gradient at step 18: 0.07644332200288773\n",
      "Max gradient at step 19: 0.14812125265598297\n",
      "Max gradient at step 20: 0.07693130522966385\n",
      "Max gradient at step 21: 0.13725706934928894\n",
      "Max gradient at step 22: 0.11530297249555588\n",
      "Max gradient at step 23: 0.11776522547006607\n",
      "Max gradient at step 24: 0.09588363021612167\n",
      "Max gradient at step 25: 0.12110521644353867\n",
      "Max gradient at step 26: 0.12238891422748566\n",
      "Max gradient at step 27: 0.08310452848672867\n",
      "Max gradient at step 28: 0.10026846081018448\n",
      "Max gradient at step 29: 0.10903948545455933\n",
      "Max gradient at step 30: 0.09421545267105103\n",
      "Max gradient at step 31: 0.08359450846910477\n",
      "Max gradient at step 32: 0.12069541215896606\n",
      "Max gradient at step 33: 0.11610008031129837\n",
      "Max gradient at step 34: 0.17957499623298645\n",
      "Max gradient at step 35: 0.08968780189752579\n",
      "Max gradient at step 36: 0.1848006695508957\n",
      "Max gradient at step 37: 0.10021842271089554\n",
      "Max gradient at step 38: 0.11002699285745621\n",
      "Max gradient at step 39: 0.12397695332765579\n",
      "Max gradient at step 40: 0.10487405955791473\n",
      "Max gradient at step 41: 0.12675559520721436\n",
      "Max gradient at step 42: 0.10018274188041687\n",
      "Max gradient at step 43: 0.1332753598690033\n",
      "Max gradient at step 44: 0.14718538522720337\n",
      "Max gradient at step 45: 0.11101687699556351\n",
      "Max gradient at step 46: 0.1590873897075653\n",
      "Max gradient at step 47: 0.11431264877319336\n",
      "Max gradient at step 48: 0.11588077247142792\n",
      "Max gradient at step 49: 0.21588784456253052\n",
      "Max gradient at step 50: 0.13475161790847778\n",
      "Max gradient at step 51: 0.13234642148017883\n",
      "Max gradient at step 52: 0.18041060864925385\n",
      "Max gradient at step 53: 0.10978291928768158\n",
      "Max gradient at step 54: 0.10732224583625793\n",
      "Max gradient at step 55: 0.1413702368736267\n",
      "Max gradient at step 56: 0.11497940868139267\n",
      "Max gradient at step 57: 0.1276777684688568\n",
      "Max gradient at step 58: 0.17947566509246826\n",
      "Max gradient at step 59: 0.12587052583694458\n",
      "Max gradient at step 60: 0.15484508872032166\n",
      "Max gradient at step 61: 0.1771172136068344\n",
      "Max gradient at step 62: 0.14401453733444214\n",
      "Max gradient at step 63: 0.13333843648433685\n",
      "Max gradient at step 64: 0.09940812736749649\n",
      "Max gradient at step 65: 0.12975117564201355\n",
      "Max gradient at step 66: 0.12638022005558014\n",
      "Max gradient at step 67: 0.20466969907283783\n",
      "Max gradient at step 68: 0.11551383137702942\n",
      "Max gradient at step 69: 0.13654690980911255\n",
      "Max gradient at step 70: 0.15454643964767456\n",
      "Max gradient at step 71: 0.16478553414344788\n",
      "Max gradient at step 72: 0.1009758934378624\n",
      "Max gradient at step 73: 0.14228889346122742\n",
      "Max gradient at step 74: 0.14709210395812988\n",
      "Max gradient at step 75: 0.17350097000598907\n",
      "Max gradient at step 76: 0.12214335799217224\n",
      "Max gradient at step 77: 0.13527989387512207\n",
      "Max gradient at step 78: 0.13671110570430756\n",
      "Max gradient at step 79: 0.14816808700561523\n",
      "Max gradient at step 80: 0.12624837458133698\n",
      "Max gradient at step 81: 0.13732361793518066\n",
      "Max gradient at step 82: 0.14757804572582245\n",
      "Max gradient at step 83: 0.13950617611408234\n",
      "Max gradient at step 84: 0.1396178901195526\n",
      "Max gradient at step 85: 0.16453146934509277\n",
      "Max gradient at step 86: 0.21867085993289948\n",
      "Max gradient at step 87: 0.17812302708625793\n",
      "Max gradient at step 88: 0.16963277757167816\n",
      "Max gradient at step 89: 0.20024406909942627\n",
      "Max gradient at step 90: 0.175044983625412\n",
      "Max gradient at step 91: 0.1679115742444992\n",
      "Max gradient at step 92: 0.1685405969619751\n",
      "Max gradient at step 93: 0.14430448412895203\n",
      "Max gradient at step 94: 0.17318440973758698\n",
      "Max gradient at step 95: 0.2115388959646225\n",
      "Max gradient at step 96: 0.17296504974365234\n",
      "Max gradient at step 97: 0.16278406977653503\n",
      "Max gradient at step 98: 0.18801136314868927\n",
      "Max gradient at step 99: 0.1522151231765747\n",
      "Max gradient at step 100: 0.18751493096351624\n",
      "Training loss (for one batch) at step 100: 1.8240\n",
      "Max gradient at step 101: 0.17439226806163788\n",
      "Max gradient at step 102: 0.20854800939559937\n",
      "Max gradient at step 103: 0.17162032425403595\n",
      "Max gradient at step 104: 0.1671549528837204\n",
      "Max gradient at step 105: 0.14676111936569214\n",
      "Max gradient at step 106: 0.16867418587207794\n",
      "Max gradient at step 107: 0.16326478123664856\n",
      "Max gradient at step 108: 0.20677734911441803\n",
      "Max gradient at step 109: 0.2142040729522705\n",
      "Max gradient at step 110: 0.18810930848121643\n",
      "Max gradient at step 111: 0.19415229558944702\n",
      "Max gradient at step 112: 0.1612144112586975\n",
      "Max gradient at step 113: 0.19922183454036713\n",
      "Max gradient at step 114: 0.17262226343154907\n",
      "Max gradient at step 115: 0.17089858651161194\n",
      "Max gradient at step 116: 0.1642715334892273\n",
      "Max gradient at step 117: 0.19490370154380798\n",
      "Max gradient at step 118: 0.18567919731140137\n",
      "Max gradient at step 119: 0.16324996948242188\n",
      "Max gradient at step 120: 0.18369244039058685\n",
      "Max gradient at step 121: 0.13543276488780975\n",
      "Max gradient at step 122: 0.20793858170509338\n",
      "Max gradient at step 123: 0.16462677717208862\n",
      "Max gradient at step 124: 0.18529635667800903\n",
      "Max gradient at step 125: 0.19198793172836304\n",
      "Max gradient at step 126: 0.1909913271665573\n",
      "Max gradient at step 127: 0.2886345386505127\n",
      "Max gradient at step 128: 0.1910124272108078\n",
      "Max gradient at step 129: 0.22539697587490082\n",
      "Max gradient at step 130: 0.19844022393226624\n",
      "Max gradient at step 131: 0.15865272283554077\n",
      "Max gradient at step 132: 0.1996154487133026\n",
      "Max gradient at step 133: 0.17608040571212769\n",
      "Max gradient at step 134: 0.2059241235256195\n",
      "Max gradient at step 135: 0.15077361464500427\n",
      "Max gradient at step 136: 0.202327698469162\n",
      "Max gradient at step 137: 0.17990586161613464\n",
      "Max gradient at step 138: 0.1799958199262619\n",
      "Max gradient at step 139: 0.1826343685388565\n",
      "Max gradient at step 140: 0.18533222377300262\n",
      "Max gradient at step 141: 0.18676885962486267\n",
      "Max gradient at step 142: 0.2053380310535431\n",
      "Max gradient at step 143: 0.20531868934631348\n",
      "Max gradient at step 144: 0.18539074063301086\n",
      "Max gradient at step 145: 0.17878562211990356\n",
      "Max gradient at step 146: 0.1520397812128067\n",
      "Max gradient at step 147: 0.1494630128145218\n",
      "Max gradient at step 148: 0.14589489996433258\n",
      "Max gradient at step 149: 0.20985905826091766\n",
      "Max gradient at step 150: 0.1355723738670349\n",
      "Max gradient at step 151: 0.19675281643867493\n",
      "Max gradient at step 152: 0.18676406145095825\n",
      "Max gradient at step 153: 0.19255568087100983\n",
      "Max gradient at step 154: 0.14695093035697937\n",
      "Max gradient at step 155: 0.1728624701499939\n",
      "Max gradient at step 156: 0.2234635055065155\n",
      "Max gradient at step 157: 0.22030788660049438\n",
      "Max gradient at step 158: 0.18202099204063416\n",
      "Max gradient at step 159: 0.2700504958629608\n",
      "Max gradient at step 160: 0.1766674518585205\n",
      "Max gradient at step 161: 0.17081612348556519\n",
      "Max gradient at step 162: 0.16221998631954193\n",
      "Max gradient at step 163: 0.16262878477573395\n",
      "Max gradient at step 164: 0.16098585724830627\n",
      "Max gradient at step 165: 0.20120073854923248\n",
      "Max gradient at step 166: 0.18978869915008545\n",
      "Max gradient at step 167: 0.17333655059337616\n",
      "Max gradient at step 168: 0.16632673144340515\n",
      "Max gradient at step 169: 0.23650671541690826\n",
      "Max gradient at step 170: 0.1381170153617859\n",
      "Max gradient at step 171: 0.17192403972148895\n",
      "Max gradient at step 172: 0.15908437967300415\n",
      "Max gradient at step 173: 0.17341837286949158\n",
      "Max gradient at step 174: 0.27861112356185913\n",
      "Max gradient at step 175: 0.1563122719526291\n",
      "Max gradient at step 176: 0.17865844070911407\n",
      "Max gradient at step 177: 0.1433408409357071\n",
      "Max gradient at step 178: 0.16808363795280457\n",
      "Max gradient at step 179: 0.13552150130271912\n",
      "Max gradient at step 180: 0.14615073800086975\n",
      "Max gradient at step 181: 0.1969158798456192\n",
      "Max gradient at step 182: 0.18291443586349487\n",
      "Max gradient at step 183: 0.15598192811012268\n",
      "Max gradient at step 184: 0.2244505137205124\n",
      "Max gradient at step 185: 0.15691867470741272\n",
      "Max gradient at step 186: 0.14140278100967407\n",
      "Max gradient at step 187: 0.1433010846376419\n",
      "Max gradient at step 188: 0.17385341227054596\n",
      "Max gradient at step 189: 0.20916490256786346\n",
      "Max gradient at step 190: 0.14614972472190857\n",
      "Max gradient at step 191: 0.2514078915119171\n",
      "Max gradient at step 192: 0.11355256289243698\n",
      "Max gradient at step 193: 0.16064758598804474\n",
      "Max gradient at step 194: 0.157842755317688\n",
      "Max gradient at step 195: 0.1388419270515442\n",
      "Max gradient at step 196: 0.1824309229850769\n",
      "Max gradient at step 197: 0.1563960611820221\n",
      "Max gradient at step 198: 0.1992371678352356\n",
      "Max gradient at step 199: 0.21068911254405975\n",
      "Max gradient at step 200: 0.1545933187007904\n",
      "Training loss (for one batch) at step 200: 0.7710\n",
      "Max gradient at step 201: 0.21672001481056213\n",
      "Max gradient at step 202: 0.21364238858222961\n",
      "Max gradient at step 203: 0.1617741733789444\n",
      "Max gradient at step 204: 0.16198550164699554\n",
      "Max gradient at step 205: 0.1582920402288437\n",
      "Max gradient at step 206: 0.13928094506263733\n",
      "Max gradient at step 207: 0.15183310210704803\n",
      "Max gradient at step 208: 0.2119157761335373\n",
      "Max gradient at step 209: 0.16490669548511505\n",
      "Max gradient at step 210: 0.11316367983818054\n",
      "Max gradient at step 211: 0.13158614933490753\n",
      "Max gradient at step 212: 0.12750735878944397\n",
      "Max gradient at step 213: 0.16978846490383148\n",
      "Max gradient at step 214: 0.265025794506073\n",
      "Max gradient at step 215: 0.1545967310667038\n",
      "Max gradient at step 216: 0.13655756413936615\n",
      "Max gradient at step 217: 0.12068389356136322\n",
      "Max gradient at step 218: 0.1686951369047165\n",
      "Max gradient at step 219: 0.24967709183692932\n",
      "Max gradient at step 220: 0.2503568232059479\n",
      "Max gradient at step 221: 0.17578424513339996\n",
      "Max gradient at step 222: 0.16326197981834412\n",
      "Max gradient at step 223: 0.20730742812156677\n",
      "Max gradient at step 224: 0.1931786835193634\n",
      "Max gradient at step 225: 0.230892613530159\n",
      "Max gradient at step 226: 0.1221371591091156\n",
      "Max gradient at step 227: 0.12342716008424759\n",
      "Max gradient at step 228: 0.21722421050071716\n",
      "Max gradient at step 229: 0.1365991234779358\n",
      "Max gradient at step 230: 0.17769140005111694\n",
      "Max gradient at step 231: 0.12815767526626587\n",
      "Max gradient at step 232: 0.14496436715126038\n",
      "Max gradient at step 233: 0.136447474360466\n",
      "Max gradient at step 234: 0.17213520407676697\n",
      "Max gradient at step 235: 0.16926683485507965\n",
      "Max gradient at step 236: 0.11066725850105286\n",
      "Max gradient at step 237: 0.10103059560060501\n",
      "Max gradient at step 238: 0.15277786552906036\n",
      "Max gradient at step 239: 0.20959025621414185\n",
      "Max gradient at step 240: 0.16409644484519958\n",
      "Max gradient at step 241: 0.15673251450061798\n",
      "Max gradient at step 242: 0.15727484226226807\n",
      "Max gradient at step 243: 0.20093540847301483\n",
      "Max gradient at step 244: 0.16656818985939026\n",
      "Max gradient at step 245: 0.15539731085300446\n",
      "Max gradient at step 246: 0.16442841291427612\n",
      "Max gradient at step 247: 0.15249250829219818\n",
      "Max gradient at step 248: 0.21473141014575958\n",
      "Max gradient at step 249: 0.18238694965839386\n",
      "Max gradient at step 250: 0.11189448088407516\n",
      "Max gradient at step 251: 0.15078860521316528\n",
      "Max gradient at step 252: 0.17763303220272064\n",
      "Max gradient at step 253: 0.12891879677772522\n",
      "Max gradient at step 254: 0.1812666803598404\n",
      "Max gradient at step 255: 0.102631576359272\n",
      "Max gradient at step 256: 0.10341836512088776\n",
      "Max gradient at step 257: 0.22235074639320374\n",
      "Max gradient at step 258: 0.1746918112039566\n",
      "Max gradient at step 259: 0.1410970538854599\n",
      "Max gradient at step 260: 0.13670767843723297\n",
      "Max gradient at step 261: 0.1348648965358734\n",
      "Max gradient at step 262: 0.18075312674045563\n",
      "Max gradient at step 263: 0.14732319116592407\n",
      "Max gradient at step 264: 0.1141740158200264\n",
      "Max gradient at step 265: 0.09654740244150162\n",
      "Max gradient at step 266: 0.10722175240516663\n",
      "Max gradient at step 267: 0.13887575268745422\n",
      "Max gradient at step 268: 0.16016240417957306\n",
      "Max gradient at step 269: 0.12290849536657333\n",
      "Max gradient at step 270: 0.27610889077186584\n",
      "Max gradient at step 271: 0.16026726365089417\n",
      "Max gradient at step 272: 0.10679922252893448\n",
      "Max gradient at step 273: 0.17840281128883362\n",
      "Max gradient at step 274: 0.12835067510604858\n",
      "Max gradient at step 275: 0.12933051586151123\n",
      "Max gradient at step 276: 0.10785422474145889\n",
      "Max gradient at step 277: 0.16321824491024017\n",
      "Max gradient at step 278: 0.20496152341365814\n",
      "Max gradient at step 279: 0.11166708171367645\n",
      "Max gradient at step 280: 0.11680644005537033\n",
      "Max gradient at step 281: 0.12085305154323578\n",
      "Max gradient at step 282: 0.22304290533065796\n",
      "Max gradient at step 283: 0.14532935619354248\n",
      "Max gradient at step 284: 0.16213056445121765\n",
      "Max gradient at step 285: 0.15859992802143097\n",
      "Max gradient at step 286: 0.09056848287582397\n",
      "Max gradient at step 287: 0.13668617606163025\n",
      "Max gradient at step 288: 0.14303310215473175\n",
      "Max gradient at step 289: 0.17200596630573273\n",
      "Max gradient at step 290: 0.1280386596918106\n",
      "Max gradient at step 291: 0.13140149414539337\n",
      "Max gradient at step 292: 0.09587837010622025\n",
      "Max gradient at step 293: 0.3074232041835785\n",
      "Max gradient at step 294: 0.1528116911649704\n",
      "Max gradient at step 295: 0.10506604611873627\n",
      "Max gradient at step 296: 0.11398548632860184\n",
      "Max gradient at step 297: 0.14319339394569397\n",
      "Max gradient at step 298: 0.14786936342716217\n",
      "Max gradient at step 299: 0.162696972489357\n",
      "Max gradient at step 300: 0.12611918151378632\n",
      "Training loss (for one batch) at step 300: 0.5998\n",
      "Max gradient at step 301: 0.14970725774765015\n",
      "Max gradient at step 302: 0.16231583058834076\n",
      "Max gradient at step 303: 0.17998254299163818\n",
      "Max gradient at step 304: 0.15742208063602448\n",
      "Max gradient at step 305: 0.11098502576351166\n",
      "Max gradient at step 306: 0.15695323050022125\n",
      "Max gradient at step 307: 0.12463202327489853\n",
      "Max gradient at step 308: 0.10402355343103409\n",
      "Max gradient at step 309: 0.08952321857213974\n",
      "Max gradient at step 310: 0.11592335999011993\n",
      "Max gradient at step 311: 0.1402459591627121\n",
      "Max gradient at step 312: 0.13635939359664917\n",
      "Max gradient at step 313: 0.1341027319431305\n",
      "Max gradient at step 314: 0.129736065864563\n",
      "Max gradient at step 315: 0.2982970178127289\n",
      "Max gradient at step 316: 0.145224928855896\n",
      "Max gradient at step 317: 0.20969463884830475\n",
      "Max gradient at step 318: 0.13531407713890076\n",
      "Max gradient at step 319: 0.12001533061265945\n",
      "Max gradient at step 320: 0.17647024989128113\n",
      "Max gradient at step 321: 0.16884830594062805\n",
      "Max gradient at step 322: 0.13336719572544098\n",
      "Max gradient at step 323: 0.17669765651226044\n",
      "Max gradient at step 324: 0.2015717625617981\n",
      "Max gradient at step 325: 0.17299093306064606\n",
      "Max gradient at step 326: 0.1379936784505844\n",
      "Max gradient at step 327: 0.13995176553726196\n",
      "Max gradient at step 328: 0.20378395915031433\n",
      "Max gradient at step 329: 0.105727419257164\n",
      "Max gradient at step 330: 0.20626051723957062\n",
      "Max gradient at step 331: 0.1370142102241516\n",
      "Max gradient at step 332: 0.16291557252407074\n",
      "Max gradient at step 333: 0.12727124989032745\n",
      "Max gradient at step 334: 0.11607452481985092\n",
      "Max gradient at step 335: 0.14001759886741638\n",
      "Max gradient at step 336: 0.19250249862670898\n",
      "Max gradient at step 337: 0.21374906599521637\n",
      "Max gradient at step 338: 0.21373099088668823\n",
      "Max gradient at step 339: 0.18349768221378326\n",
      "Max gradient at step 340: 0.11053880304098129\n",
      "Max gradient at step 341: 0.22030045092105865\n",
      "Max gradient at step 342: 0.1409894824028015\n",
      "Max gradient at step 343: 0.10250783711671829\n",
      "Max gradient at step 344: 0.13651692867279053\n",
      "Max gradient at step 345: 0.1576061099767685\n",
      "Max gradient at step 346: 0.19382071495056152\n",
      "Max gradient at step 347: 0.23541304469108582\n",
      "Max gradient at step 348: 0.10431444644927979\n",
      "Max gradient at step 349: 0.16607612371444702\n",
      "Max gradient at step 350: 0.18316657841205597\n",
      "Max gradient at step 351: 0.2186601459980011\n",
      "Max gradient at step 352: 0.11710552871227264\n",
      "Max gradient at step 353: 0.19339527189731598\n",
      "Max gradient at step 354: 0.16190071403980255\n",
      "Max gradient at step 355: 0.20717386901378632\n",
      "Max gradient at step 356: 0.2202223837375641\n",
      "Max gradient at step 357: 0.21184778213500977\n",
      "Max gradient at step 358: 0.19787819683551788\n",
      "Max gradient at step 359: 0.1282380372285843\n",
      "Max gradient at step 360: 0.20612457394599915\n",
      "Max gradient at step 361: 0.18241620063781738\n",
      "Max gradient at step 362: 0.15472418069839478\n",
      "Max gradient at step 363: 0.12156105041503906\n",
      "Max gradient at step 364: 0.24686454236507416\n",
      "Max gradient at step 365: 0.32615816593170166\n",
      "Max gradient at step 366: 0.1750190705060959\n",
      "Max gradient at step 367: 0.12371409684419632\n",
      "Max gradient at step 368: 0.1835635006427765\n",
      "Max gradient at step 369: 0.1891258955001831\n",
      "Max gradient at step 370: 0.1571013629436493\n",
      "Max gradient at step 371: 0.20642831921577454\n",
      "Max gradient at step 372: 0.17843465507030487\n",
      "Max gradient at step 373: 0.1251242458820343\n",
      "Max gradient at step 374: 0.15609978139400482\n",
      "Max gradient at step 375: 0.10528036206960678\n",
      "Max gradient at step 376: 0.13602745532989502\n",
      "Max gradient at step 377: 0.16107293963432312\n",
      "Max gradient at step 378: 0.26630517840385437\n",
      "Max gradient at step 379: 0.18294334411621094\n",
      "Max gradient at step 380: 0.15397454798221588\n",
      "Max gradient at step 381: 0.15256576240062714\n",
      "Max gradient at step 382: 0.16742783784866333\n",
      "Max gradient at step 383: 0.15244612097740173\n",
      "Max gradient at step 384: 0.11652584373950958\n",
      "Max gradient at step 385: 0.18152615427970886\n",
      "Max gradient at step 386: 0.15971408784389496\n",
      "Max gradient at step 387: 0.13311000168323517\n",
      "Max gradient at step 388: 0.16352857649326324\n",
      "Max gradient at step 389: 0.18574827909469604\n",
      "Max gradient at step 390: 0.16307857632637024\n",
      "Max gradient at step 391: 0.1812727302312851\n",
      "Max gradient at step 392: 0.179254412651062\n",
      "Max gradient at step 393: 0.2666977345943451\n",
      "Max gradient at step 394: 0.1838218718767166\n",
      "Max gradient at step 395: 0.22088931500911713\n",
      "Max gradient at step 396: 0.1377202570438385\n",
      "Max gradient at step 397: 0.13185834884643555\n",
      "Max gradient at step 398: 0.13005785644054413\n",
      "Max gradient at step 399: 0.08937063813209534\n",
      "Max gradient at step 400: 0.18899017572402954\n",
      "Training loss (for one batch) at step 400: 0.6325\n",
      "Max gradient at step 401: 0.1977105438709259\n",
      "Max gradient at step 402: 0.29391416907310486\n",
      "Max gradient at step 403: 0.1738748848438263\n",
      "Max gradient at step 404: 0.10981721431016922\n",
      "Max gradient at step 405: 0.16167397797107697\n",
      "Max gradient at step 406: 0.12123697251081467\n",
      "Max gradient at step 407: 0.3122406005859375\n",
      "Max gradient at step 408: 0.1770235300064087\n",
      "Max gradient at step 409: 0.1660243570804596\n",
      "Max gradient at step 410: 0.1263226419687271\n",
      "Max gradient at step 411: 0.14654046297073364\n",
      "Max gradient at step 412: 0.16593272984027863\n",
      "Max gradient at step 413: 0.24981336295604706\n",
      "Max gradient at step 414: 0.17399939894676208\n",
      "Max gradient at step 415: 0.16837178170681\n",
      "Max gradient at step 416: 0.11499960720539093\n",
      "Max gradient at step 417: 0.14511485397815704\n",
      "Max gradient at step 418: 0.10238706320524216\n",
      "Max gradient at step 419: 0.1645262986421585\n",
      "Max gradient at step 420: 0.17421086132526398\n",
      "Max gradient at step 421: 0.18200938403606415\n",
      "Max gradient at step 422: 0.19200509786605835\n",
      "Max gradient at step 423: 0.18784673511981964\n",
      "Max gradient at step 424: 0.16560505330562592\n",
      "Max gradient at step 425: 0.11955077201128006\n",
      "Max gradient at step 426: 0.11536365747451782\n",
      "Max gradient at step 427: 0.19200678169727325\n",
      "Max gradient at step 428: 0.18314941227436066\n",
      "Max gradient at step 429: 0.1450803279876709\n",
      "Max gradient at step 430: 0.157890185713768\n",
      "Max gradient at step 431: 0.1272198110818863\n",
      "Max gradient at step 432: 0.1564348042011261\n",
      "Max gradient at step 433: 0.12355648726224899\n",
      "Max gradient at step 434: 0.31113120913505554\n",
      "Max gradient at step 435: 0.20095819234848022\n",
      "Max gradient at step 436: 0.17483925819396973\n",
      "Max gradient at step 437: 0.14394228160381317\n",
      "Max gradient at step 438: 0.16169428825378418\n",
      "Max gradient at step 439: 0.15929795801639557\n",
      "Max gradient at step 440: 0.14018042385578156\n",
      "Max gradient at step 441: 0.14581573009490967\n",
      "Max gradient at step 442: 0.18343809247016907\n",
      "Max gradient at step 443: 0.11493638902902603\n",
      "Max gradient at step 444: 0.1662418395280838\n",
      "Max gradient at step 445: 0.12482615560293198\n",
      "Max gradient at step 446: 0.14756402373313904\n",
      "Max gradient at step 447: 0.10333945602178574\n",
      "Max gradient at step 448: 0.1297856867313385\n",
      "Max gradient at step 449: 0.13842545449733734\n",
      "Max gradient at step 450: 0.1389477252960205\n",
      "Max gradient at step 451: 0.17526130378246307\n",
      "Max gradient at step 452: 0.21415743231773376\n",
      "Max gradient at step 453: 0.28532251715660095\n",
      "Max gradient at step 454: 0.19905886054039001\n",
      "Max gradient at step 455: 0.11057993769645691\n",
      "Max gradient at step 456: 0.17040622234344482\n",
      "Max gradient at step 457: 0.15492358803749084\n",
      "Max gradient at step 458: 0.14934031665325165\n",
      "Max gradient at step 459: 0.2152189016342163\n",
      "Max gradient at step 460: 0.13230031728744507\n",
      "Max gradient at step 461: 0.16278599202632904\n",
      "Max gradient at step 462: 0.3391055762767792\n",
      "Max gradient at step 463: 0.35064855217933655\n",
      "Max gradient at step 464: 0.20635101199150085\n",
      "Max gradient at step 465: 0.13727080821990967\n",
      "Max gradient at step 466: 0.15992261469364166\n",
      "Max gradient at step 467: 0.10632693022489548\n",
      "Max gradient at step 468: 0.1493890881538391\n",
      "Max gradient at step 469: 0.1982003152370453\n",
      "Max gradient at step 470: 0.11826599389314651\n",
      "Max gradient at step 471: 0.1671515852212906\n",
      "Max gradient at step 472: 0.22969630360603333\n",
      "Max gradient at step 473: 0.2500074505805969\n",
      "Max gradient at step 474: 0.21940352022647858\n",
      "Max gradient at step 475: 0.19531072676181793\n",
      "Max gradient at step 476: 0.16198334097862244\n",
      "Max gradient at step 477: 0.16241344809532166\n",
      "Max gradient at step 478: 0.19186079502105713\n",
      "Max gradient at step 479: 0.1400562971830368\n",
      "Max gradient at step 480: 0.10844366997480392\n",
      "Max gradient at step 481: 0.12676618993282318\n",
      "Max gradient at step 482: 0.20425063371658325\n",
      "Max gradient at step 483: 0.27251002192497253\n",
      "Max gradient at step 484: 0.1911400407552719\n",
      "Max gradient at step 485: 0.11142956465482712\n",
      "Max gradient at step 486: 0.1534336656332016\n",
      "Max gradient at step 487: 0.15113478899002075\n",
      "Max gradient at step 488: 0.2096097469329834\n",
      "Max gradient at step 489: 0.12989765405654907\n",
      "Max gradient at step 490: 0.2074250429868698\n",
      "Max gradient at step 491: 0.09321586787700653\n",
      "Max gradient at step 492: 0.08891814202070236\n",
      "Max gradient at step 493: 0.2293950468301773\n",
      "Max gradient at step 494: 0.14365935325622559\n",
      "Max gradient at step 495: 0.3012864589691162\n",
      "Max gradient at step 496: 0.15557590126991272\n",
      "Max gradient at step 497: 0.11206706613302231\n",
      "Max gradient at step 498: 0.20832180976867676\n",
      "Max gradient at step 499: 0.15839402377605438\n",
      "Max gradient at step 500: 0.24161529541015625\n",
      "Training loss (for one batch) at step 500: 0.3129\n",
      "Max gradient at step 501: 0.2642476558685303\n",
      "Max gradient at step 502: 0.14104551076889038\n",
      "Max gradient at step 503: 0.16203831136226654\n",
      "Max gradient at step 504: 0.1589028686285019\n",
      "Max gradient at step 505: 0.11191697418689728\n",
      "Max gradient at step 506: 0.1016460731625557\n",
      "Max gradient at step 507: 0.08244957029819489\n",
      "Max gradient at step 508: 0.13288071751594543\n",
      "Max gradient at step 509: 0.18601912260055542\n",
      "Max gradient at step 510: 0.3020840287208557\n",
      "Max gradient at step 511: 0.2126006931066513\n",
      "Max gradient at step 512: 0.13160888850688934\n",
      "Max gradient at step 513: 0.2017827033996582\n",
      "Max gradient at step 514: 0.15759116411209106\n",
      "Max gradient at step 515: 0.1746397167444229\n",
      "Max gradient at step 516: 0.09918569028377533\n",
      "Max gradient at step 517: 0.28958961367607117\n",
      "Max gradient at step 518: 0.19314999878406525\n",
      "Max gradient at step 519: 0.1722569763660431\n",
      "Max gradient at step 520: 0.12039943784475327\n",
      "Max gradient at step 521: 0.11477199196815491\n",
      "Max gradient at step 522: 0.17062470316886902\n",
      "Max gradient at step 523: 0.1655920147895813\n",
      "Max gradient at step 524: 0.1622682809829712\n",
      "Max gradient at step 525: 0.12935078144073486\n",
      "Max gradient at step 526: 0.297245055437088\n",
      "Max gradient at step 527: 0.14138145744800568\n",
      "Max gradient at step 528: 0.222835972905159\n",
      "Max gradient at step 529: 0.0767655223608017\n",
      "Max gradient at step 530: 0.1168571338057518\n",
      "Max gradient at step 531: 0.15277455747127533\n",
      "Max gradient at step 532: 0.1266283541917801\n",
      "Max gradient at step 533: 0.17675718665122986\n",
      "Max gradient at step 534: 0.14599475264549255\n",
      "Max gradient at step 535: 0.13490846753120422\n",
      "Max gradient at step 536: 0.13242895901203156\n",
      "Max gradient at step 537: 0.14240820705890656\n",
      "Max gradient at step 538: 0.2418835163116455\n",
      "Max gradient at step 539: 0.18395604193210602\n",
      "Max gradient at step 540: 0.3043077886104584\n",
      "Max gradient at step 541: 0.12712399661540985\n",
      "Max gradient at step 542: 0.09087216109037399\n",
      "Max gradient at step 543: 0.14506298303604126\n",
      "Max gradient at step 544: 0.12883205711841583\n",
      "Max gradient at step 545: 0.18228977918624878\n",
      "Max gradient at step 546: 0.16819113492965698\n",
      "Max gradient at step 547: 0.1538311094045639\n",
      "Max gradient at step 548: 0.14713500440120697\n",
      "Max gradient at step 549: 0.1939961016178131\n",
      "Max gradient at step 550: 0.1825275868177414\n",
      "Max gradient at step 551: 0.17858470976352692\n",
      "Max gradient at step 552: 0.1969357281923294\n",
      "Max gradient at step 553: 0.32062187790870667\n",
      "Max gradient at step 554: 0.09071893244981766\n",
      "Max gradient at step 555: 0.2054065763950348\n",
      "Max gradient at step 556: 0.10478424280881882\n",
      "Max gradient at step 557: 0.0924486368894577\n",
      "Max gradient at step 558: 0.256962388753891\n",
      "Max gradient at step 559: 0.12171003222465515\n",
      "Max gradient at step 560: 0.2356497198343277\n",
      "Max gradient at step 561: 0.1386599987745285\n",
      "Max gradient at step 562: 0.10992748290300369\n",
      "Max gradient at step 563: 0.21422924101352692\n",
      "Max gradient at step 564: 0.17291638255119324\n",
      "Max gradient at step 565: 0.10330783575773239\n",
      "Max gradient at step 566: 0.13008633255958557\n",
      "Max gradient at step 567: 0.21704131364822388\n",
      "Max gradient at step 568: 0.1862250566482544\n",
      "Max gradient at step 569: 0.1518050730228424\n",
      "Max gradient at step 570: 0.13052697479724884\n",
      "Max gradient at step 571: 0.2113354355096817\n",
      "Max gradient at step 572: 0.1127457544207573\n",
      "Max gradient at step 573: 0.09137878566980362\n",
      "Max gradient at step 574: 0.2248804271221161\n",
      "Max gradient at step 575: 0.11601029336452484\n",
      "Max gradient at step 576: 0.09068721532821655\n",
      "Max gradient at step 577: 0.11920353025197983\n",
      "Max gradient at step 578: 0.15063661336898804\n",
      "Max gradient at step 579: 0.14573174715042114\n",
      "Max gradient at step 580: 0.16244743764400482\n",
      "Max gradient at step 581: 0.12596513330936432\n",
      "Max gradient at step 582: 0.18160401284694672\n",
      "Max gradient at step 583: 0.1122560203075409\n",
      "Max gradient at step 584: 0.275135338306427\n",
      "Max gradient at step 585: 0.1535230278968811\n",
      "Max gradient at step 586: 0.11853162199258804\n",
      "Max gradient at step 587: 0.15645264089107513\n",
      "Max gradient at step 588: 0.12038541585206985\n",
      "Max gradient at step 589: 0.1258164346218109\n",
      "Max gradient at step 590: 0.11389771103858948\n",
      "Max gradient at step 591: 0.15372715890407562\n",
      "Max gradient at step 592: 0.08887793123722076\n",
      "Max gradient at step 593: 0.1286889910697937\n",
      "Max gradient at step 594: 0.18234366178512573\n",
      "Max gradient at step 595: 0.10688603669404984\n",
      "Max gradient at step 596: 0.33560433983802795\n",
      "Max gradient at step 597: 0.1769566535949707\n",
      "Max gradient at step 598: 0.11283296346664429\n",
      "Max gradient at step 599: 0.11681948602199554\n",
      "Max gradient at step 600: 0.19221296906471252\n",
      "Training loss (for one batch) at step 600: 0.5461\n",
      "Max gradient at step 601: 0.09106073528528214\n",
      "Max gradient at step 602: 0.1456962376832962\n",
      "Max gradient at step 603: 0.22077660262584686\n",
      "Max gradient at step 604: 0.1142357587814331\n",
      "Max gradient at step 605: 0.09268877655267715\n",
      "Max gradient at step 606: 0.2191772758960724\n",
      "Max gradient at step 607: 0.1299326866865158\n",
      "Max gradient at step 608: 0.1338314414024353\n",
      "Max gradient at step 609: 0.16182024776935577\n",
      "Max gradient at step 610: 0.1314646154642105\n",
      "Max gradient at step 611: 0.12124242633581161\n",
      "Max gradient at step 612: 0.07276750355958939\n",
      "Max gradient at step 613: 0.22843825817108154\n",
      "Max gradient at step 614: 0.16239218413829803\n",
      "Max gradient at step 615: 0.22337386012077332\n",
      "Max gradient at step 616: 0.13361729681491852\n",
      "Max gradient at step 617: 0.13136298954486847\n",
      "Max gradient at step 618: 0.13426515460014343\n",
      "Max gradient at step 619: 0.20805859565734863\n",
      "Max gradient at step 620: 0.11893954873085022\n",
      "Max gradient at step 621: 0.13138698041439056\n",
      "Max gradient at step 622: 0.22616156935691833\n",
      "Max gradient at step 623: 0.1400749236345291\n",
      "Max gradient at step 624: 0.16580933332443237\n",
      "Max gradient at step 625: 0.16360408067703247\n",
      "Max gradient at step 626: 0.2258797436952591\n",
      "Max gradient at step 627: 0.1995638608932495\n",
      "Max gradient at step 628: 0.20307055115699768\n",
      "Max gradient at step 629: 0.17375817894935608\n",
      "Max gradient at step 630: 0.17113392055034637\n",
      "Max gradient at step 631: 0.24411165714263916\n",
      "Max gradient at step 632: 0.14848874509334564\n",
      "Max gradient at step 633: 0.269234299659729\n",
      "Max gradient at step 634: 0.1752847135066986\n",
      "Max gradient at step 635: 0.1225171834230423\n",
      "Max gradient at step 636: 0.21132123470306396\n",
      "Max gradient at step 637: 0.09873349219560623\n",
      "Max gradient at step 638: 0.11709747463464737\n",
      "Max gradient at step 639: 0.19934657216072083\n",
      "Max gradient at step 640: 0.14094851911067963\n",
      "Max gradient at step 641: 0.15852877497673035\n",
      "Max gradient at step 642: 0.15421365201473236\n",
      "Max gradient at step 643: 0.1105676218867302\n",
      "Max gradient at step 644: 0.12428250908851624\n",
      "Max gradient at step 645: 0.12903441488742828\n",
      "Max gradient at step 646: 0.15001754462718964\n",
      "Max gradient at step 647: 0.25856921076774597\n",
      "Max gradient at step 648: 0.1497718095779419\n",
      "Max gradient at step 649: 0.13487517833709717\n",
      "Max gradient at step 650: 0.21870803833007812\n",
      "Max gradient at step 651: 0.13199546933174133\n",
      "Max gradient at step 652: 0.08406022936105728\n",
      "Max gradient at step 653: 0.1299891173839569\n",
      "Max gradient at step 654: 0.21684882044792175\n",
      "Max gradient at step 655: 0.17682665586471558\n",
      "Max gradient at step 656: 0.17835447192192078\n",
      "Max gradient at step 657: 0.10792351514101028\n",
      "Max gradient at step 658: 0.13071629405021667\n",
      "Max gradient at step 659: 0.3313668370246887\n",
      "Max gradient at step 660: 0.17615248262882233\n",
      "Max gradient at step 661: 0.22376219928264618\n",
      "Max gradient at step 662: 0.20769667625427246\n",
      "Max gradient at step 663: 0.23506510257720947\n",
      "Max gradient at step 664: 0.14561089873313904\n",
      "Max gradient at step 665: 0.1280282884836197\n",
      "Max gradient at step 666: 0.12978383898735046\n",
      "Max gradient at step 667: 0.22150582075119019\n",
      "Max gradient at step 668: 0.1903809756040573\n",
      "Max gradient at step 669: 0.1632898449897766\n",
      "Max gradient at step 670: 0.22788198292255402\n",
      "Max gradient at step 671: 0.17052562534809113\n",
      "Max gradient at step 672: 0.19975724816322327\n",
      "Max gradient at step 673: 0.16601023077964783\n",
      "Max gradient at step 674: 0.26395195722579956\n",
      "Max gradient at step 675: 0.14648105204105377\n",
      "Max gradient at step 676: 0.1634242981672287\n",
      "Max gradient at step 677: 0.22431576251983643\n",
      "Max gradient at step 678: 0.12349560856819153\n",
      "Max gradient at step 679: 0.16074681282043457\n",
      "Max gradient at step 680: 0.2118488848209381\n",
      "Max gradient at step 681: 0.15018470585346222\n",
      "Max gradient at step 682: 0.17984770238399506\n",
      "Max gradient at step 683: 0.15070392191410065\n",
      "Max gradient at step 684: 0.12009835243225098\n",
      "Max gradient at step 685: 0.1407298743724823\n",
      "Max gradient at step 686: 0.1740231215953827\n",
      "Max gradient at step 687: 0.2022925615310669\n",
      "Max gradient at step 688: 0.12215851247310638\n",
      "Max gradient at step 689: 0.12009796500205994\n",
      "Max gradient at step 690: 0.2220955193042755\n",
      "Max gradient at step 691: 0.15985341370105743\n",
      "Max gradient at step 692: 0.18534578382968903\n",
      "Max gradient at step 693: 0.22448931634426117\n",
      "Max gradient at step 694: 0.21145956218242645\n",
      "Max gradient at step 695: 0.2320476919412613\n",
      "Max gradient at step 696: 0.13610951602458954\n",
      "Max gradient at step 697: 0.121501125395298\n",
      "Max gradient at step 698: 0.20756135880947113\n",
      "Max gradient at step 699: 0.12255639582872391\n",
      "Max gradient at step 700: 0.21073485910892487\n",
      "Training loss (for one batch) at step 700: 0.5082\n",
      "Max gradient at step 701: 0.18010227382183075\n",
      "Max gradient at step 702: 0.14191469550132751\n",
      "Max gradient at step 703: 0.15657825767993927\n",
      "Max gradient at step 704: 0.18951953947544098\n",
      "Max gradient at step 705: 0.2110222727060318\n",
      "Max gradient at step 706: 0.1948401778936386\n",
      "Max gradient at step 707: 0.09281523525714874\n",
      "Max gradient at step 708: 0.14465896785259247\n",
      "Max gradient at step 709: 0.2707502841949463\n",
      "Max gradient at step 710: 0.1380774974822998\n",
      "Max gradient at step 711: 0.13151298463344574\n",
      "Max gradient at step 712: 0.18203307688236237\n",
      "Max gradient at step 713: 0.20967301726341248\n",
      "Max gradient at step 714: 0.2621721029281616\n",
      "Max gradient at step 715: 0.14900775253772736\n",
      "Max gradient at step 716: 0.08626537024974823\n",
      "Max gradient at step 717: 0.14943046867847443\n",
      "Max gradient at step 718: 0.1275123655796051\n",
      "Max gradient at step 719: 0.10482518374919891\n",
      "Max gradient at step 720: 0.2137073576450348\n",
      "Max gradient at step 721: 0.1489410400390625\n",
      "Max gradient at step 722: 0.13137027621269226\n",
      "Max gradient at step 723: 0.10345809906721115\n",
      "Max gradient at step 724: 0.14285574853420258\n",
      "Max gradient at step 725: 0.17155668139457703\n",
      "Max gradient at step 726: 0.16424037516117096\n",
      "Max gradient at step 727: 0.13187485933303833\n",
      "Max gradient at step 728: 0.2287357747554779\n",
      "Max gradient at step 729: 0.1335902363061905\n",
      "Max gradient at step 730: 0.0632888525724411\n",
      "Max gradient at step 731: 0.16061125695705414\n",
      "Max gradient at step 732: 0.08282308280467987\n",
      "Max gradient at step 733: 0.12013160437345505\n",
      "Max gradient at step 734: 0.2020222395658493\n",
      "Max gradient at step 735: 0.23759976029396057\n",
      "Max gradient at step 736: 0.11409325897693634\n",
      "Max gradient at step 737: 0.24396046996116638\n",
      "Max gradient at step 738: 0.11851519346237183\n",
      "Max gradient at step 739: 0.19258521497249603\n",
      "Max gradient at step 740: 0.19671589136123657\n",
      "Max gradient at step 741: 0.23559127748012543\n",
      "Max gradient at step 742: 0.18102052807807922\n",
      "Max gradient at step 743: 0.11579164862632751\n",
      "Max gradient at step 744: 0.13106989860534668\n",
      "Max gradient at step 745: 0.10772999376058578\n",
      "Max gradient at step 746: 0.20682081580162048\n",
      "Max gradient at step 747: 0.20729297399520874\n",
      "Max gradient at step 748: 0.2170223891735077\n",
      "Max gradient at step 749: 0.1717764139175415\n",
      "Max gradient at step 750: 0.10314271599054337\n",
      "Max gradient at step 751: 0.16572654247283936\n",
      "Max gradient at step 752: 0.12690603733062744\n",
      "Max gradient at step 753: 0.16238701343536377\n",
      "Max gradient at step 754: 0.11069083213806152\n",
      "Max gradient at step 755: 0.07164986431598663\n",
      "Max gradient at step 756: 0.20674027502536774\n",
      "Max gradient at step 757: 0.11702629923820496\n",
      "Max gradient at step 758: 0.1762886941432953\n",
      "Max gradient at step 759: 0.13931211829185486\n",
      "Max gradient at step 760: 0.16849026083946228\n",
      "Max gradient at step 761: 0.1431657075881958\n",
      "Max gradient at step 762: 0.2626742124557495\n",
      "Max gradient at step 763: 0.13081319630146027\n",
      "Max gradient at step 764: 0.18103854358196259\n",
      "Max gradient at step 765: 0.15523700416088104\n",
      "Max gradient at step 766: 0.23711155354976654\n",
      "Max gradient at step 767: 0.12613682448863983\n",
      "Max gradient at step 768: 0.09485894441604614\n",
      "Max gradient at step 769: 0.23023410141468048\n",
      "Max gradient at step 770: 0.13044217228889465\n",
      "Max gradient at step 771: 0.12019405514001846\n",
      "Max gradient at step 772: 0.2134638875722885\n",
      "Max gradient at step 773: 0.17106366157531738\n",
      "Max gradient at step 774: 0.11063488572835922\n",
      "Max gradient at step 775: 0.12792879343032837\n",
      "Max gradient at step 776: 0.12540076673030853\n",
      "Max gradient at step 777: 0.13157077133655548\n",
      "Max gradient at step 778: 0.15998396277427673\n",
      "Max gradient at step 779: 0.13402724266052246\n",
      "Max gradient at step 780: 0.14263954758644104\n",
      "Max gradient at step 781: 0.28975167870521545\n",
      "Max gradient at step 782: 0.1120503768324852\n",
      "Max gradient at step 783: 0.1763514280319214\n",
      "Max gradient at step 784: 0.10532528162002563\n",
      "Max gradient at step 785: 0.14142553508281708\n",
      "Max gradient at step 786: 0.1906348317861557\n",
      "Max gradient at step 787: 0.19949471950531006\n",
      "Max gradient at step 788: 0.13053640723228455\n",
      "Max gradient at step 789: 0.16954255104064941\n",
      "Max gradient at step 790: 0.2309950590133667\n",
      "Max gradient at step 791: 0.20492340624332428\n",
      "Max gradient at step 792: 0.18430602550506592\n",
      "Max gradient at step 793: 0.11548420041799545\n",
      "Max gradient at step 794: 0.16891780495643616\n",
      "Max gradient at step 795: 0.1481325924396515\n",
      "Max gradient at step 796: 0.137117400765419\n",
      "Max gradient at step 797: 0.1762682944536209\n",
      "Max gradient at step 798: 0.16136302053928375\n",
      "Max gradient at step 799: 0.05999629944562912\n",
      "Max gradient at step 800: 0.12608298659324646\n",
      "Training loss (for one batch) at step 800: 0.3247\n",
      "Max gradient at step 801: 0.22666414082050323\n",
      "Max gradient at step 802: 0.16626431047916412\n",
      "Max gradient at step 803: 0.14100366830825806\n",
      "Max gradient at step 804: 0.1715167760848999\n",
      "Max gradient at step 805: 0.13830985128879547\n",
      "Max gradient at step 806: 0.17353665828704834\n",
      "Max gradient at step 807: 0.1735030710697174\n",
      "Max gradient at step 808: 0.08893795311450958\n",
      "Max gradient at step 809: 0.14465682208538055\n",
      "Max gradient at step 810: 0.24023835361003876\n",
      "Max gradient at step 811: 0.315341055393219\n",
      "Max gradient at step 812: 0.10466869920492172\n",
      "Max gradient at step 813: 0.21934962272644043\n",
      "Max gradient at step 814: 0.20137274265289307\n",
      "Max gradient at step 815: 0.13997416198253632\n",
      "Max gradient at step 816: 0.1625993400812149\n",
      "Max gradient at step 817: 0.11355620622634888\n",
      "Max gradient at step 818: 0.15440049767494202\n",
      "Max gradient at step 819: 0.19788500666618347\n",
      "Max gradient at step 820: 0.17278580367565155\n",
      "Max gradient at step 821: 0.1988302320241928\n",
      "Max gradient at step 822: 0.13107728958129883\n",
      "Max gradient at step 823: 0.2952677607536316\n",
      "Max gradient at step 824: 0.20942769944667816\n",
      "Max gradient at step 825: 0.1678713858127594\n",
      "Max gradient at step 826: 0.16381213068962097\n",
      "Max gradient at step 827: 0.10750459879636765\n",
      "Max gradient at step 828: 0.11356461048126221\n",
      "Max gradient at step 829: 0.11036323010921478\n",
      "Max gradient at step 830: 0.19576632976531982\n",
      "Max gradient at step 831: 0.2023773044347763\n",
      "Max gradient at step 832: 0.19490252435207367\n",
      "Max gradient at step 833: 0.26634636521339417\n",
      "Max gradient at step 834: 0.2036321759223938\n",
      "Max gradient at step 835: 0.1755455881357193\n",
      "Max gradient at step 836: 0.23219940066337585\n",
      "Max gradient at step 837: 0.11008767783641815\n",
      "Max gradient at step 838: 0.2197314202785492\n",
      "Max gradient at step 839: 0.09688620269298553\n",
      "Max gradient at step 840: 0.1330190896987915\n",
      "Max gradient at step 841: 0.2019200325012207\n",
      "Max gradient at step 842: 0.16908882558345795\n",
      "Max gradient at step 843: 0.15301471948623657\n",
      "Max gradient at step 844: 0.17999841272830963\n",
      "Max gradient at step 845: 0.21617072820663452\n",
      "Max gradient at step 846: 0.1744891256093979\n",
      "Max gradient at step 847: 0.14615759253501892\n",
      "Max gradient at step 848: 0.11996549367904663\n",
      "Max gradient at step 849: 0.1332007646560669\n",
      "Max gradient at step 850: 0.08710145205259323\n",
      "Max gradient at step 851: 0.23883052170276642\n",
      "Max gradient at step 852: 0.20519889891147614\n",
      "Max gradient at step 853: 0.0926671028137207\n",
      "Max gradient at step 854: 0.22174325585365295\n",
      "Max gradient at step 855: 0.2363179326057434\n",
      "Max gradient at step 856: 0.10802093893289566\n",
      "Max gradient at step 857: 0.15332502126693726\n",
      "Max gradient at step 858: 0.22748345136642456\n",
      "Max gradient at step 859: 0.14948579668998718\n",
      "Max gradient at step 860: 0.10750269889831543\n",
      "Max gradient at step 861: 0.14697368443012238\n",
      "Max gradient at step 862: 0.1336725503206253\n",
      "Max gradient at step 863: 0.24508708715438843\n",
      "Max gradient at step 864: 0.09610357880592346\n",
      "Max gradient at step 865: 0.1224493458867073\n",
      "Max gradient at step 866: 0.06905089318752289\n",
      "Max gradient at step 867: 0.1853821575641632\n",
      "Max gradient at step 868: 0.15362577140331268\n",
      "Max gradient at step 869: 0.21051710844039917\n",
      "Max gradient at step 870: 0.11451936513185501\n",
      "Max gradient at step 871: 0.13717660307884216\n",
      "Max gradient at step 872: 0.18103384971618652\n",
      "Max gradient at step 873: 0.11605768650770187\n",
      "Max gradient at step 874: 0.1655598133802414\n",
      "Max gradient at step 875: 0.14769813418388367\n",
      "Max gradient at step 876: 0.21668808162212372\n",
      "Max gradient at step 877: 0.18561697006225586\n",
      "Max gradient at step 878: 0.1599707007408142\n",
      "Max gradient at step 879: 0.20817404985427856\n",
      "Max gradient at step 880: 0.19437824189662933\n",
      "Max gradient at step 881: 0.175266832113266\n",
      "Max gradient at step 882: 0.1661347895860672\n",
      "Max gradient at step 883: 0.21841518580913544\n",
      "Max gradient at step 884: 0.09097372740507126\n",
      "Max gradient at step 885: 0.213131383061409\n",
      "Max gradient at step 886: 0.26056477427482605\n",
      "Max gradient at step 887: 0.18125948309898376\n",
      "Max gradient at step 888: 0.23475876450538635\n",
      "Max gradient at step 889: 0.15216794610023499\n",
      "Max gradient at step 890: 0.15574544668197632\n",
      "Max gradient at step 891: 0.0952647402882576\n",
      "Max gradient at step 892: 0.19668425619602203\n",
      "Max gradient at step 893: 0.17555879056453705\n",
      "Max gradient at step 894: 0.25254809856414795\n",
      "Max gradient at step 895: 0.2978377640247345\n",
      "Max gradient at step 896: 0.23131433129310608\n",
      "Max gradient at step 897: 0.12230875343084335\n",
      "Max gradient at step 898: 0.173688605427742\n",
      "Max gradient at step 899: 0.11459997296333313\n",
      "Max gradient at step 900: 0.13806329667568207\n",
      "Training loss (for one batch) at step 900: 0.2612\n",
      "Max gradient at step 901: 0.1241971105337143\n",
      "Max gradient at step 902: 0.11652010679244995\n",
      "Max gradient at step 903: 0.26809513568878174\n",
      "Max gradient at step 904: 0.11674052476882935\n",
      "Max gradient at step 905: 0.12248408049345016\n",
      "Max gradient at step 906: 0.1336977481842041\n",
      "Max gradient at step 907: 0.14931286871433258\n",
      "Max gradient at step 908: 0.14314661920070648\n",
      "Max gradient at step 909: 0.17455603182315826\n",
      "Max gradient at step 910: 0.18863815069198608\n",
      "Max gradient at step 911: 0.19720801711082458\n",
      "Max gradient at step 912: 0.15278515219688416\n",
      "Max gradient at step 913: 0.13685034215450287\n",
      "Max gradient at step 914: 0.19181060791015625\n",
      "Max gradient at step 915: 0.11457306891679764\n",
      "Max gradient at step 916: 0.09885425865650177\n",
      "Max gradient at step 917: 0.12384694069623947\n",
      "Max gradient at step 918: 0.1700807809829712\n",
      "Max gradient at step 919: 0.17717353999614716\n",
      "Max gradient at step 920: 0.17324572801589966\n",
      "Max gradient at step 921: 0.2674621641635895\n",
      "Max gradient at step 922: 0.14390972256660461\n",
      "Max gradient at step 923: 0.15346503257751465\n",
      "Max gradient at step 924: 0.22059446573257446\n",
      "Max gradient at step 925: 0.18267317116260529\n",
      "Max gradient at step 926: 0.1457621306180954\n",
      "Max gradient at step 927: 0.13999593257904053\n",
      "Max gradient at step 928: 0.1463697999715805\n",
      "Max gradient at step 929: 0.1758682131767273\n",
      "Max gradient at step 930: 0.22653739154338837\n",
      "Max gradient at step 931: 0.16969922184944153\n",
      "Max gradient at step 932: 0.14395388960838318\n",
      "Max gradient at step 933: 0.10180993378162384\n",
      "Max gradient at step 934: 0.2727651000022888\n",
      "Max gradient at step 935: 0.13905826210975647\n",
      "Max gradient at step 936: 0.20191378891468048\n",
      "Max gradient at step 937: 0.1300191432237625\n",
      "Max gradient at step 938: 0.16110074520111084\n",
      "Max gradient at step 939: 0.17621789872646332\n",
      "Max gradient at step 940: 0.16815754771232605\n",
      "Max gradient at step 941: 0.16161435842514038\n",
      "Max gradient at step 942: 0.18942061066627502\n",
      "Max gradient at step 943: 0.21534870564937592\n",
      "Max gradient at step 944: 0.13116738200187683\n",
      "Max gradient at step 945: 0.13834910094738007\n",
      "Max gradient at step 946: 0.1480141133069992\n",
      "Max gradient at step 947: 0.27135056257247925\n",
      "Max gradient at step 948: 0.1886291801929474\n",
      "Max gradient at step 949: 0.15139774978160858\n",
      "Max gradient at step 950: 0.18096724152565002\n",
      "Max gradient at step 951: 0.2105238437652588\n",
      "Max gradient at step 952: 0.136630579829216\n",
      "Max gradient at step 953: 0.2484966367483139\n",
      "Max gradient at step 954: 0.17352914810180664\n",
      "Max gradient at step 955: 0.37443193793296814\n",
      "Max gradient at step 956: 0.11912687122821808\n",
      "Max gradient at step 957: 0.13785459101200104\n",
      "Max gradient at step 958: 0.2057281881570816\n",
      "Max gradient at step 959: 0.1149667277932167\n",
      "Max gradient at step 960: 0.2009107619524002\n",
      "Max gradient at step 961: 0.1909015029668808\n",
      "Max gradient at step 962: 0.20647932589054108\n",
      "Max gradient at step 963: 0.17908187210559845\n",
      "Max gradient at step 964: 0.20830614864826202\n",
      "Max gradient at step 965: 0.1394725739955902\n",
      "Max gradient at step 966: 0.18756872415542603\n",
      "Max gradient at step 967: 0.17813700437545776\n",
      "Max gradient at step 968: 0.1994030624628067\n",
      "Max gradient at step 969: 0.19313713908195496\n",
      "Max gradient at step 970: 0.1432414948940277\n",
      "Max gradient at step 971: 0.10940064489841461\n",
      "Max gradient at step 972: 0.15750275552272797\n",
      "Max gradient at step 973: 0.1972702145576477\n",
      "Max gradient at step 974: 0.1781678944826126\n",
      "Max gradient at step 975: 0.0909443125128746\n",
      "Max gradient at step 976: 0.1662950962781906\n",
      "Max gradient at step 977: 0.200942263007164\n",
      "Max gradient at step 978: 0.1359708607196808\n",
      "Max gradient at step 979: 0.1980176717042923\n",
      "Max gradient at step 980: 0.24291837215423584\n",
      "Max gradient at step 981: 0.1788383275270462\n",
      "Max gradient at step 982: 0.14126895368099213\n",
      "Max gradient at step 983: 0.10368098318576813\n",
      "Max gradient at step 984: 0.23870624601840973\n",
      "Max gradient at step 985: 0.15191058814525604\n",
      "Max gradient at step 986: 0.2172842025756836\n",
      "Max gradient at step 987: 0.33741557598114014\n",
      "Max gradient at step 988: 0.2641356885433197\n",
      "Max gradient at step 989: 0.1551380306482315\n",
      "Max gradient at step 990: 0.17038661241531372\n",
      "Max gradient at step 991: 0.41999638080596924\n",
      "Max gradient at step 992: 0.24057835340499878\n",
      "Max gradient at step 993: 0.18365634977817535\n",
      "Max gradient at step 994: 0.20570513606071472\n",
      "Max gradient at step 995: 0.22441919147968292\n",
      "Max gradient at step 996: 0.19761161506175995\n",
      "Max gradient at step 997: 0.20243938267230988\n",
      "Max gradient at step 998: 0.12171895056962967\n",
      "Max gradient at step 999: 0.2653175890445709\n",
      "Max gradient at step 1000: 0.08381827175617218\n",
      "Training loss (for one batch) at step 1000: 0.1558\n",
      "Max gradient at step 1001: 0.16757291555404663\n",
      "Max gradient at step 1002: 0.2509957253932953\n",
      "Max gradient at step 1003: 0.19255313277244568\n",
      "Max gradient at step 1004: 0.21562354266643524\n",
      "Max gradient at step 1005: 0.1729944497346878\n",
      "Max gradient at step 1006: 0.129815936088562\n",
      "Max gradient at step 1007: 0.20532391965389252\n",
      "Max gradient at step 1008: 0.1288112998008728\n",
      "Max gradient at step 1009: 0.1909942477941513\n",
      "Max gradient at step 1010: 0.1347046196460724\n",
      "Max gradient at step 1011: 0.21173059940338135\n",
      "Max gradient at step 1012: 0.1972256898880005\n",
      "Max gradient at step 1013: 0.09605234116315842\n",
      "Max gradient at step 1014: 0.1887609362602234\n",
      "Max gradient at step 1015: 0.24063651263713837\n",
      "Max gradient at step 1016: 0.15268561244010925\n",
      "Max gradient at step 1017: 0.14907772839069366\n",
      "Max gradient at step 1018: 0.1285402774810791\n",
      "Max gradient at step 1019: 0.15504300594329834\n",
      "Max gradient at step 1020: 0.11661334335803986\n",
      "Max gradient at step 1021: 0.10313307493925095\n",
      "Max gradient at step 1022: 0.13523465394973755\n",
      "Max gradient at step 1023: 0.16626441478729248\n",
      "Max gradient at step 1024: 0.2448716014623642\n",
      "Max gradient at step 1025: 0.15030018985271454\n",
      "Max gradient at step 1026: 0.13659709692001343\n",
      "Max gradient at step 1027: 0.1332366019487381\n",
      "Max gradient at step 1028: 0.11889514327049255\n",
      "Max gradient at step 1029: 0.18464350700378418\n",
      "Max gradient at step 1030: 0.16509504616260529\n",
      "Max gradient at step 1031: 0.1226685643196106\n",
      "Max gradient at step 1032: 0.19032080471515656\n",
      "Max gradient at step 1033: 0.18824486434459686\n",
      "Max gradient at step 1034: 0.05588831380009651\n",
      "Max gradient at step 1035: 0.11117982864379883\n",
      "Max gradient at step 1036: 0.26731449365615845\n",
      "Max gradient at step 1037: 0.13617365062236786\n",
      "Max gradient at step 1038: 0.14306606352329254\n",
      "Max gradient at step 1039: 0.24737688899040222\n",
      "Max gradient at step 1040: 0.20080548524856567\n",
      "Max gradient at step 1041: 0.11794119328260422\n",
      "Max gradient at step 1042: 0.15397988259792328\n",
      "Max gradient at step 1043: 0.1902361363172531\n",
      "Max gradient at step 1044: 0.04533576965332031\n",
      "Max gradient at step 1045: 0.17248313128948212\n",
      "Max gradient at step 1046: 0.2516602575778961\n",
      "Max gradient at step 1047: 0.17462646961212158\n",
      "Max gradient at step 1048: 0.09588370472192764\n",
      "Max gradient at step 1049: 0.08257053792476654\n",
      "Max gradient at step 1050: 0.3157747983932495\n",
      "Max gradient at step 1051: 0.16685040295124054\n",
      "Max gradient at step 1052: 0.19940590858459473\n",
      "Max gradient at step 1053: 0.14417077600955963\n",
      "Max gradient at step 1054: 0.10828711092472076\n",
      "Max gradient at step 1055: 0.11536337435245514\n",
      "Max gradient at step 1056: 0.19583770632743835\n",
      "Max gradient at step 1057: 0.12620019912719727\n",
      "Max gradient at step 1058: 0.19401860237121582\n",
      "Max gradient at step 1059: 0.1857074797153473\n",
      "Max gradient at step 1060: 0.12334582209587097\n",
      "Max gradient at step 1061: 0.14739856123924255\n",
      "Max gradient at step 1062: 0.21688337624073029\n",
      "Max gradient at step 1063: 0.1785738170146942\n",
      "Max gradient at step 1064: 0.14892256259918213\n",
      "Max gradient at step 1065: 0.10387002676725388\n",
      "Max gradient at step 1066: 0.13779528439044952\n",
      "Max gradient at step 1067: 0.09711024910211563\n",
      "Max gradient at step 1068: 0.1288803517818451\n",
      "Max gradient at step 1069: 0.21468298137187958\n",
      "Max gradient at step 1070: 0.2682421803474426\n",
      "Max gradient at step 1071: 0.1880250722169876\n",
      "Max gradient at step 1072: 0.20123575627803802\n",
      "Max gradient at step 1073: 0.15942129492759705\n",
      "Max gradient at step 1074: 0.14524014294147491\n",
      "Max gradient at step 1075: 0.1372189223766327\n",
      "Max gradient at step 1076: 0.15089616179466248\n",
      "Max gradient at step 1077: 0.23158535361289978\n",
      "Max gradient at step 1078: 0.08988359570503235\n",
      "Max gradient at step 1079: 0.09862305223941803\n",
      "Max gradient at step 1080: 0.1318494975566864\n",
      "Max gradient at step 1081: 0.21472543478012085\n",
      "Max gradient at step 1082: 0.25319626927375793\n",
      "Max gradient at step 1083: 0.14417915046215057\n",
      "Max gradient at step 1084: 0.2139034867286682\n",
      "Max gradient at step 1085: 0.16576896607875824\n",
      "Max gradient at step 1086: 0.3547746241092682\n",
      "Max gradient at step 1087: 0.13939662277698517\n",
      "Max gradient at step 1088: 0.1144915446639061\n",
      "Max gradient at step 1089: 0.10562190413475037\n",
      "Max gradient at step 1090: 0.13849258422851562\n",
      "Max gradient at step 1091: 0.12597908079624176\n",
      "Max gradient at step 1092: 0.1305805891752243\n",
      "Max gradient at step 1093: 0.09761332720518112\n",
      "Max gradient at step 1094: 0.11974155902862549\n",
      "Max gradient at step 1095: 0.16496893763542175\n",
      "Max gradient at step 1096: 0.22950638830661774\n",
      "Max gradient at step 1097: 0.14045344293117523\n",
      "Max gradient at step 1098: 0.07241400331258774\n",
      "Max gradient at step 1099: 0.11696948856115341\n",
      "Max gradient at step 1100: 0.1864817589521408\n",
      "Training loss (for one batch) at step 1100: 0.6322\n",
      "Max gradient at step 1101: 0.06453927606344223\n",
      "Max gradient at step 1102: 0.14054551720619202\n",
      "Max gradient at step 1103: 0.08296248316764832\n",
      "Max gradient at step 1104: 0.2100498378276825\n",
      "Max gradient at step 1105: 0.12366314977407455\n",
      "Max gradient at step 1106: 0.19901777803897858\n",
      "Max gradient at step 1107: 0.26668232679367065\n",
      "Max gradient at step 1108: 0.10383372753858566\n",
      "Max gradient at step 1109: 0.12825198471546173\n",
      "Max gradient at step 1110: 0.1501941978931427\n",
      "Max gradient at step 1111: 0.15650905668735504\n",
      "Max gradient at step 1112: 0.11839395761489868\n",
      "Max gradient at step 1113: 0.0914272665977478\n",
      "Max gradient at step 1114: 0.18804198503494263\n",
      "Max gradient at step 1115: 0.23676921427249908\n",
      "Max gradient at step 1116: 0.12750110030174255\n",
      "Max gradient at step 1117: 0.12919694185256958\n",
      "Max gradient at step 1118: 0.1276094764471054\n",
      "Max gradient at step 1119: 0.18990221619606018\n",
      "Max gradient at step 1120: 0.18073569238185883\n",
      "Max gradient at step 1121: 0.22274450957775116\n",
      "Max gradient at step 1122: 0.17903810739517212\n",
      "Max gradient at step 1123: 0.109949491918087\n",
      "Max gradient at step 1124: 0.1927461475133896\n",
      "Max gradient at step 1125: 0.14762213826179504\n",
      "Max gradient at step 1126: 0.10303843766450882\n",
      "Max gradient at step 1127: 0.12142695486545563\n",
      "Max gradient at step 1128: 0.179800346493721\n",
      "Max gradient at step 1129: 0.3265056908130646\n",
      "Max gradient at step 1130: 0.25580674409866333\n",
      "Max gradient at step 1131: 0.11263884603977203\n",
      "Max gradient at step 1132: 0.2068253755569458\n",
      "Max gradient at step 1133: 0.1618681252002716\n",
      "Max gradient at step 1134: 0.1444568783044815\n",
      "Max gradient at step 1135: 0.32091760635375977\n",
      "Max gradient at step 1136: 0.19036273658275604\n",
      "Max gradient at step 1137: 0.20902475714683533\n",
      "Max gradient at step 1138: 0.20084694027900696\n",
      "Max gradient at step 1139: 0.12888221442699432\n",
      "Max gradient at step 1140: 0.12043719738721848\n",
      "Max gradient at step 1141: 0.17207537591457367\n",
      "Max gradient at step 1142: 0.2448766827583313\n",
      "Max gradient at step 1143: 0.19032537937164307\n",
      "Max gradient at step 1144: 0.17687608301639557\n",
      "Max gradient at step 1145: 0.09162293374538422\n",
      "Max gradient at step 1146: 0.12259865552186966\n",
      "Max gradient at step 1147: 0.09434187412261963\n",
      "Max gradient at step 1148: 0.18948926031589508\n",
      "Max gradient at step 1149: 0.3406233489513397\n",
      "Max gradient at step 1150: 0.23149417340755463\n",
      "Max gradient at step 1151: 0.2223629206418991\n",
      "Max gradient at step 1152: 0.21094220876693726\n",
      "Max gradient at step 1153: 0.1528368592262268\n",
      "Max gradient at step 1154: 0.21795780956745148\n",
      "Max gradient at step 1155: 0.3015286326408386\n",
      "Max gradient at step 1156: 0.12348806858062744\n",
      "Max gradient at step 1157: 0.30798497796058655\n",
      "Max gradient at step 1158: 0.12907391786575317\n",
      "Max gradient at step 1159: 0.09198585897684097\n",
      "Max gradient at step 1160: 0.2679183781147003\n",
      "Max gradient at step 1161: 0.09567498415708542\n",
      "Max gradient at step 1162: 0.11003195494413376\n",
      "Max gradient at step 1163: 0.10889595746994019\n",
      "Max gradient at step 1164: 0.12023252993822098\n",
      "Max gradient at step 1165: 0.17022161185741425\n",
      "Max gradient at step 1166: 0.16728925704956055\n",
      "Max gradient at step 1167: 0.19896656274795532\n",
      "Max gradient at step 1168: 0.16125865280628204\n",
      "Max gradient at step 1169: 0.1823287159204483\n",
      "Max gradient at step 1170: 0.14341571927070618\n",
      "Max gradient at step 1171: 0.23080693185329437\n",
      "Max gradient at step 1172: 0.052666496485471725\n",
      "Max gradient at step 1173: 0.2103552371263504\n",
      "Max gradient at step 1174: 0.293583482503891\n",
      "Max gradient at step 1175: 0.1597340852022171\n",
      "Max gradient at step 1176: 0.1831762045621872\n",
      "Max gradient at step 1177: 0.1837375909090042\n",
      "Max gradient at step 1178: 0.10439060628414154\n",
      "Max gradient at step 1179: 0.18009953200817108\n",
      "Max gradient at step 1180: 0.1430160403251648\n",
      "Max gradient at step 1181: 0.09868846833705902\n",
      "Max gradient at step 1182: 0.19735224545001984\n",
      "Max gradient at step 1183: 0.16530472040176392\n",
      "Max gradient at step 1184: 0.1981474757194519\n",
      "Max gradient at step 1185: 0.2474256157875061\n",
      "Max gradient at step 1186: 0.11032959818840027\n",
      "Max gradient at step 1187: 0.20731325447559357\n",
      "Max gradient at step 1188: 0.2215575873851776\n",
      "Max gradient at step 1189: 0.17959482967853546\n",
      "Max gradient at step 1190: 0.17705807089805603\n",
      "Max gradient at step 1191: 0.23360814154148102\n",
      "Max gradient at step 1192: 0.14375364780426025\n",
      "Max gradient at step 1193: 0.14597995579242706\n",
      "Max gradient at step 1194: 0.15679171681404114\n",
      "Max gradient at step 1195: 0.11581655591726303\n",
      "Max gradient at step 1196: 0.17192697525024414\n",
      "Max gradient at step 1197: 0.16957467794418335\n",
      "Max gradient at step 1198: 0.10566479712724686\n",
      "Max gradient at step 1199: 0.1325843632221222\n",
      "Max gradient at step 1200: 0.2885008454322815\n",
      "Training loss (for one batch) at step 1200: 0.4227\n",
      "Max gradient at step 1201: 0.1424287110567093\n",
      "Max gradient at step 1202: 0.19819273054599762\n",
      "Max gradient at step 1203: 0.2967894673347473\n",
      "Max gradient at step 1204: 0.17505423724651337\n",
      "Max gradient at step 1205: 0.1912795901298523\n",
      "Max gradient at step 1206: 0.12962470948696136\n",
      "Max gradient at step 1207: 0.1394122987985611\n",
      "Max gradient at step 1208: 0.20750553905963898\n",
      "Max gradient at step 1209: 0.31999990344047546\n",
      "Max gradient at step 1210: 0.20506636798381805\n",
      "Max gradient at step 1211: 0.1312102973461151\n",
      "Max gradient at step 1212: 0.29484793543815613\n",
      "Max gradient at step 1213: 0.29813122749328613\n",
      "Max gradient at step 1214: 0.0922011286020279\n",
      "Max gradient at step 1215: 0.12688443064689636\n",
      "Max gradient at step 1216: 0.13332490622997284\n",
      "Max gradient at step 1217: 0.15987899899482727\n",
      "Max gradient at step 1218: 0.16495710611343384\n",
      "Max gradient at step 1219: 0.17959700524806976\n",
      "Max gradient at step 1220: 0.13106770813465118\n",
      "Max gradient at step 1221: 0.15598665177822113\n",
      "Max gradient at step 1222: 0.2331485003232956\n",
      "Max gradient at step 1223: 0.11406495422124863\n",
      "Max gradient at step 1224: 0.07507787644863129\n",
      "Max gradient at step 1225: 0.21766453981399536\n",
      "Max gradient at step 1226: 0.20448656380176544\n",
      "Max gradient at step 1227: 0.15142370760440826\n",
      "Max gradient at step 1228: 0.13429585099220276\n",
      "Max gradient at step 1229: 0.09251388907432556\n",
      "Max gradient at step 1230: 0.2612535059452057\n",
      "Max gradient at step 1231: 0.09855697304010391\n",
      "Max gradient at step 1232: 0.1889462172985077\n",
      "Max gradient at step 1233: 0.1816033273935318\n",
      "Max gradient at step 1234: 0.13176096975803375\n",
      "Max gradient at step 1235: 0.20903897285461426\n",
      "Max gradient at step 1236: 0.1561068296432495\n",
      "Max gradient at step 1237: 0.29352137446403503\n",
      "Max gradient at step 1238: 0.1877899467945099\n",
      "Max gradient at step 1239: 0.13172729313373566\n",
      "Max gradient at step 1240: 0.13198836147785187\n",
      "Max gradient at step 1241: 0.14365871250629425\n",
      "Max gradient at step 1242: 0.1969546228647232\n",
      "Max gradient at step 1243: 0.18925051391124725\n",
      "Max gradient at step 1244: 0.09985843300819397\n",
      "Max gradient at step 1245: 0.16415712237358093\n",
      "Max gradient at step 1246: 0.25861918926239014\n",
      "Max gradient at step 1247: 0.17687611281871796\n",
      "Max gradient at step 1248: 0.2989095449447632\n",
      "Max gradient at step 1249: 0.20979484915733337\n",
      "Max gradient at step 1250: 0.116744764149189\n",
      "Max gradient at step 1251: 0.2509929835796356\n",
      "Max gradient at step 1252: 0.20117180049419403\n",
      "Max gradient at step 1253: 0.2045161873102188\n",
      "Max gradient at step 1254: 0.1883755475282669\n",
      "Max gradient at step 1255: 0.12201400846242905\n",
      "Max gradient at step 1256: 0.1320025771856308\n",
      "Max gradient at step 1257: 0.2821079194545746\n",
      "Max gradient at step 1258: 0.1148751974105835\n",
      "Max gradient at step 1259: 0.1423918902873993\n",
      "Max gradient at step 1260: 0.11104422062635422\n",
      "Max gradient at step 1261: 0.18252620100975037\n",
      "Max gradient at step 1262: 0.26793771982192993\n",
      "Max gradient at step 1263: 0.15994825959205627\n",
      "Max gradient at step 1264: 0.135278582572937\n",
      "Max gradient at step 1265: 0.12229263782501221\n",
      "Max gradient at step 1266: 0.1828136295080185\n",
      "Max gradient at step 1267: 0.19691574573516846\n",
      "Max gradient at step 1268: 0.1904166340827942\n",
      "Max gradient at step 1269: 0.1541101485490799\n",
      "Max gradient at step 1270: 0.19713206589221954\n",
      "Max gradient at step 1271: 0.3107389807701111\n",
      "Max gradient at step 1272: 0.21924741566181183\n",
      "Max gradient at step 1273: 0.19606682658195496\n",
      "Max gradient at step 1274: 0.19436968863010406\n",
      "Max gradient at step 1275: 0.19555343687534332\n",
      "Max gradient at step 1276: 0.1251324713230133\n",
      "Max gradient at step 1277: 0.061217546463012695\n",
      "Max gradient at step 1278: 0.09150811284780502\n",
      "Max gradient at step 1279: 0.1380542814731598\n",
      "Max gradient at step 1280: 0.17329420149326324\n",
      "Max gradient at step 1281: 0.08642653375864029\n",
      "Max gradient at step 1282: 0.14185279607772827\n",
      "Max gradient at step 1283: 0.12824322283267975\n",
      "Max gradient at step 1284: 0.14866895973682404\n",
      "Max gradient at step 1285: 0.24570362269878387\n",
      "Max gradient at step 1286: 0.2786318361759186\n",
      "Max gradient at step 1287: 0.22687143087387085\n",
      "Max gradient at step 1288: 0.15845434367656708\n",
      "Max gradient at step 1289: 0.21711832284927368\n",
      "Max gradient at step 1290: 0.17318157851696014\n",
      "Max gradient at step 1291: 0.12661980092525482\n",
      "Max gradient at step 1292: 0.21225374937057495\n",
      "Max gradient at step 1293: 0.162551149725914\n",
      "Max gradient at step 1294: 0.33148789405822754\n",
      "Max gradient at step 1295: 0.21933814883232117\n",
      "Max gradient at step 1296: 0.21025870740413666\n",
      "Max gradient at step 1297: 0.19015781581401825\n",
      "Max gradient at step 1298: 0.10763772577047348\n",
      "Max gradient at step 1299: 0.394248902797699\n",
      "Max gradient at step 1300: 0.12380781024694443\n",
      "Training loss (for one batch) at step 1300: 0.3793\n",
      "Max gradient at step 1301: 0.2658780515193939\n",
      "Max gradient at step 1302: 0.201557457447052\n",
      "Max gradient at step 1303: 0.2937122583389282\n",
      "Max gradient at step 1304: 0.15531353652477264\n",
      "Max gradient at step 1305: 0.24569545686244965\n",
      "Max gradient at step 1306: 0.2930275499820709\n",
      "Max gradient at step 1307: 0.2891148626804352\n",
      "Max gradient at step 1308: 0.17254532873630524\n",
      "Max gradient at step 1309: 0.14468009769916534\n",
      "Max gradient at step 1310: 0.22026994824409485\n",
      "Max gradient at step 1311: 0.14706654846668243\n",
      "Max gradient at step 1312: 0.1761128157377243\n",
      "Max gradient at step 1313: 0.12031001597642899\n",
      "Max gradient at step 1314: 0.13763660192489624\n",
      "Max gradient at step 1315: 0.3744027316570282\n",
      "Max gradient at step 1316: 0.12362264841794968\n",
      "Max gradient at step 1317: 0.21972903609275818\n",
      "Max gradient at step 1318: 0.15375301241874695\n",
      "Max gradient at step 1319: 0.22482553124427795\n",
      "Max gradient at step 1320: 0.23240989446640015\n",
      "Max gradient at step 1321: 0.1412956416606903\n",
      "Max gradient at step 1322: 0.10208571702241898\n",
      "Max gradient at step 1323: 0.1207135021686554\n",
      "Max gradient at step 1324: 0.17221032083034515\n",
      "Max gradient at step 1325: 0.1649480164051056\n",
      "Max gradient at step 1326: 0.30513066053390503\n",
      "Max gradient at step 1327: 0.18170075118541718\n",
      "Max gradient at step 1328: 0.25477108359336853\n",
      "Max gradient at step 1329: 0.19137781858444214\n",
      "Max gradient at step 1330: 0.1520727425813675\n",
      "Max gradient at step 1331: 0.1459246128797531\n",
      "Max gradient at step 1332: 0.25432077050209045\n",
      "Max gradient at step 1333: 0.11891920119524002\n",
      "Max gradient at step 1334: 0.12390907108783722\n",
      "Max gradient at step 1335: 0.13975606858730316\n",
      "Max gradient at step 1336: 0.1056782677769661\n",
      "Max gradient at step 1337: 0.23380400240421295\n",
      "Max gradient at step 1338: 0.276780903339386\n",
      "Max gradient at step 1339: 0.193653866648674\n",
      "Max gradient at step 1340: 0.1312304586172104\n",
      "Max gradient at step 1341: 0.2199394851922989\n",
      "Max gradient at step 1342: 0.18639861047267914\n",
      "Max gradient at step 1343: 0.2847498059272766\n",
      "Max gradient at step 1344: 0.0571226105093956\n",
      "Max gradient at step 1345: 0.2962634861469269\n",
      "Max gradient at step 1346: 0.13758008182048798\n",
      "Max gradient at step 1347: 0.1935458481311798\n",
      "Max gradient at step 1348: 0.09399200230836868\n",
      "Max gradient at step 1349: 0.18857014179229736\n",
      "Max gradient at step 1350: 0.2004368156194687\n",
      "Max gradient at step 1351: 0.22675690054893494\n",
      "Max gradient at step 1352: 0.16263239085674286\n",
      "Max gradient at step 1353: 0.20276139676570892\n",
      "Max gradient at step 1354: 0.3057217597961426\n",
      "Max gradient at step 1355: 0.20428486168384552\n",
      "Max gradient at step 1356: 0.1571541130542755\n",
      "Max gradient at step 1357: 0.15885183215141296\n",
      "Max gradient at step 1358: 0.1264771968126297\n",
      "Max gradient at step 1359: 0.16294050216674805\n",
      "Max gradient at step 1360: 0.18344998359680176\n",
      "Max gradient at step 1361: 0.1886713057756424\n",
      "Max gradient at step 1362: 0.15428218245506287\n",
      "Max gradient at step 1363: 0.09548158943653107\n",
      "Max gradient at step 1364: 0.19524569809436798\n",
      "Max gradient at step 1365: 0.15351645648479462\n",
      "Max gradient at step 1366: 0.1401088833808899\n",
      "Max gradient at step 1367: 0.10472491383552551\n",
      "Max gradient at step 1368: 0.17541785538196564\n",
      "Max gradient at step 1369: 0.13105370104312897\n",
      "Max gradient at step 1370: 0.2892240583896637\n",
      "Max gradient at step 1371: 0.33240705728530884\n",
      "Max gradient at step 1372: 0.2189081758260727\n",
      "Max gradient at step 1373: 0.10805200040340424\n",
      "Max gradient at step 1374: 0.23843494057655334\n",
      "Max gradient at step 1375: 0.14923301339149475\n",
      "Max gradient at step 1376: 0.08971633017063141\n",
      "Max gradient at step 1377: 0.17034630477428436\n",
      "Max gradient at step 1378: 0.141283318400383\n",
      "Max gradient at step 1379: 0.3265230655670166\n",
      "Max gradient at step 1380: 0.22729285061359406\n",
      "Max gradient at step 1381: 0.10567755252122879\n",
      "Max gradient at step 1382: 0.15070046484470367\n",
      "Max gradient at step 1383: 0.17460675537586212\n",
      "Max gradient at step 1384: 0.2955031991004944\n",
      "Max gradient at step 1385: 0.1558735966682434\n",
      "Max gradient at step 1386: 0.27208730578422546\n",
      "Max gradient at step 1387: 0.14282090961933136\n",
      "Max gradient at step 1388: 0.19295701384544373\n",
      "Max gradient at step 1389: 0.1614009439945221\n",
      "Max gradient at step 1390: 0.14404131472110748\n",
      "Max gradient at step 1391: 0.12575052678585052\n",
      "Max gradient at step 1392: 0.1170852854847908\n",
      "Max gradient at step 1393: 0.2002929449081421\n",
      "Max gradient at step 1394: 0.08966480195522308\n",
      "Max gradient at step 1395: 0.1324705332517624\n",
      "Max gradient at step 1396: 0.14856667816638947\n",
      "Max gradient at step 1397: 0.24838010966777802\n",
      "Max gradient at step 1398: 0.17468298971652985\n",
      "Max gradient at step 1399: 0.18607470393180847\n",
      "Max gradient at step 1400: 0.140866219997406\n",
      "Training loss (for one batch) at step 1400: 0.3014\n",
      "Max gradient at step 1401: 0.21846219897270203\n",
      "Max gradient at step 1402: 0.096133753657341\n",
      "Max gradient at step 1403: 0.158964142203331\n",
      "Max gradient at step 1404: 0.10913532972335815\n",
      "Max gradient at step 1405: 0.23593750596046448\n",
      "Max gradient at step 1406: 0.3274374306201935\n",
      "Max gradient at step 1407: 0.1771056205034256\n",
      "Max gradient at step 1408: 0.13060012459754944\n",
      "Max gradient at step 1409: 0.12239192426204681\n",
      "Max gradient at step 1410: 0.13479028642177582\n",
      "Max gradient at step 1411: 0.14528904855251312\n",
      "Max gradient at step 1412: 0.18030987679958344\n",
      "Max gradient at step 1413: 0.21335169672966003\n",
      "Max gradient at step 1414: 0.11927895992994308\n",
      "Max gradient at step 1415: 0.07974772155284882\n",
      "Max gradient at step 1416: 0.1539376974105835\n",
      "Max gradient at step 1417: 0.23457372188568115\n",
      "Max gradient at step 1418: 0.4704033136367798\n",
      "Max gradient at step 1419: 0.16395451128482819\n",
      "Max gradient at step 1420: 0.28807997703552246\n",
      "Max gradient at step 1421: 0.1778927892446518\n",
      "Max gradient at step 1422: 0.11932564526796341\n",
      "Max gradient at step 1423: 0.13253462314605713\n",
      "Max gradient at step 1424: 0.16152456402778625\n",
      "Max gradient at step 1425: 0.07973496615886688\n",
      "Max gradient at step 1426: 0.22539611160755157\n",
      "Max gradient at step 1427: 0.11789505183696747\n",
      "Max gradient at step 1428: 0.13187746703624725\n",
      "Max gradient at step 1429: 0.0718182697892189\n",
      "Max gradient at step 1430: 0.19352948665618896\n",
      "Max gradient at step 1431: 0.1778191179037094\n",
      "Max gradient at step 1432: 0.29032522439956665\n",
      "Max gradient at step 1433: 0.2595410943031311\n",
      "Max gradient at step 1434: 0.14028948545455933\n",
      "Max gradient at step 1435: 0.22549062967300415\n",
      "Max gradient at step 1436: 0.10817521065473557\n",
      "Max gradient at step 1437: 0.1796802282333374\n",
      "Max gradient at step 1438: 0.2628594934940338\n",
      "Max gradient at step 1439: 0.18018867075443268\n",
      "Max gradient at step 1440: 0.19443704187870026\n",
      "Max gradient at step 1441: 0.11769627779722214\n",
      "Max gradient at step 1442: 0.1892022043466568\n",
      "Max gradient at step 1443: 0.1433083713054657\n",
      "Max gradient at step 1444: 0.20865602791309357\n",
      "Max gradient at step 1445: 0.16563892364501953\n",
      "Max gradient at step 1446: 0.20705272257328033\n",
      "Max gradient at step 1447: 0.11843565851449966\n",
      "Max gradient at step 1448: 0.26135674118995667\n",
      "Max gradient at step 1449: 0.1886748969554901\n",
      "Max gradient at step 1450: 0.13969500362873077\n",
      "Max gradient at step 1451: 0.11665347218513489\n",
      "Max gradient at step 1452: 0.2284364104270935\n",
      "Max gradient at step 1453: 0.1979854851961136\n",
      "Max gradient at step 1454: 0.19610603153705597\n",
      "Max gradient at step 1455: 0.1572895348072052\n",
      "Max gradient at step 1456: 0.21712003648281097\n",
      "Max gradient at step 1457: 0.20100706815719604\n",
      "Max gradient at step 1458: 0.3047772943973541\n",
      "Max gradient at step 1459: 0.15286432206630707\n",
      "Max gradient at step 1460: 0.30499929189682007\n",
      "Max gradient at step 1461: 0.29131150245666504\n",
      "Max gradient at step 1462: 0.1082373559474945\n",
      "Max gradient at step 1463: 0.10586241632699966\n",
      "Max gradient at step 1464: 0.14537878334522247\n",
      "Max gradient at step 1465: 0.1681380718946457\n",
      "Max gradient at step 1466: 0.16488638520240784\n",
      "Max gradient at step 1467: 0.1434401273727417\n",
      "Max gradient at step 1468: 0.0849946141242981\n",
      "Max gradient at step 1469: 0.24334794282913208\n",
      "Max gradient at step 1470: 0.16618718206882477\n",
      "Max gradient at step 1471: 0.1119753047823906\n",
      "Max gradient at step 1472: 0.18336737155914307\n",
      "Max gradient at step 1473: 0.1338663399219513\n",
      "Max gradient at step 1474: 0.08339522778987885\n",
      "Max gradient at step 1475: 0.19464899599552155\n",
      "Max gradient at step 1476: 0.28219056129455566\n",
      "Max gradient at step 1477: 0.18991227447986603\n",
      "Max gradient at step 1478: 0.1873660534620285\n",
      "Max gradient at step 1479: 0.10290130227804184\n",
      "Max gradient at step 1480: 0.12213926762342453\n",
      "Max gradient at step 1481: 0.13709606230258942\n",
      "Max gradient at step 1482: 0.11933717876672745\n",
      "Max gradient at step 1483: 0.1551331877708435\n",
      "Max gradient at step 1484: 0.14613384008407593\n",
      "Max gradient at step 1485: 0.13204416632652283\n",
      "Max gradient at step 1486: 0.11947498470544815\n",
      "Max gradient at step 1487: 0.10482840240001678\n",
      "Max gradient at step 1488: 0.14426474273204803\n",
      "Max gradient at step 1489: 0.1486133635044098\n",
      "Max gradient at step 1490: 0.21479396522045135\n",
      "Max gradient at step 1491: 0.18817833065986633\n",
      "Max gradient at step 1492: 0.08343125134706497\n",
      "Max gradient at step 1493: 0.17404116690158844\n",
      "Max gradient at step 1494: 0.20822294056415558\n",
      "Max gradient at step 1495: 0.12071952223777771\n",
      "Max gradient at step 1496: 0.07083655148744583\n",
      "Max gradient at step 1497: 0.165010467171669\n",
      "Max gradient at step 1498: 0.20981687307357788\n",
      "Max gradient at step 1499: 0.2060813456773758\n",
      "Max gradient at step 1500: 0.09870896488428116\n",
      "Training loss (for one batch) at step 1500: 0.1633\n",
      "Max gradient at step 1501: 0.13876737654209137\n",
      "Max gradient at step 1502: 0.16771693527698517\n",
      "Max gradient at step 1503: 0.14644084870815277\n",
      "Max gradient at step 1504: 0.1352953165769577\n",
      "Max gradient at step 1505: 0.15609058737754822\n",
      "Max gradient at step 1506: 0.13126766681671143\n",
      "Max gradient at step 1507: 0.14705702662467957\n",
      "Max gradient at step 1508: 0.06124340742826462\n",
      "Max gradient at step 1509: 0.2732437551021576\n",
      "Max gradient at step 1510: 0.21420352160930634\n",
      "Max gradient at step 1511: 0.19544297456741333\n",
      "Max gradient at step 1512: 0.20850421488285065\n",
      "Max gradient at step 1513: 0.18371227383613586\n",
      "Max gradient at step 1514: 0.23813606798648834\n",
      "Max gradient at step 1515: 0.16854286193847656\n",
      "Max gradient at step 1516: 0.2045327126979828\n",
      "Max gradient at step 1517: 0.17504067718982697\n",
      "Max gradient at step 1518: 0.16730497777462006\n",
      "Max gradient at step 1519: 0.1422153115272522\n",
      "Max gradient at step 1520: 0.2171919196844101\n",
      "Max gradient at step 1521: 0.13664937019348145\n",
      "Max gradient at step 1522: 0.1688273400068283\n",
      "Max gradient at step 1523: 0.17283131182193756\n",
      "Max gradient at step 1524: 0.09463942050933838\n",
      "Max gradient at step 1525: 0.17110008001327515\n",
      "Max gradient at step 1526: 0.1968022882938385\n",
      "Max gradient at step 1527: 0.2119467854499817\n",
      "Max gradient at step 1528: 0.19353647530078888\n",
      "Max gradient at step 1529: 0.13726477324962616\n",
      "Max gradient at step 1530: 0.16786091029644012\n",
      "Max gradient at step 1531: 0.1760910004377365\n",
      "Max gradient at step 1532: 0.17589624226093292\n",
      "Max gradient at step 1533: 0.24894818663597107\n",
      "Max gradient at step 1534: 0.2782064378261566\n",
      "Max gradient at step 1535: 0.13567599654197693\n",
      "Max gradient at step 1536: 0.240215465426445\n",
      "Max gradient at step 1537: 0.2079135775566101\n",
      "Max gradient at step 1538: 0.21172110736370087\n",
      "Max gradient at step 1539: 0.16632309556007385\n",
      "Max gradient at step 1540: 0.2324429750442505\n",
      "Max gradient at step 1541: 0.11171174049377441\n",
      "Max gradient at step 1542: 0.19831055402755737\n",
      "Max gradient at step 1543: 0.12193852663040161\n",
      "Max gradient at step 1544: 0.17513997852802277\n",
      "Max gradient at step 1545: 0.2576349973678589\n",
      "Max gradient at step 1546: 0.11821752786636353\n",
      "Max gradient at step 1547: 0.14284388720989227\n",
      "Max gradient at step 1548: 0.12328467518091202\n",
      "Max gradient at step 1549: 0.22880567610263824\n",
      "Max gradient at step 1550: 0.1621452271938324\n",
      "Max gradient at step 1551: 0.14889591932296753\n",
      "Max gradient at step 1552: 0.182218536734581\n",
      "Max gradient at step 1553: 0.08638113737106323\n",
      "Max gradient at step 1554: 0.2251938134431839\n",
      "Max gradient at step 1555: 0.1651591807603836\n",
      "Max gradient at step 1556: 0.18832729756832123\n",
      "Max gradient at step 1557: 0.08356769382953644\n",
      "Max gradient at step 1558: 0.1592002660036087\n",
      "Max gradient at step 1559: 0.11772889643907547\n",
      "Max gradient at step 1560: 0.18880772590637207\n",
      "Max gradient at step 1561: 0.1669531762599945\n",
      "Max gradient at step 1562: 0.2620830833911896\n",
      "Max gradient at step 1563: 0.13830150663852692\n",
      "Max gradient at step 1564: 0.24040229618549347\n",
      "Max gradient at step 1565: 0.17036348581314087\n",
      "Max gradient at step 1566: 0.20749332010746002\n",
      "Max gradient at step 1567: 0.12965357303619385\n",
      "Max gradient at step 1568: 0.13503776490688324\n",
      "Max gradient at step 1569: 0.09446711838245392\n",
      "Max gradient at step 1570: 0.2597503066062927\n",
      "Max gradient at step 1571: 0.1744508296251297\n",
      "Max gradient at step 1572: 0.219703808426857\n",
      "Max gradient at step 1573: 0.08418381214141846\n",
      "Max gradient at step 1574: 0.22219136357307434\n",
      "Max gradient at step 1575: 0.14102362096309662\n",
      "Max gradient at step 1576: 0.13464027643203735\n",
      "Max gradient at step 1577: 0.17018017172813416\n",
      "Max gradient at step 1578: 0.23347274959087372\n",
      "Max gradient at step 1579: 0.13161250948905945\n",
      "Max gradient at step 1580: 0.17332620918750763\n",
      "Max gradient at step 1581: 0.08353866636753082\n",
      "Max gradient at step 1582: 0.21142245829105377\n",
      "Max gradient at step 1583: 0.25666505098342896\n",
      "Max gradient at step 1584: 0.0931389331817627\n",
      "Max gradient at step 1585: 0.09011600911617279\n",
      "Max gradient at step 1586: 0.26962846517562866\n",
      "Max gradient at step 1587: 0.23361366987228394\n",
      "Max gradient at step 1588: 0.0996178537607193\n",
      "Max gradient at step 1589: 0.15953798592090607\n",
      "Max gradient at step 1590: 0.16931065917015076\n",
      "Max gradient at step 1591: 0.3030262291431427\n",
      "Max gradient at step 1592: 0.1407088041305542\n",
      "Max gradient at step 1593: 0.2732214033603668\n",
      "Max gradient at step 1594: 0.11837279051542282\n",
      "Max gradient at step 1595: 0.17886310815811157\n",
      "Max gradient at step 1596: 0.25384047627449036\n",
      "Max gradient at step 1597: 0.20607306063175201\n",
      "Max gradient at step 1598: 0.21024954319000244\n",
      "Max gradient at step 1599: 0.24912029504776\n",
      "Max gradient at step 1600: 0.23656931519508362\n",
      "Training loss (for one batch) at step 1600: 0.2499\n",
      "Max gradient at step 1601: 0.11907240748405457\n",
      "Max gradient at step 1602: 0.1447048783302307\n",
      "Max gradient at step 1603: 0.17735382914543152\n",
      "Max gradient at step 1604: 0.20407593250274658\n",
      "Max gradient at step 1605: 0.07730841636657715\n",
      "Max gradient at step 1606: 0.11374703794717789\n",
      "Max gradient at step 1607: 0.14184467494487762\n",
      "Max gradient at step 1608: 0.11235734820365906\n",
      "Max gradient at step 1609: 0.17581206560134888\n",
      "Max gradient at step 1610: 0.1400379240512848\n",
      "Max gradient at step 1611: 0.11832510679960251\n",
      "Max gradient at step 1612: 0.1241307482123375\n",
      "Max gradient at step 1613: 0.14230647683143616\n",
      "Max gradient at step 1614: 0.1895863562822342\n",
      "Max gradient at step 1615: 0.20954468846321106\n",
      "Max gradient at step 1616: 0.31881532073020935\n",
      "Max gradient at step 1617: 0.1613907814025879\n",
      "Max gradient at step 1618: 0.15400293469429016\n",
      "Max gradient at step 1619: 0.1372000128030777\n",
      "Max gradient at step 1620: 0.16190341114997864\n",
      "Max gradient at step 1621: 0.09384123980998993\n",
      "Max gradient at step 1622: 0.17636749148368835\n",
      "Max gradient at step 1623: 0.27850788831710815\n",
      "Max gradient at step 1624: 0.15374182164669037\n",
      "Max gradient at step 1625: 0.12797391414642334\n",
      "Max gradient at step 1626: 0.1754901111125946\n",
      "Max gradient at step 1627: 0.1077374741435051\n",
      "Max gradient at step 1628: 0.19389905035495758\n",
      "Max gradient at step 1629: 0.17828315496444702\n",
      "Max gradient at step 1630: 0.22358179092407227\n",
      "Max gradient at step 1631: 0.22643274068832397\n",
      "Max gradient at step 1632: 0.22636911273002625\n",
      "Max gradient at step 1633: 0.12168287485837936\n",
      "Max gradient at step 1634: 0.17814107239246368\n",
      "Max gradient at step 1635: 0.23573368787765503\n",
      "Max gradient at step 1636: 0.26424843072891235\n",
      "Max gradient at step 1637: 0.2843841314315796\n",
      "Max gradient at step 1638: 0.11671022325754166\n",
      "Max gradient at step 1639: 0.16603530943393707\n",
      "Max gradient at step 1640: 0.10168380290269852\n",
      "Max gradient at step 1641: 0.093304343521595\n",
      "Max gradient at step 1642: 0.12273537367582321\n",
      "Max gradient at step 1643: 0.25971367955207825\n",
      "Max gradient at step 1644: 0.13660846650600433\n",
      "Max gradient at step 1645: 0.1483919322490692\n",
      "Max gradient at step 1646: 0.19454824924468994\n",
      "Max gradient at step 1647: 0.2094130665063858\n",
      "Max gradient at step 1648: 0.17347896099090576\n",
      "Max gradient at step 1649: 0.1183907762169838\n",
      "Max gradient at step 1650: 0.08235397934913635\n",
      "Max gradient at step 1651: 0.16484799981117249\n",
      "Max gradient at step 1652: 0.2211647778749466\n",
      "Max gradient at step 1653: 0.16200768947601318\n",
      "Max gradient at step 1654: 0.11410170793533325\n",
      "Max gradient at step 1655: 0.12822942435741425\n",
      "Max gradient at step 1656: 0.23468224704265594\n",
      "Max gradient at step 1657: 0.14248232543468475\n",
      "Max gradient at step 1658: 0.21681593358516693\n",
      "Max gradient at step 1659: 0.1650591343641281\n",
      "Max gradient at step 1660: 0.11094833165407181\n",
      "Max gradient at step 1661: 0.17236799001693726\n",
      "Max gradient at step 1662: 0.28624629974365234\n",
      "Max gradient at step 1663: 0.2108515053987503\n",
      "Max gradient at step 1664: 0.1324862688779831\n",
      "Max gradient at step 1665: 0.18805672228336334\n",
      "Max gradient at step 1666: 0.1617342233657837\n",
      "Max gradient at step 1667: 0.11833511292934418\n",
      "Max gradient at step 1668: 0.13292312622070312\n",
      "Max gradient at step 1669: 0.2353011816740036\n",
      "Max gradient at step 1670: 0.20914940536022186\n",
      "Max gradient at step 1671: 0.071858249604702\n",
      "Max gradient at step 1672: 0.12136362493038177\n",
      "Max gradient at step 1673: 0.270458459854126\n",
      "Max gradient at step 1674: 0.22331717610359192\n",
      "Max gradient at step 1675: 0.13498112559318542\n",
      "Max gradient at step 1676: 0.11851181089878082\n",
      "Max gradient at step 1677: 0.18146127462387085\n",
      "Max gradient at step 1678: 0.12355592846870422\n",
      "Max gradient at step 1679: 0.20006069540977478\n",
      "Max gradient at step 1680: 0.20925769209861755\n",
      "Max gradient at step 1681: 0.18225565552711487\n",
      "Max gradient at step 1682: 0.1842970848083496\n",
      "Max gradient at step 1683: 0.09621622413396835\n",
      "Max gradient at step 1684: 0.21011705696582794\n",
      "Max gradient at step 1685: 0.15613721311092377\n",
      "Max gradient at step 1686: 0.1616518646478653\n",
      "Max gradient at step 1687: 0.3056977093219757\n",
      "Max gradient at step 1688: 0.1558331400156021\n",
      "Max gradient at step 1689: 0.17969265580177307\n",
      "Max gradient at step 1690: 0.0979212298989296\n",
      "Max gradient at step 1691: 0.17218667268753052\n",
      "Max gradient at step 1692: 0.12659567594528198\n",
      "Max gradient at step 1693: 0.17134256660938263\n",
      "Max gradient at step 1694: 0.19573085010051727\n",
      "Max gradient at step 1695: 0.21816354990005493\n",
      "Max gradient at step 1696: 0.16879549622535706\n",
      "Max gradient at step 1697: 0.1590995043516159\n",
      "Max gradient at step 1698: 0.11213929951190948\n",
      "Max gradient at step 1699: 0.2087593525648117\n",
      "Max gradient at step 1700: 0.17665578424930573\n",
      "Training loss (for one batch) at step 1700: 0.1239\n",
      "Max gradient at step 1701: 0.12628865242004395\n",
      "Max gradient at step 1702: 0.10941570997238159\n",
      "Max gradient at step 1703: 0.13626760244369507\n",
      "Max gradient at step 1704: 0.06208117678761482\n",
      "Max gradient at step 1705: 0.1429886370897293\n",
      "Max gradient at step 1706: 0.23006846010684967\n",
      "Max gradient at step 1707: 0.13750839233398438\n",
      "Max gradient at step 1708: 0.17352324724197388\n",
      "Max gradient at step 1709: 0.201348215341568\n",
      "Max gradient at step 1710: 0.11479397863149643\n",
      "Max gradient at step 1711: 0.2812705934047699\n",
      "Max gradient at step 1712: 0.1988391876220703\n",
      "Max gradient at step 1713: 0.15044473111629486\n",
      "Max gradient at step 1714: 0.14168958365917206\n",
      "Max gradient at step 1715: 0.12708745896816254\n",
      "Max gradient at step 1716: 0.1349986344575882\n",
      "Max gradient at step 1717: 0.1263401061296463\n",
      "Max gradient at step 1718: 0.10771097987890244\n",
      "Max gradient at step 1719: 0.10953111946582794\n",
      "Max gradient at step 1720: 0.12026451528072357\n",
      "Max gradient at step 1721: 0.21902459859848022\n",
      "Max gradient at step 1722: 0.1530497670173645\n",
      "Max gradient at step 1723: 0.13768085837364197\n",
      "Max gradient at step 1724: 0.18076366186141968\n",
      "Max gradient at step 1725: 0.15913288295269012\n",
      "Max gradient at step 1726: 0.21793651580810547\n",
      "Max gradient at step 1727: 0.3826247751712799\n",
      "Max gradient at step 1728: 0.22340457141399384\n",
      "Max gradient at step 1729: 0.1385374516248703\n",
      "Max gradient at step 1730: 0.15823696553707123\n",
      "Max gradient at step 1731: 0.09314505755901337\n",
      "Max gradient at step 1732: 0.15613004565238953\n",
      "Max gradient at step 1733: 0.19421789050102234\n",
      "Max gradient at step 1734: 0.12003407627344131\n",
      "Max gradient at step 1735: 0.0849975198507309\n",
      "Max gradient at step 1736: 0.17904295027256012\n",
      "Max gradient at step 1737: 0.18575601279735565\n",
      "Max gradient at step 1738: 0.11117776483297348\n",
      "Max gradient at step 1739: 0.22423553466796875\n",
      "Max gradient at step 1740: 0.14222483336925507\n",
      "Max gradient at step 1741: 0.2183717042207718\n",
      "Max gradient at step 1742: 0.18217384815216064\n",
      "Max gradient at step 1743: 0.17196018993854523\n",
      "Max gradient at step 1744: 0.12627173960208893\n",
      "Max gradient at step 1745: 0.1806136667728424\n",
      "Max gradient at step 1746: 0.16400937736034393\n",
      "Max gradient at step 1747: 0.17183761298656464\n",
      "Max gradient at step 1748: 0.11794890463352203\n",
      "Max gradient at step 1749: 0.12595242261886597\n",
      "Max gradient at step 1750: 0.1290266066789627\n",
      "Max gradient at step 1751: 0.13293981552124023\n",
      "Max gradient at step 1752: 0.11358216404914856\n",
      "Max gradient at step 1753: 0.17110443115234375\n",
      "Max gradient at step 1754: 0.09548714756965637\n",
      "Max gradient at step 1755: 0.0658932626247406\n",
      "Max gradient at step 1756: 0.10955268889665604\n",
      "Max gradient at step 1757: 0.1811012327671051\n",
      "Max gradient at step 1758: 0.12173749506473541\n",
      "Max gradient at step 1759: 0.1514715701341629\n",
      "Max gradient at step 1760: 0.14350078999996185\n",
      "Max gradient at step 1761: 0.13582348823547363\n",
      "Max gradient at step 1762: 0.17698541283607483\n",
      "Max gradient at step 1763: 0.1235431581735611\n",
      "Max gradient at step 1764: 0.18162254989147186\n",
      "Max gradient at step 1765: 0.09065312147140503\n",
      "Max gradient at step 1766: 0.14300350844860077\n",
      "Max gradient at step 1767: 0.13745863735675812\n",
      "Max gradient at step 1768: 0.2181238979101181\n",
      "Max gradient at step 1769: 0.11055915802717209\n",
      "Max gradient at step 1770: 0.09956111758947372\n",
      "Max gradient at step 1771: 0.24527040123939514\n",
      "Max gradient at step 1772: 0.131850928068161\n",
      "Max gradient at step 1773: 0.24796849489212036\n",
      "Max gradient at step 1774: 0.12175321578979492\n",
      "Max gradient at step 1775: 0.12223567068576813\n",
      "Max gradient at step 1776: 0.162194162607193\n",
      "Max gradient at step 1777: 0.26487600803375244\n",
      "Max gradient at step 1778: 0.4794696271419525\n",
      "Max gradient at step 1779: 0.1972157061100006\n",
      "Max gradient at step 1780: 0.3083846867084503\n",
      "Max gradient at step 1781: 0.11849258840084076\n",
      "Max gradient at step 1782: 0.13165293633937836\n",
      "Max gradient at step 1783: 0.15321402251720428\n",
      "Max gradient at step 1784: 0.13541892170906067\n",
      "Max gradient at step 1785: 0.16432999074459076\n",
      "Max gradient at step 1786: 0.12244481593370438\n",
      "Max gradient at step 1787: 0.2144940346479416\n",
      "Max gradient at step 1788: 0.14264991879463196\n",
      "Max gradient at step 1789: 0.07532723993062973\n",
      "Max gradient at step 1790: 0.24516969919204712\n",
      "Max gradient at step 1791: 0.17574763298034668\n",
      "Max gradient at step 1792: 0.2701413631439209\n",
      "Max gradient at step 1793: 0.15334337949752808\n",
      "Max gradient at step 1794: 0.13885195553302765\n",
      "Max gradient at step 1795: 0.21701152622699738\n",
      "Max gradient at step 1796: 0.22701679170131683\n",
      "Max gradient at step 1797: 0.14855721592903137\n",
      "Max gradient at step 1798: 0.08613134920597076\n",
      "Max gradient at step 1799: 0.16698068380355835\n",
      "Max gradient at step 1800: 0.17534902691841125\n",
      "Training loss (for one batch) at step 1800: 0.2339\n",
      "Max gradient at step 1801: 0.21165111660957336\n",
      "Max gradient at step 1802: 0.12485674768686295\n",
      "Max gradient at step 1803: 0.2227635532617569\n",
      "Max gradient at step 1804: 0.13952766358852386\n",
      "Max gradient at step 1805: 0.08631657063961029\n",
      "Max gradient at step 1806: 0.16132113337516785\n",
      "Max gradient at step 1807: 0.13357163965702057\n",
      "Max gradient at step 1808: 0.10249374806880951\n",
      "Max gradient at step 1809: 0.08778198063373566\n",
      "Max gradient at step 1810: 0.09395492821931839\n",
      "Max gradient at step 1811: 0.2636964023113251\n",
      "Max gradient at step 1812: 0.2469654530286789\n",
      "Max gradient at step 1813: 0.09892713278532028\n",
      "Max gradient at step 1814: 0.12837137281894684\n",
      "Max gradient at step 1815: 0.1346786916255951\n",
      "Max gradient at step 1816: 0.13671600818634033\n",
      "Max gradient at step 1817: 0.10210510343313217\n",
      "Max gradient at step 1818: 0.11813615262508392\n",
      "Max gradient at step 1819: 0.1648675501346588\n",
      "Max gradient at step 1820: 0.2438315600156784\n",
      "Max gradient at step 1821: 0.07801362127065659\n",
      "Max gradient at step 1822: 0.2594362199306488\n",
      "Max gradient at step 1823: 0.11535222083330154\n",
      "Max gradient at step 1824: 0.14808595180511475\n",
      "Max gradient at step 1825: 0.12848570942878723\n",
      "Max gradient at step 1826: 0.13494758307933807\n",
      "Max gradient at step 1827: 0.13867788016796112\n",
      "Max gradient at step 1828: 0.15218929946422577\n",
      "Max gradient at step 1829: 0.19642044603824615\n",
      "Max gradient at step 1830: 0.15158644318580627\n",
      "Max gradient at step 1831: 0.04809005185961723\n",
      "Max gradient at step 1832: 0.173758402466774\n",
      "Max gradient at step 1833: 0.3009188175201416\n",
      "Max gradient at step 1834: 0.13993597030639648\n",
      "Max gradient at step 1835: 0.15708208084106445\n",
      "Max gradient at step 1836: 0.18179866671562195\n",
      "Max gradient at step 1837: 0.06107357144355774\n",
      "Max gradient at step 1838: 0.1912415474653244\n",
      "Max gradient at step 1839: 0.11250147968530655\n",
      "Max gradient at step 1840: 0.09711568802595139\n",
      "Max gradient at step 1841: 0.20848459005355835\n",
      "Max gradient at step 1842: 0.09818345308303833\n",
      "Max gradient at step 1843: 0.21186909079551697\n",
      "Max gradient at step 1844: 0.09135401248931885\n",
      "Max gradient at step 1845: 0.1693439781665802\n",
      "Max gradient at step 1846: 0.25211086869239807\n",
      "Max gradient at step 1847: 0.15266069769859314\n",
      "Max gradient at step 1848: 0.11360100656747818\n",
      "Max gradient at step 1849: 0.10456804931163788\n",
      "Max gradient at step 1850: 0.11572739481925964\n",
      "Max gradient at step 1851: 0.17429353296756744\n",
      "Max gradient at step 1852: 0.18901637196540833\n",
      "Max gradient at step 1853: 0.0440167598426342\n",
      "Max gradient at step 1854: 0.09902801364660263\n",
      "Max gradient at step 1855: 0.17410659790039062\n",
      "Max gradient at step 1856: 0.0436815544962883\n",
      "Max gradient at step 1857: 0.1335388571023941\n",
      "Max gradient at step 1858: 0.16065962612628937\n",
      "Max gradient at step 1859: 0.07398159056901932\n",
      "Max gradient at step 1860: 0.07120541483163834\n",
      "Max gradient at step 1861: 0.13464951515197754\n",
      "Max gradient at step 1862: 0.1563384234905243\n",
      "Max gradient at step 1863: 0.26132985949516296\n",
      "Max gradient at step 1864: 0.2192976176738739\n",
      "Max gradient at step 1865: 0.21713624894618988\n",
      "Max gradient at step 1866: 0.11460091173648834\n",
      "Max gradient at step 1867: 0.1276969462633133\n",
      "Max gradient at step 1868: 0.1341194063425064\n",
      "Max gradient at step 1869: 0.05625443905591965\n",
      "Max gradient at step 1870: 0.10158443450927734\n",
      "Max gradient at step 1871: 0.17751812934875488\n",
      "Max gradient at step 1872: 0.11827404052019119\n",
      "Max gradient at step 1873: 0.26814353466033936\n",
      "Max gradient at step 1874: 0.18018202483654022\n",
      "Epoch 2/4\n",
      "Max gradient at step 0: 0.15121327340602875\n",
      "Training loss (for one batch) at step 0: 0.2998\n",
      "Max gradient at step 1: 0.22611267864704132\n",
      "Max gradient at step 2: 0.09344973415136337\n",
      "Max gradient at step 3: 0.3131665885448456\n",
      "Max gradient at step 4: 0.1508987843990326\n",
      "Max gradient at step 5: 0.21766674518585205\n",
      "Max gradient at step 6: 0.19530139863491058\n",
      "Max gradient at step 7: 0.16990956664085388\n",
      "Max gradient at step 8: 0.15089423954486847\n",
      "Max gradient at step 9: 0.19172769784927368\n",
      "Max gradient at step 10: 0.128476083278656\n",
      "Max gradient at step 11: 0.23196201026439667\n",
      "Max gradient at step 12: 0.41790083050727844\n",
      "Max gradient at step 13: 0.17115658521652222\n",
      "Max gradient at step 14: 0.16103889048099518\n",
      "Max gradient at step 15: 0.1457679569721222\n",
      "Max gradient at step 16: 0.28272151947021484\n",
      "Max gradient at step 17: 0.09561654180288315\n",
      "Max gradient at step 18: 0.13611803948879242\n",
      "Max gradient at step 19: 0.2211546152830124\n",
      "Max gradient at step 20: 0.14461548626422882\n",
      "Max gradient at step 21: 0.12368428707122803\n",
      "Max gradient at step 22: 0.1550900638103485\n",
      "Max gradient at step 23: 0.1545521467924118\n",
      "Max gradient at step 24: 0.14923350512981415\n",
      "Max gradient at step 25: 0.2063043713569641\n",
      "Max gradient at step 26: 0.15403470396995544\n",
      "Max gradient at step 27: 0.15007217228412628\n",
      "Max gradient at step 28: 0.17643065750598907\n",
      "Max gradient at step 29: 0.21592912077903748\n",
      "Max gradient at step 30: 0.13389980792999268\n",
      "Max gradient at step 31: 0.12153669446706772\n",
      "Max gradient at step 32: 0.4302356541156769\n",
      "Max gradient at step 33: 0.22662535309791565\n",
      "Max gradient at step 34: 0.17155274748802185\n",
      "Max gradient at step 35: 0.1709979921579361\n",
      "Max gradient at step 36: 0.13118010759353638\n",
      "Max gradient at step 37: 0.16988009214401245\n",
      "Max gradient at step 38: 0.25242680311203003\n",
      "Max gradient at step 39: 0.13364580273628235\n",
      "Max gradient at step 40: 0.2123316377401352\n",
      "Max gradient at step 41: 0.16854248940944672\n",
      "Max gradient at step 42: 0.12801405787467957\n",
      "Max gradient at step 43: 0.17286571860313416\n",
      "Max gradient at step 44: 0.1669730395078659\n",
      "Max gradient at step 45: 0.13921517133712769\n",
      "Max gradient at step 46: 0.15907831490039825\n",
      "Max gradient at step 47: 0.05554795637726784\n",
      "Max gradient at step 48: 0.15730534493923187\n",
      "Max gradient at step 49: 0.2478589564561844\n",
      "Max gradient at step 50: 0.11336712539196014\n",
      "Max gradient at step 51: 0.2969629764556885\n",
      "Max gradient at step 52: 0.2357599437236786\n",
      "Max gradient at step 53: 0.08311870694160461\n",
      "Max gradient at step 54: 0.11227979511022568\n",
      "Max gradient at step 55: 0.2266402542591095\n",
      "Max gradient at step 56: 0.15982013940811157\n",
      "Max gradient at step 57: 0.2065085917711258\n",
      "Max gradient at step 58: 0.11399367451667786\n",
      "Max gradient at step 59: 0.09450390934944153\n",
      "Max gradient at step 60: 0.13607312738895416\n",
      "Max gradient at step 61: 0.30382299423217773\n",
      "Max gradient at step 62: 0.15217779576778412\n",
      "Max gradient at step 63: 0.19918885827064514\n",
      "Max gradient at step 64: 0.14823076128959656\n",
      "Max gradient at step 65: 0.059927601367235184\n",
      "Max gradient at step 66: 0.1640084683895111\n",
      "Max gradient at step 67: 0.2142871618270874\n",
      "Max gradient at step 68: 0.12921054661273956\n",
      "Max gradient at step 69: 0.13439007103443146\n",
      "Max gradient at step 70: 0.2633378505706787\n",
      "Max gradient at step 71: 0.1595914214849472\n",
      "Max gradient at step 72: 0.23599208891391754\n",
      "Max gradient at step 73: 0.11007341742515564\n",
      "Max gradient at step 74: 0.13630011677742004\n",
      "Max gradient at step 75: 0.2367931604385376\n",
      "Max gradient at step 76: 0.07124672830104828\n",
      "Max gradient at step 77: 0.11145995557308197\n",
      "Max gradient at step 78: 0.22976331412792206\n",
      "Max gradient at step 79: 0.2255811244249344\n",
      "Max gradient at step 80: 0.15066316723823547\n",
      "Max gradient at step 81: 0.12469688802957535\n",
      "Max gradient at step 82: 0.11797729134559631\n",
      "Max gradient at step 83: 0.1313297152519226\n",
      "Max gradient at step 84: 0.23388558626174927\n",
      "Max gradient at step 85: 0.08656299114227295\n",
      "Max gradient at step 86: 0.057849399745464325\n",
      "Max gradient at step 87: 0.1409912407398224\n",
      "Max gradient at step 88: 0.19876834750175476\n",
      "Max gradient at step 89: 0.20668256282806396\n",
      "Max gradient at step 90: 0.10314217954874039\n",
      "Max gradient at step 91: 0.1088835671544075\n",
      "Max gradient at step 92: 0.23019690811634064\n",
      "Max gradient at step 93: 0.14816971123218536\n",
      "Max gradient at step 94: 0.20481988787651062\n",
      "Max gradient at step 95: 0.16212603449821472\n",
      "Max gradient at step 96: 0.14729008078575134\n",
      "Max gradient at step 97: 0.1724117398262024\n",
      "Max gradient at step 98: 0.10621605813503265\n",
      "Max gradient at step 99: 0.17340725660324097\n",
      "Max gradient at step 100: 0.24725830554962158\n",
      "Training loss (for one batch) at step 100: 0.7799\n",
      "Max gradient at step 101: 0.08607704192399979\n",
      "Max gradient at step 102: 0.3410298824310303\n",
      "Max gradient at step 103: 0.12102077901363373\n",
      "Max gradient at step 104: 0.15228736400604248\n",
      "Max gradient at step 105: 0.2103271484375\n",
      "Max gradient at step 106: 0.28279268741607666\n",
      "Max gradient at step 107: 0.1993885338306427\n",
      "Max gradient at step 108: 0.0736524686217308\n",
      "Max gradient at step 109: 0.3200351297855377\n",
      "Max gradient at step 110: 0.04910031333565712\n",
      "Max gradient at step 111: 0.11821384727954865\n",
      "Max gradient at step 112: 0.24458838999271393\n",
      "Max gradient at step 113: 0.1492874175310135\n",
      "Max gradient at step 114: 0.08783955872058868\n",
      "Max gradient at step 115: 0.1272369623184204\n",
      "Max gradient at step 116: 0.1467221975326538\n",
      "Max gradient at step 117: 0.08513101190328598\n",
      "Max gradient at step 118: 0.1530521959066391\n",
      "Max gradient at step 119: 0.19756832718849182\n",
      "Max gradient at step 120: 0.1872967928647995\n",
      "Max gradient at step 121: 0.16391177475452423\n",
      "Max gradient at step 122: 0.15568538010120392\n",
      "Max gradient at step 123: 0.12866397202014923\n",
      "Max gradient at step 124: 0.18519115447998047\n",
      "Max gradient at step 125: 0.07767879962921143\n",
      "Max gradient at step 126: 0.15464559197425842\n",
      "Max gradient at step 127: 0.12266425043344498\n",
      "Max gradient at step 128: 0.19089511036872864\n",
      "Max gradient at step 129: 0.18089713156223297\n",
      "Max gradient at step 130: 0.16893436014652252\n",
      "Max gradient at step 131: 0.1535181850194931\n",
      "Max gradient at step 132: 0.3856281638145447\n",
      "Max gradient at step 133: 0.16720737516880035\n",
      "Max gradient at step 134: 0.19092360138893127\n",
      "Max gradient at step 135: 0.24199914932250977\n",
      "Max gradient at step 136: 0.1974928379058838\n",
      "Max gradient at step 137: 0.42283326387405396\n",
      "Max gradient at step 138: 0.25531336665153503\n",
      "Max gradient at step 139: 0.22222162783145905\n",
      "Max gradient at step 140: 0.1585606038570404\n",
      "Max gradient at step 141: 0.13006195425987244\n",
      "Max gradient at step 142: 0.13784539699554443\n",
      "Max gradient at step 143: 0.1342860460281372\n",
      "Max gradient at step 144: 0.11843088269233704\n",
      "Max gradient at step 145: 0.1976044625043869\n",
      "Max gradient at step 146: 0.08718360215425491\n",
      "Max gradient at step 147: 0.15934446454048157\n",
      "Max gradient at step 148: 0.14992396533489227\n",
      "Max gradient at step 149: 0.21191337704658508\n",
      "Max gradient at step 150: 0.3124188184738159\n",
      "Max gradient at step 151: 0.15055719017982483\n",
      "Max gradient at step 152: 0.20809191465377808\n",
      "Max gradient at step 153: 0.3338163197040558\n",
      "Max gradient at step 154: 0.08582717180252075\n",
      "Max gradient at step 155: 0.17705905437469482\n",
      "Max gradient at step 156: 0.08928404003381729\n",
      "Max gradient at step 157: 0.12109032273292542\n",
      "Max gradient at step 158: 0.17057637870311737\n",
      "Max gradient at step 159: 0.20296619832515717\n",
      "Max gradient at step 160: 0.11309582740068436\n",
      "Max gradient at step 161: 0.26708221435546875\n",
      "Max gradient at step 162: 0.1204603910446167\n",
      "Max gradient at step 163: 0.179214209318161\n",
      "Max gradient at step 164: 0.10522963106632233\n",
      "Max gradient at step 165: 0.20191967487335205\n",
      "Max gradient at step 166: 0.18907485902309418\n",
      "Max gradient at step 167: 0.22361762821674347\n",
      "Max gradient at step 168: 0.14356157183647156\n",
      "Max gradient at step 169: 0.12026103585958481\n",
      "Max gradient at step 170: 0.07296871393918991\n",
      "Max gradient at step 171: 0.1256529539823532\n",
      "Max gradient at step 172: 0.14838683605194092\n",
      "Max gradient at step 173: 0.24417655169963837\n",
      "Max gradient at step 174: 0.12690678238868713\n",
      "Max gradient at step 175: 0.13611513376235962\n",
      "Max gradient at step 176: 0.12729190289974213\n",
      "Max gradient at step 177: 0.06660629063844681\n",
      "Max gradient at step 178: 0.10651478916406631\n",
      "Max gradient at step 179: 0.12438960373401642\n",
      "Max gradient at step 180: 0.1316157728433609\n",
      "Max gradient at step 181: 0.11346930265426636\n",
      "Max gradient at step 182: 0.12877878546714783\n",
      "Max gradient at step 183: 0.3132798671722412\n",
      "Max gradient at step 184: 0.1555866152048111\n",
      "Max gradient at step 185: 0.08413909375667572\n",
      "Max gradient at step 186: 0.08678785711526871\n",
      "Max gradient at step 187: 0.18880145251750946\n",
      "Max gradient at step 188: 0.19447115063667297\n",
      "Max gradient at step 189: 0.22320528328418732\n",
      "Max gradient at step 190: 0.15550044178962708\n",
      "Max gradient at step 191: 0.09323650598526001\n",
      "Max gradient at step 192: 0.16568709909915924\n",
      "Max gradient at step 193: 0.16276869177818298\n",
      "Max gradient at step 194: 0.11272016167640686\n",
      "Max gradient at step 195: 0.11528661102056503\n",
      "Max gradient at step 196: 0.08948064595460892\n",
      "Max gradient at step 197: 0.12387578934431076\n",
      "Max gradient at step 198: 0.24041667580604553\n",
      "Max gradient at step 199: 0.15663723647594452\n",
      "Max gradient at step 200: 0.14883139729499817\n",
      "Training loss (for one batch) at step 200: 0.0995\n",
      "Max gradient at step 201: 0.2655767798423767\n",
      "Max gradient at step 202: 0.07907947152853012\n",
      "Max gradient at step 203: 0.3070823848247528\n",
      "Max gradient at step 204: 0.23530267179012299\n",
      "Max gradient at step 205: 0.1689894050359726\n",
      "Max gradient at step 206: 0.36244940757751465\n",
      "Max gradient at step 207: 0.21899369359016418\n",
      "Max gradient at step 208: 0.14185744524002075\n",
      "Max gradient at step 209: 0.12041269242763519\n",
      "Max gradient at step 210: 0.18885745108127594\n",
      "Max gradient at step 211: 0.1411857008934021\n",
      "Max gradient at step 212: 0.11653407663106918\n",
      "Max gradient at step 213: 0.1317204236984253\n",
      "Max gradient at step 214: 0.10277535021305084\n",
      "Max gradient at step 215: 0.11842253059148788\n",
      "Max gradient at step 216: 0.1792163848876953\n",
      "Max gradient at step 217: 0.24761700630187988\n",
      "Max gradient at step 218: 0.16645729541778564\n",
      "Max gradient at step 219: 0.1176408901810646\n",
      "Max gradient at step 220: 0.11061464250087738\n",
      "Max gradient at step 221: 0.15390028059482574\n",
      "Max gradient at step 222: 0.12271532416343689\n",
      "Max gradient at step 223: 0.09658714383840561\n",
      "Max gradient at step 224: 0.1381351798772812\n",
      "Max gradient at step 225: 0.09930822253227234\n",
      "Max gradient at step 226: 0.1415979117155075\n",
      "Max gradient at step 227: 0.130588561296463\n",
      "Max gradient at step 228: 0.1302829384803772\n",
      "Max gradient at step 229: 0.1261683702468872\n",
      "Max gradient at step 230: 0.35794156789779663\n",
      "Max gradient at step 231: 0.1744498312473297\n",
      "Max gradient at step 232: 0.13780713081359863\n",
      "Max gradient at step 233: 0.10222317278385162\n",
      "Max gradient at step 234: 0.2939971685409546\n",
      "Max gradient at step 235: 0.13697129487991333\n",
      "Max gradient at step 236: 0.33694496750831604\n",
      "Max gradient at step 237: 0.14216017723083496\n",
      "Max gradient at step 238: 0.20501986145973206\n",
      "Max gradient at step 239: 0.19008144736289978\n",
      "Max gradient at step 240: 0.13713492453098297\n",
      "Max gradient at step 241: 0.4524531960487366\n",
      "Max gradient at step 242: 0.21211448311805725\n",
      "Max gradient at step 243: 0.1605311930179596\n",
      "Max gradient at step 244: 0.12613867223262787\n",
      "Max gradient at step 245: 0.33769598603248596\n",
      "Max gradient at step 246: 0.19943782687187195\n",
      "Max gradient at step 247: 0.1214434877038002\n",
      "Max gradient at step 248: 0.21651411056518555\n",
      "Max gradient at step 249: 0.1589450091123581\n",
      "Max gradient at step 250: 0.29468581080436707\n",
      "Max gradient at step 251: 0.15781526267528534\n",
      "Max gradient at step 252: 0.2853385806083679\n",
      "Max gradient at step 253: 0.2358592003583908\n",
      "Max gradient at step 254: 0.09909810870885849\n",
      "Max gradient at step 255: 0.2727791368961334\n",
      "Max gradient at step 256: 0.13982431590557098\n",
      "Max gradient at step 257: 0.13818146288394928\n",
      "Max gradient at step 258: 0.1539876013994217\n",
      "Max gradient at step 259: 0.3579424321651459\n",
      "Max gradient at step 260: 0.15358592569828033\n",
      "Max gradient at step 261: 0.16292856633663177\n",
      "Max gradient at step 262: 0.1559489220380783\n",
      "Max gradient at step 263: 0.2524459958076477\n",
      "Max gradient at step 264: 0.21363846957683563\n",
      "Max gradient at step 265: 0.17284396290779114\n",
      "Max gradient at step 266: 0.24374079704284668\n",
      "Max gradient at step 267: 0.1343107670545578\n",
      "Max gradient at step 268: 0.1597631275653839\n",
      "Max gradient at step 269: 0.17123988270759583\n",
      "Max gradient at step 270: 0.06044084206223488\n",
      "Max gradient at step 271: 0.31904998421669006\n",
      "Max gradient at step 272: 0.11718311160802841\n",
      "Max gradient at step 273: 0.20838181674480438\n",
      "Max gradient at step 274: 0.15419045090675354\n",
      "Max gradient at step 275: 0.06777653843164444\n",
      "Max gradient at step 276: 0.22829188406467438\n",
      "Max gradient at step 277: 0.07810051739215851\n",
      "Max gradient at step 278: 0.21550248563289642\n",
      "Max gradient at step 279: 0.24987339973449707\n",
      "Max gradient at step 280: 0.2498847246170044\n",
      "Max gradient at step 281: 0.13730324804782867\n",
      "Max gradient at step 282: 0.15222078561782837\n",
      "Max gradient at step 283: 0.4099312424659729\n",
      "Max gradient at step 284: 0.1703287959098816\n",
      "Max gradient at step 285: 0.17433705925941467\n",
      "Max gradient at step 286: 0.13033688068389893\n",
      "Max gradient at step 287: 0.18246743083000183\n",
      "Max gradient at step 288: 0.22407032549381256\n",
      "Max gradient at step 289: 0.17531369626522064\n",
      "Max gradient at step 290: 0.0723409429192543\n",
      "Max gradient at step 291: 0.17859221994876862\n",
      "Max gradient at step 292: 0.14988110959529877\n",
      "Max gradient at step 293: 0.14209102094173431\n",
      "Max gradient at step 294: 0.1978955864906311\n",
      "Max gradient at step 295: 0.10493464767932892\n",
      "Max gradient at step 296: 0.18995298445224762\n",
      "Max gradient at step 297: 0.18248599767684937\n",
      "Max gradient at step 298: 0.2839638590812683\n",
      "Max gradient at step 299: 0.12605619430541992\n",
      "Max gradient at step 300: 0.1493062824010849\n",
      "Training loss (for one batch) at step 300: 0.2965\n",
      "Max gradient at step 301: 0.15563450753688812\n",
      "Max gradient at step 302: 0.08328022807836533\n",
      "Max gradient at step 303: 0.11600144952535629\n",
      "Max gradient at step 304: 0.15695564448833466\n",
      "Max gradient at step 305: 0.19979539513587952\n",
      "Max gradient at step 306: 0.1744781881570816\n",
      "Max gradient at step 307: 0.14259055256843567\n",
      "Max gradient at step 308: 0.09408757835626602\n",
      "Max gradient at step 309: 0.2124059796333313\n",
      "Max gradient at step 310: 0.22820335626602173\n",
      "Max gradient at step 311: 0.1620071530342102\n",
      "Max gradient at step 312: 0.20392778515815735\n",
      "Max gradient at step 313: 0.1759830117225647\n",
      "Max gradient at step 314: 0.21773101389408112\n",
      "Max gradient at step 315: 0.08779937028884888\n",
      "Max gradient at step 316: 0.14417807757854462\n",
      "Max gradient at step 317: 0.1595611423254013\n",
      "Max gradient at step 318: 0.09061041474342346\n",
      "Max gradient at step 319: 0.21050578355789185\n",
      "Max gradient at step 320: 0.27222299575805664\n",
      "Max gradient at step 321: 0.2884295880794525\n",
      "Max gradient at step 322: 0.11393499374389648\n",
      "Max gradient at step 323: 0.1656559705734253\n",
      "Max gradient at step 324: 0.105953149497509\n",
      "Max gradient at step 325: 0.28228628635406494\n",
      "Max gradient at step 326: 0.1893479824066162\n",
      "Max gradient at step 327: 0.2225370854139328\n",
      "Max gradient at step 328: 0.1764562427997589\n",
      "Max gradient at step 329: 0.21438682079315186\n",
      "Max gradient at step 330: 0.11376945674419403\n",
      "Max gradient at step 331: 0.18555058538913727\n",
      "Max gradient at step 332: 0.1577482521533966\n",
      "Max gradient at step 333: 0.18157778680324554\n",
      "Max gradient at step 334: 0.16753412783145905\n",
      "Max gradient at step 335: 0.23192551732063293\n",
      "Max gradient at step 336: 0.129277303814888\n",
      "Max gradient at step 337: 0.22424526512622833\n",
      "Max gradient at step 338: 0.1779952496290207\n",
      "Max gradient at step 339: 0.12176340073347092\n",
      "Max gradient at step 340: 0.22303932905197144\n",
      "Max gradient at step 341: 0.18603196740150452\n",
      "Max gradient at step 342: 0.22992980480194092\n",
      "Max gradient at step 343: 0.10441228747367859\n",
      "Max gradient at step 344: 0.16978934407234192\n",
      "Max gradient at step 345: 0.12350783497095108\n",
      "Max gradient at step 346: 0.3287546932697296\n",
      "Max gradient at step 347: 0.2244492471218109\n",
      "Max gradient at step 348: 0.14002171158790588\n",
      "Max gradient at step 349: 0.1605243980884552\n",
      "Max gradient at step 350: 0.16803273558616638\n",
      "Max gradient at step 351: 0.10273119062185287\n",
      "Max gradient at step 352: 0.20804712176322937\n",
      "Max gradient at step 353: 0.16760264337062836\n",
      "Max gradient at step 354: 0.2984229028224945\n",
      "Max gradient at step 355: 0.1315165013074875\n",
      "Max gradient at step 356: 0.13680139183998108\n",
      "Max gradient at step 357: 0.09506414830684662\n",
      "Max gradient at step 358: 0.13594283163547516\n",
      "Max gradient at step 359: 0.20937737822532654\n",
      "Max gradient at step 360: 0.21008267998695374\n",
      "Max gradient at step 361: 0.16854645311832428\n",
      "Max gradient at step 362: 0.18146885931491852\n",
      "Max gradient at step 363: 0.14503784477710724\n",
      "Max gradient at step 364: 0.26209044456481934\n",
      "Max gradient at step 365: 0.16430781781673431\n",
      "Max gradient at step 366: 0.14473603665828705\n",
      "Max gradient at step 367: 0.15006782114505768\n",
      "Max gradient at step 368: 0.2174086719751358\n",
      "Max gradient at step 369: 0.17615096271038055\n",
      "Max gradient at step 370: 0.16794666647911072\n",
      "Max gradient at step 371: 0.26530203223228455\n",
      "Max gradient at step 372: 0.16760753095149994\n",
      "Max gradient at step 373: 0.18208657205104828\n",
      "Max gradient at step 374: 0.20121927559375763\n",
      "Max gradient at step 375: 0.1818135380744934\n",
      "Max gradient at step 376: 0.15584105253219604\n",
      "Max gradient at step 377: 0.22974753379821777\n",
      "Max gradient at step 378: 0.18290390074253082\n",
      "Max gradient at step 379: 0.20228441059589386\n",
      "Max gradient at step 380: 0.2530480623245239\n",
      "Max gradient at step 381: 0.1553671956062317\n",
      "Max gradient at step 382: 0.27364256978034973\n",
      "Max gradient at step 383: 0.1718960553407669\n",
      "Max gradient at step 384: 0.10428861528635025\n",
      "Max gradient at step 385: 0.14091619849205017\n",
      "Max gradient at step 386: 0.15469197928905487\n",
      "Max gradient at step 387: 0.17942260205745697\n",
      "Max gradient at step 388: 0.1077003926038742\n",
      "Max gradient at step 389: 0.20367753505706787\n",
      "Max gradient at step 390: 0.20646701753139496\n",
      "Max gradient at step 391: 0.221217080950737\n",
      "Max gradient at step 392: 0.17304164171218872\n",
      "Max gradient at step 393: 0.16997480392456055\n",
      "Max gradient at step 394: 0.14094465970993042\n",
      "Max gradient at step 395: 0.173701673746109\n",
      "Max gradient at step 396: 0.26223111152648926\n",
      "Max gradient at step 397: 0.35351884365081787\n",
      "Max gradient at step 398: 0.1479010134935379\n",
      "Max gradient at step 399: 0.13049127161502838\n",
      "Max gradient at step 400: 0.2630804181098938\n",
      "Training loss (for one batch) at step 400: 0.4463\n",
      "Max gradient at step 401: 0.32450392842292786\n",
      "Max gradient at step 402: 0.09210501611232758\n",
      "Max gradient at step 403: 0.14320750534534454\n",
      "Max gradient at step 404: 0.22474272549152374\n",
      "Max gradient at step 405: 0.22467108070850372\n",
      "Max gradient at step 406: 0.17661389708518982\n",
      "Max gradient at step 407: 0.2459690123796463\n",
      "Max gradient at step 408: 0.17780213057994843\n",
      "Max gradient at step 409: 0.16932186484336853\n",
      "Max gradient at step 410: 0.12171909213066101\n",
      "Max gradient at step 411: 0.27828076481819153\n",
      "Max gradient at step 412: 0.17690248787403107\n",
      "Max gradient at step 413: 0.19529083371162415\n",
      "Max gradient at step 414: 0.20800112187862396\n",
      "Max gradient at step 415: 0.11662008613348007\n",
      "Max gradient at step 416: 0.25994887948036194\n",
      "Max gradient at step 417: 0.13092461228370667\n",
      "Max gradient at step 418: 0.13388900458812714\n",
      "Max gradient at step 419: 0.27854660153388977\n",
      "Max gradient at step 420: 0.2414952665567398\n",
      "Max gradient at step 421: 0.25294312834739685\n",
      "Max gradient at step 422: 0.09371288120746613\n",
      "Max gradient at step 423: 0.086435966193676\n",
      "Max gradient at step 424: 0.15044207870960236\n",
      "Max gradient at step 425: 0.18977543711662292\n",
      "Max gradient at step 426: 0.14629527926445007\n",
      "Max gradient at step 427: 0.10849599540233612\n",
      "Max gradient at step 428: 0.19556763768196106\n",
      "Max gradient at step 429: 0.21060828864574432\n",
      "Max gradient at step 430: 0.16965067386627197\n",
      "Max gradient at step 431: 0.16236548125743866\n",
      "Max gradient at step 432: 0.297179639339447\n",
      "Max gradient at step 433: 0.15494853258132935\n",
      "Max gradient at step 434: 0.13532456755638123\n",
      "Max gradient at step 435: 0.27100756764411926\n",
      "Max gradient at step 436: 0.1568666696548462\n",
      "Max gradient at step 437: 0.15182064473628998\n",
      "Max gradient at step 438: 0.14495131373405457\n",
      "Max gradient at step 439: 0.1389485001564026\n",
      "Max gradient at step 440: 0.1296558827161789\n",
      "Max gradient at step 441: 0.17254146933555603\n",
      "Max gradient at step 442: 0.20144306123256683\n",
      "Max gradient at step 443: 0.2962689995765686\n",
      "Max gradient at step 444: 0.21698172390460968\n",
      "Max gradient at step 445: 0.13030624389648438\n",
      "Max gradient at step 446: 0.18870539963245392\n",
      "Max gradient at step 447: 0.12898634374141693\n",
      "Max gradient at step 448: 0.23271358013153076\n",
      "Max gradient at step 449: 0.30002301931381226\n",
      "Max gradient at step 450: 0.26519450545310974\n",
      "Max gradient at step 451: 0.06438037008047104\n",
      "Max gradient at step 452: 0.12031588703393936\n",
      "Max gradient at step 453: 0.13944274187088013\n",
      "Max gradient at step 454: 0.2407025396823883\n",
      "Max gradient at step 455: 0.14587785303592682\n",
      "Max gradient at step 456: 0.23139725625514984\n",
      "Max gradient at step 457: 0.13047809898853302\n",
      "Max gradient at step 458: 0.22545509040355682\n",
      "Max gradient at step 459: 0.18657219409942627\n",
      "Max gradient at step 460: 0.11734289675951004\n",
      "Max gradient at step 461: 0.25460782647132874\n",
      "Max gradient at step 462: 0.18831120431423187\n",
      "Max gradient at step 463: 0.18044890463352203\n",
      "Max gradient at step 464: 0.15990526974201202\n",
      "Max gradient at step 465: 0.1868906021118164\n",
      "Max gradient at step 466: 0.11859308183193207\n",
      "Max gradient at step 467: 0.0955706387758255\n",
      "Max gradient at step 468: 0.23396947979927063\n",
      "Max gradient at step 469: 0.1382889449596405\n",
      "Max gradient at step 470: 0.21566705405712128\n",
      "Max gradient at step 471: 0.3422479033470154\n",
      "Max gradient at step 472: 0.11621109396219254\n",
      "Max gradient at step 473: 0.147822305560112\n",
      "Max gradient at step 474: 0.24637341499328613\n",
      "Max gradient at step 475: 0.11563799530267715\n",
      "Max gradient at step 476: 0.1424056738615036\n",
      "Max gradient at step 477: 0.13454067707061768\n",
      "Max gradient at step 478: 0.1724599152803421\n",
      "Max gradient at step 479: 0.15427304804325104\n",
      "Max gradient at step 480: 0.23465538024902344\n",
      "Max gradient at step 481: 0.2333749383687973\n",
      "Max gradient at step 482: 0.2858787775039673\n",
      "Max gradient at step 483: 0.18039570748806\n",
      "Max gradient at step 484: 0.12229025363922119\n",
      "Max gradient at step 485: 0.17536897957324982\n",
      "Max gradient at step 486: 0.14616994559764862\n",
      "Max gradient at step 487: 0.08194705098867416\n",
      "Max gradient at step 488: 0.18780259788036346\n",
      "Max gradient at step 489: 0.1256118267774582\n",
      "Max gradient at step 490: 0.17569884657859802\n",
      "Max gradient at step 491: 0.196275532245636\n",
      "Max gradient at step 492: 0.10439573973417282\n",
      "Max gradient at step 493: 0.2991239130496979\n",
      "Max gradient at step 494: 0.14939141273498535\n",
      "Max gradient at step 495: 0.14537720382213593\n",
      "Max gradient at step 496: 0.23704391717910767\n",
      "Max gradient at step 497: 0.131017804145813\n",
      "Max gradient at step 498: 0.21251340210437775\n",
      "Max gradient at step 499: 0.22419409453868866\n",
      "Max gradient at step 500: 0.19081991910934448\n",
      "Training loss (for one batch) at step 500: 0.4558\n",
      "Max gradient at step 501: 0.16313081979751587\n",
      "Max gradient at step 502: 0.16652962565422058\n",
      "Max gradient at step 503: 0.12089817970991135\n",
      "Max gradient at step 504: 0.11909539997577667\n",
      "Max gradient at step 505: 0.12983925640583038\n",
      "Max gradient at step 506: 0.14295019209384918\n",
      "Max gradient at step 507: 0.1216137558221817\n",
      "Max gradient at step 508: 0.12502421438694\n",
      "Max gradient at step 509: 0.1722911149263382\n",
      "Max gradient at step 510: 0.1390671730041504\n",
      "Max gradient at step 511: 0.16534525156021118\n",
      "Max gradient at step 512: 0.10392570495605469\n",
      "Max gradient at step 513: 0.15484406054019928\n",
      "Max gradient at step 514: 0.31791043281555176\n",
      "Max gradient at step 515: 0.34193500876426697\n",
      "Max gradient at step 516: 0.18870793282985687\n",
      "Max gradient at step 517: 0.25663185119628906\n",
      "Max gradient at step 518: 0.14196859300136566\n",
      "Max gradient at step 519: 0.19022533297538757\n",
      "Max gradient at step 520: 0.12969547510147095\n",
      "Max gradient at step 521: 0.2635863423347473\n",
      "Max gradient at step 522: 0.4143630266189575\n",
      "Max gradient at step 523: 0.1753210574388504\n",
      "Max gradient at step 524: 0.14244233071804047\n",
      "Max gradient at step 525: 0.10605118423700333\n",
      "Max gradient at step 526: 0.12440339475870132\n",
      "Max gradient at step 527: 0.259443998336792\n",
      "Max gradient at step 528: 0.14764079451560974\n",
      "Max gradient at step 529: 0.16760936379432678\n",
      "Max gradient at step 530: 0.23266035318374634\n",
      "Max gradient at step 531: 0.19104278087615967\n",
      "Max gradient at step 532: 0.14085568487644196\n",
      "Max gradient at step 533: 0.1338994801044464\n",
      "Max gradient at step 534: 0.22126330435276031\n",
      "Max gradient at step 535: 0.0771128386259079\n",
      "Max gradient at step 536: 0.13811902701854706\n",
      "Max gradient at step 537: 0.20277506113052368\n",
      "Max gradient at step 538: 0.1363401710987091\n",
      "Max gradient at step 539: 0.17084378004074097\n",
      "Max gradient at step 540: 0.13788869976997375\n",
      "Max gradient at step 541: 0.13866353034973145\n",
      "Max gradient at step 542: 0.23047052323818207\n",
      "Max gradient at step 543: 0.1310814917087555\n",
      "Max gradient at step 544: 0.12013393640518188\n",
      "Max gradient at step 545: 0.24236078560352325\n",
      "Max gradient at step 546: 0.16715574264526367\n",
      "Max gradient at step 547: 0.43844074010849\n",
      "Max gradient at step 548: 0.12022247910499573\n",
      "Max gradient at step 549: 0.2182338535785675\n",
      "Max gradient at step 550: 0.2010556012392044\n",
      "Max gradient at step 551: 0.18615390360355377\n",
      "Max gradient at step 552: 0.10885488986968994\n",
      "Max gradient at step 553: 0.14541666209697723\n",
      "Max gradient at step 554: 0.15900851786136627\n",
      "Max gradient at step 555: 0.1801355481147766\n",
      "Max gradient at step 556: 0.11424139142036438\n",
      "Max gradient at step 557: 0.1487387865781784\n",
      "Max gradient at step 558: 0.1553930640220642\n",
      "Max gradient at step 559: 0.19335192441940308\n",
      "Max gradient at step 560: 0.09812606126070023\n",
      "Max gradient at step 561: 0.23690691590309143\n",
      "Max gradient at step 562: 0.21931608021259308\n",
      "Max gradient at step 563: 0.1059364452958107\n",
      "Max gradient at step 564: 0.12159542739391327\n",
      "Max gradient at step 565: 0.21241328120231628\n",
      "Max gradient at step 566: 0.23163564503192902\n",
      "Max gradient at step 567: 0.12788835167884827\n",
      "Max gradient at step 568: 0.25934281945228577\n",
      "Max gradient at step 569: 0.11347541958093643\n",
      "Max gradient at step 570: 0.15545114874839783\n",
      "Max gradient at step 571: 0.23287253081798553\n",
      "Max gradient at step 572: 0.21601495146751404\n",
      "Max gradient at step 573: 0.15394476056098938\n",
      "Max gradient at step 574: 0.18627098202705383\n",
      "Max gradient at step 575: 0.18608514964580536\n",
      "Max gradient at step 576: 0.158911794424057\n",
      "Max gradient at step 577: 0.18478608131408691\n",
      "Max gradient at step 578: 0.23851756751537323\n",
      "Max gradient at step 579: 0.27482229471206665\n",
      "Max gradient at step 580: 0.14003048837184906\n",
      "Max gradient at step 581: 0.13538984954357147\n",
      "Max gradient at step 582: 0.1287105828523636\n",
      "Max gradient at step 583: 0.18241626024246216\n",
      "Max gradient at step 584: 0.14084039628505707\n",
      "Max gradient at step 585: 0.20324894785881042\n",
      "Max gradient at step 586: 0.11171024292707443\n",
      "Max gradient at step 587: 0.20641736686229706\n",
      "Max gradient at step 588: 0.16236889362335205\n",
      "Max gradient at step 589: 0.08416252583265305\n",
      "Max gradient at step 590: 0.28703129291534424\n",
      "Max gradient at step 591: 0.1252373605966568\n",
      "Max gradient at step 592: 0.12727266550064087\n",
      "Max gradient at step 593: 0.23312894999980927\n",
      "Max gradient at step 594: 0.21742039918899536\n",
      "Max gradient at step 595: 0.14056770503520966\n",
      "Max gradient at step 596: 0.1393526792526245\n",
      "Max gradient at step 597: 0.2476576864719391\n",
      "Max gradient at step 598: 0.18527549505233765\n",
      "Max gradient at step 599: 0.08214577287435532\n",
      "Max gradient at step 600: 0.1566483974456787\n",
      "Training loss (for one batch) at step 600: 0.3146\n",
      "Max gradient at step 601: 0.20149962604045868\n",
      "Max gradient at step 602: 0.18872720003128052\n",
      "Max gradient at step 603: 0.24189384281635284\n",
      "Max gradient at step 604: 0.16795895993709564\n",
      "Max gradient at step 605: 0.13010628521442413\n",
      "Max gradient at step 606: 0.2444501370191574\n",
      "Max gradient at step 607: 0.1381688117980957\n",
      "Max gradient at step 608: 0.17888256907463074\n",
      "Max gradient at step 609: 0.10524317622184753\n",
      "Max gradient at step 610: 0.21267478168010712\n",
      "Max gradient at step 611: 0.21993540227413177\n",
      "Max gradient at step 612: 0.14056606590747833\n",
      "Max gradient at step 613: 0.38155385851860046\n",
      "Max gradient at step 614: 0.11304985731840134\n",
      "Max gradient at step 615: 0.18076972663402557\n",
      "Max gradient at step 616: 0.14501185715198517\n",
      "Max gradient at step 617: 0.18501915037631989\n",
      "Max gradient at step 618: 0.21770362555980682\n",
      "Max gradient at step 619: 0.16384251415729523\n",
      "Max gradient at step 620: 0.2528889775276184\n",
      "Max gradient at step 621: 0.06590482592582703\n",
      "Max gradient at step 622: 0.1276722252368927\n",
      "Max gradient at step 623: 0.10261841118335724\n",
      "Max gradient at step 624: 0.11931022256612778\n",
      "Max gradient at step 625: 0.09106409549713135\n",
      "Max gradient at step 626: 0.12417731434106827\n",
      "Max gradient at step 627: 0.16413506865501404\n",
      "Max gradient at step 628: 0.1706433743238449\n",
      "Max gradient at step 629: 0.1206614300608635\n",
      "Max gradient at step 630: 0.1398044377565384\n",
      "Max gradient at step 631: 0.14989754557609558\n",
      "Max gradient at step 632: 0.16195820271968842\n",
      "Max gradient at step 633: 0.14130017161369324\n",
      "Max gradient at step 634: 0.15509594976902008\n",
      "Max gradient at step 635: 0.17440544068813324\n",
      "Max gradient at step 636: 0.3128012716770172\n",
      "Max gradient at step 637: 0.14088445901870728\n",
      "Max gradient at step 638: 0.1342926025390625\n",
      "Max gradient at step 639: 0.09248459339141846\n",
      "Max gradient at step 640: 0.1968381553888321\n",
      "Max gradient at step 641: 0.1870107352733612\n",
      "Max gradient at step 642: 0.3401389718055725\n",
      "Max gradient at step 643: 0.20898766815662384\n",
      "Max gradient at step 644: 0.2929508090019226\n",
      "Max gradient at step 645: 0.1726849228143692\n",
      "Max gradient at step 646: 0.1781158596277237\n",
      "Max gradient at step 647: 0.16978122293949127\n",
      "Max gradient at step 648: 0.19491508603096008\n",
      "Max gradient at step 649: 0.1757144182920456\n",
      "Max gradient at step 650: 0.1688648760318756\n",
      "Max gradient at step 651: 0.2432103157043457\n",
      "Max gradient at step 652: 0.16139955818653107\n",
      "Max gradient at step 653: 0.13498568534851074\n",
      "Max gradient at step 654: 0.24058574438095093\n",
      "Max gradient at step 655: 0.08211302012205124\n",
      "Max gradient at step 656: 0.16152460873126984\n",
      "Max gradient at step 657: 0.13910453021526337\n",
      "Max gradient at step 658: 0.16856883466243744\n",
      "Max gradient at step 659: 0.1281355917453766\n",
      "Max gradient at step 660: 0.20624153316020966\n",
      "Max gradient at step 661: 0.13682159781455994\n",
      "Max gradient at step 662: 0.1603722870349884\n",
      "Max gradient at step 663: 0.108241006731987\n",
      "Max gradient at step 664: 0.07721412181854248\n",
      "Max gradient at step 665: 0.15556342899799347\n",
      "Max gradient at step 666: 0.22373485565185547\n",
      "Max gradient at step 667: 0.16449585556983948\n",
      "Max gradient at step 668: 0.28601884841918945\n",
      "Max gradient at step 669: 0.16213181614875793\n",
      "Max gradient at step 670: 0.14980195462703705\n",
      "Max gradient at step 671: 0.1723606288433075\n",
      "Max gradient at step 672: 0.13785064220428467\n",
      "Max gradient at step 673: 0.22151008248329163\n",
      "Max gradient at step 674: 0.13636036217212677\n",
      "Max gradient at step 675: 0.179343581199646\n",
      "Max gradient at step 676: 0.2532420754432678\n",
      "Max gradient at step 677: 0.1411714106798172\n",
      "Max gradient at step 678: 0.12601737678050995\n",
      "Max gradient at step 679: 0.13286837935447693\n",
      "Max gradient at step 680: 0.12831789255142212\n",
      "Max gradient at step 681: 0.2634422183036804\n",
      "Max gradient at step 682: 0.18174371123313904\n",
      "Max gradient at step 683: 0.11463261395692825\n",
      "Max gradient at step 684: 0.17772100865840912\n",
      "Max gradient at step 685: 0.1245211809873581\n",
      "Max gradient at step 686: 0.14064526557922363\n",
      "Max gradient at step 687: 0.17548441886901855\n",
      "Max gradient at step 688: 0.13713587820529938\n",
      "Max gradient at step 689: 0.1636122614145279\n",
      "Max gradient at step 690: 0.10112660378217697\n",
      "Max gradient at step 691: 0.20930814743041992\n",
      "Max gradient at step 692: 0.17754428088665009\n",
      "Max gradient at step 693: 0.19866885244846344\n",
      "Max gradient at step 694: 0.15732914209365845\n",
      "Max gradient at step 695: 0.13296864926815033\n",
      "Max gradient at step 696: 0.16655243933200836\n",
      "Max gradient at step 697: 0.21320565044879913\n",
      "Max gradient at step 698: 0.15069016814231873\n",
      "Max gradient at step 699: 0.13767606019973755\n",
      "Max gradient at step 700: 0.17567665874958038\n",
      "Training loss (for one batch) at step 700: 0.1504\n",
      "Max gradient at step 701: 0.07166274636983871\n",
      "Max gradient at step 702: 0.15368010103702545\n",
      "Max gradient at step 703: 0.1396951973438263\n",
      "Max gradient at step 704: 0.21257592737674713\n",
      "Max gradient at step 705: 0.11066076904535294\n",
      "Max gradient at step 706: 0.16310814023017883\n",
      "Max gradient at step 707: 0.146340474486351\n",
      "Max gradient at step 708: 0.25800880789756775\n",
      "Max gradient at step 709: 0.12215986847877502\n",
      "Max gradient at step 710: 0.26129788160324097\n",
      "Max gradient at step 711: 0.09495657682418823\n",
      "Max gradient at step 712: 0.11703632771968842\n",
      "Max gradient at step 713: 0.12509877979755402\n",
      "Max gradient at step 714: 0.2541905343532562\n",
      "Max gradient at step 715: 0.20685356855392456\n",
      "Max gradient at step 716: 0.19366787374019623\n",
      "Max gradient at step 717: 0.06527725607156754\n",
      "Max gradient at step 718: 0.18715673685073853\n",
      "Max gradient at step 719: 0.1616106778383255\n",
      "Max gradient at step 720: 0.13294075429439545\n",
      "Max gradient at step 721: 0.14072026312351227\n",
      "Max gradient at step 722: 0.19173458218574524\n",
      "Max gradient at step 723: 0.11774297803640366\n",
      "Max gradient at step 724: 0.13843457400798798\n",
      "Max gradient at step 725: 0.21725882589817047\n",
      "Max gradient at step 726: 0.09090795367956161\n",
      "Max gradient at step 727: 0.09715362638235092\n",
      "Max gradient at step 728: 0.10370124131441116\n",
      "Max gradient at step 729: 0.22725678980350494\n",
      "Max gradient at step 730: 0.2300841510295868\n",
      "Max gradient at step 731: 0.2898419201374054\n",
      "Max gradient at step 732: 0.18860077857971191\n",
      "Max gradient at step 733: 0.17884980142116547\n",
      "Max gradient at step 734: 0.10798050463199615\n",
      "Max gradient at step 735: 0.18647120893001556\n",
      "Max gradient at step 736: 0.14464071393013\n",
      "Max gradient at step 737: 0.18447458744049072\n",
      "Max gradient at step 738: 0.06697095930576324\n",
      "Max gradient at step 739: 0.13556186854839325\n",
      "Max gradient at step 740: 0.13768665492534637\n",
      "Max gradient at step 741: 0.26448696851730347\n",
      "Max gradient at step 742: 0.2462608814239502\n",
      "Max gradient at step 743: 0.17519207298755646\n",
      "Max gradient at step 744: 0.24315258860588074\n",
      "Max gradient at step 745: 0.13463719189167023\n",
      "Max gradient at step 746: 0.14556579291820526\n",
      "Max gradient at step 747: 0.22094868123531342\n",
      "Max gradient at step 748: 0.08098974078893661\n",
      "Max gradient at step 749: 0.1443968415260315\n",
      "Max gradient at step 750: 0.18777965009212494\n",
      "Max gradient at step 751: 0.14465594291687012\n",
      "Max gradient at step 752: 0.21123932301998138\n",
      "Max gradient at step 753: 0.1266917884349823\n",
      "Max gradient at step 754: 0.3620914816856384\n",
      "Max gradient at step 755: 0.10963290929794312\n",
      "Max gradient at step 756: 0.401696115732193\n",
      "Max gradient at step 757: 0.1821022480726242\n",
      "Max gradient at step 758: 0.18631036579608917\n",
      "Max gradient at step 759: 0.17811992764472961\n",
      "Max gradient at step 760: 0.18441051244735718\n",
      "Max gradient at step 761: 0.2530004680156708\n",
      "Max gradient at step 762: 0.16740591824054718\n",
      "Max gradient at step 763: 0.12999455630779266\n",
      "Max gradient at step 764: 0.14943771064281464\n",
      "Max gradient at step 765: 0.1880357265472412\n",
      "Max gradient at step 766: 0.25954002141952515\n",
      "Max gradient at step 767: 0.15071703493595123\n",
      "Max gradient at step 768: 0.17253771424293518\n",
      "Max gradient at step 769: 0.2188235968351364\n",
      "Max gradient at step 770: 0.21120783686637878\n",
      "Max gradient at step 771: 0.1788075715303421\n",
      "Max gradient at step 772: 0.18290865421295166\n",
      "Max gradient at step 773: 0.0739101991057396\n",
      "Max gradient at step 774: 0.10941298305988312\n",
      "Max gradient at step 775: 0.3260165750980377\n",
      "Max gradient at step 776: 0.12174724042415619\n",
      "Max gradient at step 777: 0.1636091023683548\n",
      "Max gradient at step 778: 0.21315157413482666\n",
      "Max gradient at step 779: 0.1544252634048462\n",
      "Max gradient at step 780: 0.07891729474067688\n",
      "Max gradient at step 781: 0.18555106222629547\n",
      "Max gradient at step 782: 0.08513371646404266\n",
      "Max gradient at step 783: 0.3506110608577728\n",
      "Max gradient at step 784: 0.22618494927883148\n",
      "Max gradient at step 785: 0.1261933445930481\n",
      "Max gradient at step 786: 0.14507129788398743\n",
      "Max gradient at step 787: 0.17198596894741058\n",
      "Max gradient at step 788: 0.34179168939590454\n",
      "Max gradient at step 789: 0.19834430515766144\n",
      "Max gradient at step 790: 0.19148705899715424\n",
      "Max gradient at step 791: 0.18668845295906067\n",
      "Max gradient at step 792: 0.11337703466415405\n",
      "Max gradient at step 793: 0.12380412966012955\n",
      "Max gradient at step 794: 0.1316385120153427\n",
      "Max gradient at step 795: 0.23145180940628052\n",
      "Max gradient at step 796: 0.24694962799549103\n",
      "Max gradient at step 797: 0.176482692360878\n",
      "Max gradient at step 798: 0.10384718328714371\n",
      "Max gradient at step 799: 0.1604854017496109\n",
      "Max gradient at step 800: 0.12600143253803253\n",
      "Training loss (for one batch) at step 800: 0.3743\n",
      "Max gradient at step 801: 0.28469160199165344\n",
      "Max gradient at step 802: 0.1892300248146057\n",
      "Max gradient at step 803: 0.10793764889240265\n",
      "Max gradient at step 804: 0.16498960554599762\n",
      "Max gradient at step 805: 0.23914653062820435\n",
      "Max gradient at step 806: 0.09675392508506775\n",
      "Max gradient at step 807: 0.14193861186504364\n",
      "Max gradient at step 808: 0.1359366774559021\n",
      "Max gradient at step 809: 0.16335631906986237\n",
      "Max gradient at step 810: 0.17237047851085663\n",
      "Max gradient at step 811: 0.18110927939414978\n",
      "Max gradient at step 812: 0.26441437005996704\n",
      "Max gradient at step 813: 0.11810654401779175\n",
      "Max gradient at step 814: 0.13229022920131683\n",
      "Max gradient at step 815: 0.19364742934703827\n",
      "Max gradient at step 816: 0.12598751485347748\n",
      "Max gradient at step 817: 0.12603890895843506\n",
      "Max gradient at step 818: 0.10874822735786438\n",
      "Max gradient at step 819: 0.11162931472063065\n",
      "Max gradient at step 820: 0.13749225437641144\n",
      "Max gradient at step 821: 0.23342683911323547\n",
      "Max gradient at step 822: 0.20514999330043793\n",
      "Max gradient at step 823: 0.19426962733268738\n",
      "Max gradient at step 824: 0.2309611737728119\n",
      "Max gradient at step 825: 0.26613906025886536\n",
      "Max gradient at step 826: 0.21796004474163055\n",
      "Max gradient at step 827: 0.2270454466342926\n",
      "Max gradient at step 828: 0.40712669491767883\n",
      "Max gradient at step 829: 0.12192358076572418\n",
      "Max gradient at step 830: 0.22849074006080627\n",
      "Max gradient at step 831: 0.18482312560081482\n",
      "Max gradient at step 832: 0.1648806631565094\n",
      "Max gradient at step 833: 0.13491398096084595\n",
      "Max gradient at step 834: 0.3224722445011139\n",
      "Max gradient at step 835: 0.1264554262161255\n",
      "Max gradient at step 836: 0.16387705504894257\n",
      "Max gradient at step 837: 0.2605208456516266\n",
      "Max gradient at step 838: 0.14003460109233856\n",
      "Max gradient at step 839: 0.16200484335422516\n",
      "Max gradient at step 840: 0.15028780698776245\n",
      "Max gradient at step 841: 0.1288715898990631\n",
      "Max gradient at step 842: 0.13880430161952972\n",
      "Max gradient at step 843: 0.2463567852973938\n",
      "Max gradient at step 844: 0.20270982384681702\n",
      "Max gradient at step 845: 0.22133351862430573\n",
      "Max gradient at step 846: 0.19731447100639343\n",
      "Max gradient at step 847: 0.1217980608344078\n",
      "Max gradient at step 848: 0.16143323481082916\n",
      "Max gradient at step 849: 0.1792944073677063\n",
      "Max gradient at step 850: 0.1736309826374054\n",
      "Max gradient at step 851: 0.16207227110862732\n",
      "Max gradient at step 852: 0.20015326142311096\n",
      "Max gradient at step 853: 0.16520753502845764\n",
      "Max gradient at step 854: 0.20281314849853516\n",
      "Max gradient at step 855: 0.1732853800058365\n",
      "Max gradient at step 856: 0.10673286765813828\n",
      "Max gradient at step 857: 0.10852431505918503\n",
      "Max gradient at step 858: 0.12013047933578491\n",
      "Max gradient at step 859: 0.12455812841653824\n",
      "Max gradient at step 860: 0.12279753386974335\n",
      "Max gradient at step 861: 0.21191725134849548\n",
      "Max gradient at step 862: 0.17145510017871857\n",
      "Max gradient at step 863: 0.12445642799139023\n",
      "Max gradient at step 864: 0.13544295728206635\n",
      "Max gradient at step 865: 0.19646480679512024\n",
      "Max gradient at step 866: 0.13391855359077454\n",
      "Max gradient at step 867: 0.26063570380210876\n",
      "Max gradient at step 868: 0.1231737807393074\n",
      "Max gradient at step 869: 0.16468872129917145\n",
      "Max gradient at step 870: 0.1536003202199936\n",
      "Max gradient at step 871: 0.17783908545970917\n",
      "Max gradient at step 872: 0.11092107743024826\n",
      "Max gradient at step 873: 0.09502188861370087\n",
      "Max gradient at step 874: 0.310539186000824\n",
      "Max gradient at step 875: 0.21502283215522766\n",
      "Max gradient at step 876: 0.1064257025718689\n",
      "Max gradient at step 877: 0.1739053577184677\n",
      "Max gradient at step 878: 0.1709892600774765\n",
      "Max gradient at step 879: 0.16946689784526825\n",
      "Max gradient at step 880: 0.3322935998439789\n",
      "Max gradient at step 881: 0.21428462862968445\n",
      "Max gradient at step 882: 0.2420332133769989\n",
      "Max gradient at step 883: 0.2550342082977295\n",
      "Max gradient at step 884: 0.21852847933769226\n",
      "Max gradient at step 885: 0.2620304226875305\n",
      "Max gradient at step 886: 0.18178169429302216\n",
      "Max gradient at step 887: 0.1387483924627304\n",
      "Max gradient at step 888: 0.2059394270181656\n",
      "Max gradient at step 889: 0.13611195981502533\n",
      "Max gradient at step 890: 0.19554579257965088\n",
      "Max gradient at step 891: 0.1749303936958313\n",
      "Max gradient at step 892: 0.12587440013885498\n",
      "Max gradient at step 893: 0.13113075494766235\n",
      "Max gradient at step 894: 0.14379417896270752\n",
      "Max gradient at step 895: 0.13732728362083435\n",
      "Max gradient at step 896: 0.14778359234333038\n",
      "Max gradient at step 897: 0.09688503295183182\n",
      "Max gradient at step 898: 0.11018107831478119\n",
      "Max gradient at step 899: 0.09005282819271088\n",
      "Max gradient at step 900: 0.1983938217163086\n",
      "Training loss (for one batch) at step 900: 0.3232\n",
      "Max gradient at step 901: 0.15242822468280792\n",
      "Max gradient at step 902: 0.15528148412704468\n",
      "Max gradient at step 903: 0.18348650634288788\n",
      "Max gradient at step 904: 0.19582410156726837\n",
      "Max gradient at step 905: 0.2438947707414627\n",
      "Max gradient at step 906: 0.2266511172056198\n",
      "Max gradient at step 907: 0.16573657095432281\n",
      "Max gradient at step 908: 0.23348261415958405\n",
      "Max gradient at step 909: 0.17359420657157898\n",
      "Max gradient at step 910: 0.18374431133270264\n",
      "Max gradient at step 911: 0.3418480455875397\n",
      "Max gradient at step 912: 0.25435003638267517\n",
      "Max gradient at step 913: 0.26776257157325745\n",
      "Max gradient at step 914: 0.1437385380268097\n",
      "Max gradient at step 915: 0.16388078033924103\n",
      "Max gradient at step 916: 0.10727473348379135\n",
      "Max gradient at step 917: 0.24631008505821228\n",
      "Max gradient at step 918: 0.15738096833229065\n",
      "Max gradient at step 919: 0.18720833957195282\n",
      "Max gradient at step 920: 0.21496962010860443\n",
      "Max gradient at step 921: 0.17975647747516632\n",
      "Max gradient at step 922: 0.08472007513046265\n",
      "Max gradient at step 923: 0.2531251609325409\n",
      "Max gradient at step 924: 0.11524179577827454\n",
      "Max gradient at step 925: 0.15427838265895844\n",
      "Max gradient at step 926: 0.1646856665611267\n",
      "Max gradient at step 927: 0.1766062080860138\n",
      "Max gradient at step 928: 0.20798374712467194\n",
      "Max gradient at step 929: 0.21611113846302032\n",
      "Max gradient at step 930: 0.19287748634815216\n",
      "Max gradient at step 931: 0.09762744605541229\n",
      "Max gradient at step 932: 0.1882161796092987\n",
      "Max gradient at step 933: 0.1944204866886139\n",
      "Max gradient at step 934: 0.16892588138580322\n",
      "Max gradient at step 935: 0.22782334685325623\n",
      "Max gradient at step 936: 0.18840114772319794\n",
      "Max gradient at step 937: 0.10289882868528366\n",
      "Max gradient at step 938: 0.16086626052856445\n",
      "Max gradient at step 939: 0.25311392545700073\n",
      "Max gradient at step 940: 0.3542175889015198\n",
      "Max gradient at step 941: 0.1703786551952362\n",
      "Max gradient at step 942: 0.17320236563682556\n",
      "Max gradient at step 943: 0.14070270955562592\n",
      "Max gradient at step 944: 0.2394258826971054\n",
      "Max gradient at step 945: 0.15795081853866577\n",
      "Max gradient at step 946: 0.19756726920604706\n",
      "Max gradient at step 947: 0.1466861367225647\n",
      "Max gradient at step 948: 0.1937670111656189\n",
      "Max gradient at step 949: 0.11941508948802948\n",
      "Max gradient at step 950: 0.18079587817192078\n",
      "Max gradient at step 951: 0.24606364965438843\n",
      "Max gradient at step 952: 0.26355230808258057\n",
      "Max gradient at step 953: 0.14316509664058685\n",
      "Max gradient at step 954: 0.24238362908363342\n",
      "Max gradient at step 955: 0.19024211168289185\n",
      "Max gradient at step 956: 0.13088169693946838\n",
      "Max gradient at step 957: 0.158373162150383\n",
      "Max gradient at step 958: 0.09327064454555511\n",
      "Max gradient at step 959: 0.21892978250980377\n",
      "Max gradient at step 960: 0.1628589928150177\n",
      "Max gradient at step 961: 0.1953693926334381\n",
      "Max gradient at step 962: 0.2740134000778198\n",
      "Max gradient at step 963: 0.1527218222618103\n",
      "Max gradient at step 964: 0.11738971620798111\n",
      "Max gradient at step 965: 0.1756751537322998\n",
      "Max gradient at step 966: 0.14896269142627716\n",
      "Max gradient at step 967: 0.1196376234292984\n",
      "Max gradient at step 968: 0.20118780434131622\n",
      "Max gradient at step 969: 0.17106413841247559\n",
      "Max gradient at step 970: 0.16315998136997223\n",
      "Max gradient at step 971: 0.1994483321905136\n",
      "Max gradient at step 972: 0.15073522925376892\n",
      "Max gradient at step 973: 0.15289387106895447\n",
      "Max gradient at step 974: 0.10525845736265182\n",
      "Max gradient at step 975: 0.16820159554481506\n",
      "Max gradient at step 976: 0.21904458105564117\n",
      "Max gradient at step 977: 0.14614933729171753\n",
      "Max gradient at step 978: 0.17327800393104553\n",
      "Max gradient at step 979: 0.20754100382328033\n",
      "Max gradient at step 980: 0.17567604780197144\n",
      "Max gradient at step 981: 0.2794085144996643\n",
      "Max gradient at step 982: 0.2359832525253296\n",
      "Max gradient at step 983: 0.22863459587097168\n",
      "Max gradient at step 984: 0.19952434301376343\n",
      "Max gradient at step 985: 0.1469944566488266\n",
      "Max gradient at step 986: 0.1350005716085434\n",
      "Max gradient at step 987: 0.18655027449131012\n",
      "Max gradient at step 988: 0.3633587956428528\n",
      "Max gradient at step 989: 0.11365500837564468\n",
      "Max gradient at step 990: 0.15457065403461456\n",
      "Max gradient at step 991: 0.14170603454113007\n",
      "Max gradient at step 992: 0.24836565554141998\n",
      "Max gradient at step 993: 0.23754727840423584\n",
      "Max gradient at step 994: 0.15259599685668945\n",
      "Max gradient at step 995: 0.20266787707805634\n",
      "Max gradient at step 996: 0.11100678145885468\n",
      "Max gradient at step 997: 0.14786918461322784\n",
      "Max gradient at step 998: 0.35068315267562866\n",
      "Max gradient at step 999: 0.25506308674812317\n",
      "Max gradient at step 1000: 0.2923130393028259\n",
      "Training loss (for one batch) at step 1000: 0.6180\n",
      "Max gradient at step 1001: 0.20403388142585754\n",
      "Max gradient at step 1002: 0.17181727290153503\n",
      "Max gradient at step 1003: 0.2514191269874573\n",
      "Max gradient at step 1004: 0.12088368088006973\n",
      "Max gradient at step 1005: 0.2548161745071411\n",
      "Max gradient at step 1006: 0.13989897072315216\n",
      "Max gradient at step 1007: 0.21842822432518005\n",
      "Max gradient at step 1008: 0.184912770986557\n",
      "Max gradient at step 1009: 0.1493295580148697\n",
      "Max gradient at step 1010: 0.08790383487939835\n",
      "Max gradient at step 1011: 0.1280663013458252\n",
      "Max gradient at step 1012: 0.25018852949142456\n",
      "Max gradient at step 1013: 0.21236586570739746\n",
      "Max gradient at step 1014: 0.1255303919315338\n",
      "Max gradient at step 1015: 0.1323312371969223\n",
      "Max gradient at step 1016: 0.17734746634960175\n",
      "Max gradient at step 1017: 0.19633139669895172\n",
      "Max gradient at step 1018: 0.30392539501190186\n",
      "Max gradient at step 1019: 0.10093457996845245\n",
      "Max gradient at step 1020: 0.13904263079166412\n",
      "Max gradient at step 1021: 0.17106051743030548\n",
      "Max gradient at step 1022: 0.13091959059238434\n",
      "Max gradient at step 1023: 0.15550924837589264\n",
      "Max gradient at step 1024: 0.21281014382839203\n",
      "Max gradient at step 1025: 0.23757058382034302\n",
      "Max gradient at step 1026: 0.11678755283355713\n",
      "Max gradient at step 1027: 0.19204673171043396\n",
      "Max gradient at step 1028: 0.22923104465007782\n",
      "Max gradient at step 1029: 0.14468015730381012\n",
      "Max gradient at step 1030: 0.1920664757490158\n",
      "Max gradient at step 1031: 0.17448307573795319\n",
      "Max gradient at step 1032: 0.13797707855701447\n",
      "Max gradient at step 1033: 0.1900569200515747\n",
      "Max gradient at step 1034: 0.10479563474655151\n",
      "Max gradient at step 1035: 0.168008953332901\n",
      "Max gradient at step 1036: 0.22427117824554443\n",
      "Max gradient at step 1037: 0.04955308139324188\n",
      "Max gradient at step 1038: 0.15687505900859833\n",
      "Max gradient at step 1039: 0.13378587365150452\n",
      "Max gradient at step 1040: 0.13933268189430237\n",
      "Max gradient at step 1041: 0.1664448082447052\n",
      "Max gradient at step 1042: 0.24399714171886444\n",
      "Max gradient at step 1043: 0.1967717409133911\n",
      "Max gradient at step 1044: 0.15475982427597046\n",
      "Max gradient at step 1045: 0.15412966907024384\n",
      "Max gradient at step 1046: 0.1450222134590149\n",
      "Max gradient at step 1047: 0.15515393018722534\n",
      "Max gradient at step 1048: 0.19516421854496002\n",
      "Max gradient at step 1049: 0.16562558710575104\n",
      "Max gradient at step 1050: 0.22531048953533173\n",
      "Max gradient at step 1051: 0.22253988683223724\n",
      "Max gradient at step 1052: 0.14121505618095398\n",
      "Max gradient at step 1053: 0.1123606264591217\n",
      "Max gradient at step 1054: 0.2313760221004486\n",
      "Max gradient at step 1055: 0.24355025589466095\n",
      "Max gradient at step 1056: 0.15924224257469177\n",
      "Max gradient at step 1057: 0.08556099981069565\n",
      "Max gradient at step 1058: 0.23671254515647888\n",
      "Max gradient at step 1059: 0.16403894126415253\n",
      "Max gradient at step 1060: 0.14483188092708588\n",
      "Max gradient at step 1061: 0.23145806789398193\n",
      "Max gradient at step 1062: 0.12445934116840363\n",
      "Max gradient at step 1063: 0.15220148861408234\n",
      "Max gradient at step 1064: 0.1474214643239975\n",
      "Max gradient at step 1065: 0.2143215388059616\n",
      "Max gradient at step 1066: 0.17828789353370667\n",
      "Max gradient at step 1067: 0.05391254276037216\n",
      "Max gradient at step 1068: 0.12953796982765198\n",
      "Max gradient at step 1069: 0.22216130793094635\n",
      "Max gradient at step 1070: 0.34313568472862244\n",
      "Max gradient at step 1071: 0.19210541248321533\n",
      "Max gradient at step 1072: 0.18007779121398926\n",
      "Max gradient at step 1073: 0.20352086424827576\n",
      "Max gradient at step 1074: 0.18884006142616272\n",
      "Max gradient at step 1075: 0.19190460443496704\n",
      "Max gradient at step 1076: 0.14821065962314606\n",
      "Max gradient at step 1077: 0.2691883444786072\n",
      "Max gradient at step 1078: 0.2557796239852905\n",
      "Max gradient at step 1079: 0.33493879437446594\n",
      "Max gradient at step 1080: 0.18663449585437775\n",
      "Max gradient at step 1081: 0.14714135229587555\n",
      "Max gradient at step 1082: 0.20079590380191803\n",
      "Max gradient at step 1083: 0.20257635414600372\n",
      "Max gradient at step 1084: 0.20740385353565216\n",
      "Max gradient at step 1085: 0.21655525267124176\n",
      "Max gradient at step 1086: 0.14881771802902222\n",
      "Max gradient at step 1087: 0.12442746013402939\n",
      "Max gradient at step 1088: 0.08739100396633148\n",
      "Max gradient at step 1089: 0.2000693678855896\n",
      "Max gradient at step 1090: 0.18606014549732208\n",
      "Max gradient at step 1091: 0.14623500406742096\n",
      "Max gradient at step 1092: 0.23824302852153778\n",
      "Max gradient at step 1093: 0.14574341475963593\n",
      "Max gradient at step 1094: 0.1518135368824005\n",
      "Max gradient at step 1095: 0.14596958458423615\n",
      "Max gradient at step 1096: 0.1138063594698906\n",
      "Max gradient at step 1097: 0.25163355469703674\n",
      "Max gradient at step 1098: 0.17741598188877106\n",
      "Max gradient at step 1099: 0.1609649956226349\n",
      "Max gradient at step 1100: 0.2464427947998047\n",
      "Training loss (for one batch) at step 1100: 0.3194\n",
      "Max gradient at step 1101: 0.20251786708831787\n",
      "Max gradient at step 1102: 0.10602253675460815\n",
      "Max gradient at step 1103: 0.08846934884786606\n",
      "Max gradient at step 1104: 0.20003393292427063\n",
      "Max gradient at step 1105: 0.15410706400871277\n",
      "Max gradient at step 1106: 0.2272559553384781\n",
      "Max gradient at step 1107: 0.21545997262001038\n",
      "Max gradient at step 1108: 0.21753807365894318\n",
      "Max gradient at step 1109: 0.13698138296604156\n",
      "Max gradient at step 1110: 0.2817695140838623\n",
      "Max gradient at step 1111: 0.19347551465034485\n",
      "Max gradient at step 1112: 0.09672266244888306\n",
      "Max gradient at step 1113: 0.10620685666799545\n",
      "Max gradient at step 1114: 0.09888461977243423\n",
      "Max gradient at step 1115: 0.10432992875576019\n",
      "Max gradient at step 1116: 0.09350226819515228\n",
      "Max gradient at step 1117: 0.17916303873062134\n",
      "Max gradient at step 1118: 0.2314007729291916\n",
      "Max gradient at step 1119: 0.18100273609161377\n",
      "Max gradient at step 1120: 0.1291375607252121\n",
      "Max gradient at step 1121: 0.19162757694721222\n",
      "Max gradient at step 1122: 0.15001027286052704\n",
      "Max gradient at step 1123: 0.2554675042629242\n",
      "Max gradient at step 1124: 0.18342919647693634\n",
      "Max gradient at step 1125: 0.16124214231967926\n",
      "Max gradient at step 1126: 0.10707959532737732\n",
      "Max gradient at step 1127: 0.16037343442440033\n",
      "Max gradient at step 1128: 0.27209579944610596\n",
      "Max gradient at step 1129: 0.13375061750411987\n",
      "Max gradient at step 1130: 0.12802685797214508\n",
      "Max gradient at step 1131: 0.27027004957199097\n",
      "Max gradient at step 1132: 0.13736484944820404\n",
      "Max gradient at step 1133: 0.18981748819351196\n",
      "Max gradient at step 1134: 0.2435663342475891\n",
      "Max gradient at step 1135: 0.19207845628261566\n",
      "Max gradient at step 1136: 0.12293102592229843\n",
      "Max gradient at step 1137: 0.19809876382350922\n",
      "Max gradient at step 1138: 0.13984884321689606\n",
      "Max gradient at step 1139: 0.2540148198604584\n",
      "Max gradient at step 1140: 0.12922656536102295\n",
      "Max gradient at step 1141: 0.1465623676776886\n",
      "Max gradient at step 1142: 0.09205151349306107\n",
      "Max gradient at step 1143: 0.23468753695487976\n",
      "Max gradient at step 1144: 0.22219325602054596\n",
      "Max gradient at step 1145: 0.21997883915901184\n",
      "Max gradient at step 1146: 0.2517859935760498\n",
      "Max gradient at step 1147: 0.24704478681087494\n",
      "Max gradient at step 1148: 0.18487975001335144\n",
      "Max gradient at step 1149: 0.2824096977710724\n",
      "Max gradient at step 1150: 0.14131218194961548\n",
      "Max gradient at step 1151: 0.2026829868555069\n",
      "Max gradient at step 1152: 0.15346583724021912\n",
      "Max gradient at step 1153: 0.22423401474952698\n",
      "Max gradient at step 1154: 0.1310187578201294\n",
      "Max gradient at step 1155: 0.18137170374393463\n",
      "Max gradient at step 1156: 0.184565469622612\n",
      "Max gradient at step 1157: 0.18143230676651\n",
      "Max gradient at step 1158: 0.27452796697616577\n",
      "Max gradient at step 1159: 0.21477526426315308\n",
      "Max gradient at step 1160: 0.18497557938098907\n",
      "Max gradient at step 1161: 0.30859288573265076\n",
      "Max gradient at step 1162: 0.14044463634490967\n",
      "Max gradient at step 1163: 0.18858443200588226\n",
      "Max gradient at step 1164: 0.2423902153968811\n",
      "Max gradient at step 1165: 0.19311164319515228\n",
      "Max gradient at step 1166: 0.15563330054283142\n",
      "Max gradient at step 1167: 0.36863210797309875\n",
      "Max gradient at step 1168: 0.13072067499160767\n",
      "Max gradient at step 1169: 0.15004223585128784\n",
      "Max gradient at step 1170: 0.0768953412771225\n",
      "Max gradient at step 1171: 0.18477994203567505\n",
      "Max gradient at step 1172: 0.08657485246658325\n",
      "Max gradient at step 1173: 0.21154344081878662\n",
      "Max gradient at step 1174: 0.23082062602043152\n",
      "Max gradient at step 1175: 0.2305833399295807\n",
      "Max gradient at step 1176: 0.15186288952827454\n",
      "Max gradient at step 1177: 0.173671156167984\n",
      "Max gradient at step 1178: 0.1673360913991928\n",
      "Max gradient at step 1179: 0.11327800154685974\n",
      "Max gradient at step 1180: 0.16407813131809235\n",
      "Max gradient at step 1181: 0.12312360852956772\n",
      "Max gradient at step 1182: 0.25690335035324097\n",
      "Max gradient at step 1183: 0.3277445137500763\n",
      "Max gradient at step 1184: 0.14530783891677856\n",
      "Max gradient at step 1185: 0.14920338988304138\n",
      "Max gradient at step 1186: 0.22055257856845856\n",
      "Max gradient at step 1187: 0.2791391909122467\n",
      "Max gradient at step 1188: 0.2008284628391266\n",
      "Max gradient at step 1189: 0.186989888548851\n",
      "Max gradient at step 1190: 0.13118360936641693\n",
      "Max gradient at step 1191: 0.3360665440559387\n",
      "Max gradient at step 1192: 0.11938140541315079\n",
      "Max gradient at step 1193: 0.17046841979026794\n",
      "Max gradient at step 1194: 0.21347393095493317\n",
      "Max gradient at step 1195: 0.16592004895210266\n",
      "Max gradient at step 1196: 0.155111163854599\n",
      "Max gradient at step 1197: 0.12403873354196548\n",
      "Max gradient at step 1198: 0.09731005877256393\n",
      "Max gradient at step 1199: 0.13352163136005402\n",
      "Max gradient at step 1200: 0.15507571399211884\n",
      "Training loss (for one batch) at step 1200: 0.1962\n",
      "Max gradient at step 1201: 0.260074645280838\n",
      "Max gradient at step 1202: 0.2599460184574127\n",
      "Max gradient at step 1203: 0.33747732639312744\n",
      "Max gradient at step 1204: 0.1135975569486618\n",
      "Max gradient at step 1205: 0.12225690484046936\n",
      "Max gradient at step 1206: 0.18974480032920837\n",
      "Max gradient at step 1207: 0.12298829853534698\n",
      "Max gradient at step 1208: 0.19923079013824463\n",
      "Max gradient at step 1209: 0.19146554172039032\n",
      "Max gradient at step 1210: 0.13727125525474548\n",
      "Max gradient at step 1211: 0.500296413898468\n",
      "Max gradient at step 1212: 0.17451681196689606\n",
      "Max gradient at step 1213: 0.1309843361377716\n",
      "Max gradient at step 1214: 0.1409890204668045\n",
      "Max gradient at step 1215: 0.37107983231544495\n",
      "Max gradient at step 1216: 0.161122128367424\n",
      "Max gradient at step 1217: 0.09569697827100754\n",
      "Max gradient at step 1218: 0.1255873292684555\n",
      "Max gradient at step 1219: 0.1341862827539444\n",
      "Max gradient at step 1220: 0.16557298600673676\n",
      "Max gradient at step 1221: 0.17187806963920593\n",
      "Max gradient at step 1222: 0.22627942264080048\n",
      "Max gradient at step 1223: 0.1846078783273697\n",
      "Max gradient at step 1224: 0.20957094430923462\n",
      "Max gradient at step 1225: 0.15920235216617584\n",
      "Max gradient at step 1226: 0.13066162168979645\n",
      "Max gradient at step 1227: 0.17068971693515778\n",
      "Max gradient at step 1228: 0.1089637279510498\n",
      "Max gradient at step 1229: 0.15904517471790314\n",
      "Max gradient at step 1230: 0.1799376904964447\n",
      "Max gradient at step 1231: 0.09570112079381943\n",
      "Max gradient at step 1232: 0.15613968670368195\n",
      "Max gradient at step 1233: 0.2068294882774353\n",
      "Max gradient at step 1234: 0.07518618553876877\n",
      "Max gradient at step 1235: 0.159183070063591\n",
      "Max gradient at step 1236: 0.21793553233146667\n",
      "Max gradient at step 1237: 0.2677960693836212\n",
      "Max gradient at step 1238: 0.09075001627206802\n",
      "Max gradient at step 1239: 0.37091997265815735\n",
      "Max gradient at step 1240: 0.11520823836326599\n",
      "Max gradient at step 1241: 0.18922404944896698\n",
      "Max gradient at step 1242: 0.1563238501548767\n",
      "Max gradient at step 1243: 0.10942263156175613\n",
      "Max gradient at step 1244: 0.27373817563056946\n",
      "Max gradient at step 1245: 0.14504072070121765\n",
      "Max gradient at step 1246: 0.2151774913072586\n",
      "Max gradient at step 1247: 0.13056570291519165\n",
      "Max gradient at step 1248: 0.09580960124731064\n",
      "Max gradient at step 1249: 0.13511765003204346\n",
      "Max gradient at step 1250: 0.12704655528068542\n",
      "Max gradient at step 1251: 0.13525842130184174\n",
      "Max gradient at step 1252: 0.19252875447273254\n",
      "Max gradient at step 1253: 0.17198212444782257\n",
      "Max gradient at step 1254: 0.242566779255867\n",
      "Max gradient at step 1255: 0.16309215128421783\n",
      "Max gradient at step 1256: 0.2176048457622528\n",
      "Max gradient at step 1257: 0.09405629336833954\n",
      "Max gradient at step 1258: 0.0774768739938736\n",
      "Max gradient at step 1259: 0.20707055926322937\n",
      "Max gradient at step 1260: 0.1374744027853012\n",
      "Max gradient at step 1261: 0.1272297501564026\n",
      "Max gradient at step 1262: 0.17895792424678802\n",
      "Max gradient at step 1263: 0.208940327167511\n",
      "Max gradient at step 1264: 0.18212802708148956\n",
      "Max gradient at step 1265: 0.10535528510808945\n",
      "Max gradient at step 1266: 0.2470434159040451\n",
      "Max gradient at step 1267: 0.14742067456245422\n",
      "Max gradient at step 1268: 0.16485245525836945\n",
      "Max gradient at step 1269: 0.12805408239364624\n",
      "Max gradient at step 1270: 0.09156522899866104\n",
      "Max gradient at step 1271: 0.1452784389257431\n",
      "Max gradient at step 1272: 0.18540920317173004\n",
      "Max gradient at step 1273: 0.24426229298114777\n",
      "Max gradient at step 1274: 0.17831629514694214\n",
      "Max gradient at step 1275: 0.15722177922725677\n",
      "Max gradient at step 1276: 0.1300227791070938\n",
      "Max gradient at step 1277: 0.10120685398578644\n",
      "Max gradient at step 1278: 0.19869913160800934\n",
      "Max gradient at step 1279: 0.1321602612733841\n",
      "Max gradient at step 1280: 0.10611553490161896\n",
      "Max gradient at step 1281: 0.11694207787513733\n",
      "Max gradient at step 1282: 0.2834882438182831\n",
      "Max gradient at step 1283: 0.13689279556274414\n",
      "Max gradient at step 1284: 0.28795990347862244\n",
      "Max gradient at step 1285: 0.21175219118595123\n",
      "Max gradient at step 1286: 0.28507521748542786\n",
      "Max gradient at step 1287: 0.24885518848896027\n",
      "Max gradient at step 1288: 0.29764169454574585\n",
      "Max gradient at step 1289: 0.21041126549243927\n",
      "Max gradient at step 1290: 0.29339951276779175\n",
      "Max gradient at step 1291: 0.3188178837299347\n",
      "Max gradient at step 1292: 0.10884658992290497\n",
      "Max gradient at step 1293: 0.13397152721881866\n",
      "Max gradient at step 1294: 0.22093695402145386\n",
      "Max gradient at step 1295: 0.29225629568099976\n",
      "Max gradient at step 1296: 0.22653935849666595\n",
      "Max gradient at step 1297: 0.2395971715450287\n",
      "Max gradient at step 1298: 0.3980567753314972\n",
      "Max gradient at step 1299: 0.2435910850763321\n",
      "Max gradient at step 1300: 0.2997836172580719\n",
      "Training loss (for one batch) at step 1300: 0.6503\n",
      "Max gradient at step 1301: 0.13341407477855682\n",
      "Max gradient at step 1302: 0.2260344922542572\n",
      "Max gradient at step 1303: 0.1886664628982544\n",
      "Max gradient at step 1304: 0.11690621078014374\n",
      "Max gradient at step 1305: 0.24209299683570862\n",
      "Max gradient at step 1306: 0.10669028759002686\n",
      "Max gradient at step 1307: 0.12452641129493713\n",
      "Max gradient at step 1308: 0.1140177845954895\n",
      "Max gradient at step 1309: 0.2086762934923172\n",
      "Max gradient at step 1310: 0.1307956427335739\n",
      "Max gradient at step 1311: 0.24179479479789734\n",
      "Max gradient at step 1312: 0.24962106347084045\n",
      "Max gradient at step 1313: 0.12732692062854767\n",
      "Max gradient at step 1314: 0.1555231213569641\n",
      "Max gradient at step 1315: 0.14770729839801788\n",
      "Max gradient at step 1316: 0.26548147201538086\n",
      "Max gradient at step 1317: 0.1692694127559662\n",
      "Max gradient at step 1318: 0.23343344032764435\n",
      "Max gradient at step 1319: 0.13485758006572723\n",
      "Max gradient at step 1320: 0.16134779155254364\n",
      "Max gradient at step 1321: 0.16104601323604584\n",
      "Max gradient at step 1322: 0.17457348108291626\n",
      "Max gradient at step 1323: 0.104047492146492\n",
      "Max gradient at step 1324: 0.18024855852127075\n",
      "Max gradient at step 1325: 0.1745670884847641\n",
      "Max gradient at step 1326: 0.15428702533245087\n",
      "Max gradient at step 1327: 0.19902677834033966\n",
      "Max gradient at step 1328: 0.15575388073921204\n",
      "Max gradient at step 1329: 0.1688443422317505\n",
      "Max gradient at step 1330: 0.35855361819267273\n",
      "Max gradient at step 1331: 0.1766313910484314\n",
      "Max gradient at step 1332: 0.259227454662323\n",
      "Max gradient at step 1333: 0.2460370808839798\n",
      "Max gradient at step 1334: 0.2626643776893616\n",
      "Max gradient at step 1335: 0.11813759803771973\n",
      "Max gradient at step 1336: 0.19053395092487335\n",
      "Max gradient at step 1337: 0.04261293634772301\n",
      "Max gradient at step 1338: 0.2215113639831543\n",
      "Max gradient at step 1339: 0.25105082988739014\n",
      "Max gradient at step 1340: 0.08983652293682098\n",
      "Max gradient at step 1341: 0.13192793726921082\n",
      "Max gradient at step 1342: 0.13615866005420685\n",
      "Max gradient at step 1343: 0.21788164973258972\n",
      "Max gradient at step 1344: 0.1384686529636383\n",
      "Max gradient at step 1345: 0.23617655038833618\n",
      "Max gradient at step 1346: 0.2578586935997009\n",
      "Max gradient at step 1347: 0.07904267311096191\n",
      "Max gradient at step 1348: 0.129224956035614\n",
      "Max gradient at step 1349: 0.1254327893257141\n",
      "Max gradient at step 1350: 0.19911029934883118\n",
      "Max gradient at step 1351: 0.1974831223487854\n",
      "Max gradient at step 1352: 0.1371077299118042\n",
      "Max gradient at step 1353: 0.16521097719669342\n",
      "Max gradient at step 1354: 0.1280975341796875\n",
      "Max gradient at step 1355: 0.21990756690502167\n",
      "Max gradient at step 1356: 0.11775021255016327\n",
      "Max gradient at step 1357: 0.14539626240730286\n",
      "Max gradient at step 1358: 0.11463330686092377\n",
      "Max gradient at step 1359: 0.13513723015785217\n",
      "Max gradient at step 1360: 0.20933277904987335\n",
      "Max gradient at step 1361: 0.25486651062965393\n",
      "Max gradient at step 1362: 0.11223776638507843\n",
      "Max gradient at step 1363: 0.2148629128932953\n",
      "Max gradient at step 1364: 0.16337929666042328\n",
      "Max gradient at step 1365: 0.2096455693244934\n",
      "Max gradient at step 1366: 0.13008710741996765\n",
      "Max gradient at step 1367: 0.23828040063381195\n",
      "Max gradient at step 1368: 0.21489980816841125\n",
      "Max gradient at step 1369: 0.17648500204086304\n",
      "Max gradient at step 1370: 0.23030181229114532\n",
      "Max gradient at step 1371: 0.16656967997550964\n",
      "Max gradient at step 1372: 0.21403855085372925\n",
      "Max gradient at step 1373: 0.1365625411272049\n",
      "Max gradient at step 1374: 0.08950558304786682\n",
      "Max gradient at step 1375: 0.18931712210178375\n",
      "Max gradient at step 1376: 0.1569926142692566\n",
      "Max gradient at step 1377: 0.15603718161582947\n",
      "Max gradient at step 1378: 0.27739936113357544\n",
      "Max gradient at step 1379: 0.13690300285816193\n",
      "Max gradient at step 1380: 0.19358842074871063\n",
      "Max gradient at step 1381: 0.152686208486557\n",
      "Max gradient at step 1382: 0.2811586856842041\n",
      "Max gradient at step 1383: 0.16169928014278412\n",
      "Max gradient at step 1384: 0.15075020492076874\n",
      "Max gradient at step 1385: 0.21468128263950348\n",
      "Max gradient at step 1386: 0.2365550398826599\n",
      "Max gradient at step 1387: 0.18789876997470856\n",
      "Max gradient at step 1388: 0.13413210213184357\n",
      "Max gradient at step 1389: 0.2301148921251297\n",
      "Max gradient at step 1390: 0.19660966098308563\n",
      "Max gradient at step 1391: 0.2020409256219864\n",
      "Max gradient at step 1392: 0.23158079385757446\n",
      "Max gradient at step 1393: 0.11560114473104477\n",
      "Max gradient at step 1394: 0.21174877882003784\n",
      "Max gradient at step 1395: 0.20022815465927124\n",
      "Max gradient at step 1396: 0.22245514392852783\n",
      "Max gradient at step 1397: 0.19185101985931396\n",
      "Max gradient at step 1398: 0.22345474362373352\n",
      "Max gradient at step 1399: 0.24239033460617065\n",
      "Max gradient at step 1400: 0.18814387917518616\n",
      "Training loss (for one batch) at step 1400: 0.4185\n",
      "Max gradient at step 1401: 0.11208166182041168\n",
      "Max gradient at step 1402: 0.09064328670501709\n",
      "Max gradient at step 1403: 0.18116866052150726\n",
      "Max gradient at step 1404: 0.22847148776054382\n",
      "Max gradient at step 1405: 0.17793665826320648\n",
      "Max gradient at step 1406: 0.4243490695953369\n",
      "Max gradient at step 1407: 0.15618684887886047\n",
      "Max gradient at step 1408: 0.14488238096237183\n",
      "Max gradient at step 1409: 0.2019171565771103\n",
      "Max gradient at step 1410: 0.21264977753162384\n",
      "Max gradient at step 1411: 0.2594083845615387\n",
      "Max gradient at step 1412: 0.11114143580198288\n",
      "Max gradient at step 1413: 0.14594899117946625\n",
      "Max gradient at step 1414: 0.1040448322892189\n",
      "Max gradient at step 1415: 0.21159256994724274\n",
      "Max gradient at step 1416: 0.12241759896278381\n",
      "Max gradient at step 1417: 0.14801092445850372\n",
      "Max gradient at step 1418: 0.20792214572429657\n",
      "Max gradient at step 1419: 0.17978039383888245\n",
      "Max gradient at step 1420: 0.13685475289821625\n",
      "Max gradient at step 1421: 0.14359226822853088\n",
      "Max gradient at step 1422: 0.13091668486595154\n",
      "Max gradient at step 1423: 0.17745107412338257\n",
      "Max gradient at step 1424: 0.06346381455659866\n",
      "Max gradient at step 1425: 0.11389504373073578\n",
      "Max gradient at step 1426: 0.1203794926404953\n",
      "Max gradient at step 1427: 0.26947805285453796\n",
      "Max gradient at step 1428: 0.33606982231140137\n",
      "Max gradient at step 1429: 0.13293255865573883\n",
      "Max gradient at step 1430: 0.14570999145507812\n",
      "Max gradient at step 1431: 0.15283899009227753\n",
      "Max gradient at step 1432: 0.12409514933824539\n",
      "Max gradient at step 1433: 0.10862868279218674\n",
      "Max gradient at step 1434: 0.12077287584543228\n",
      "Max gradient at step 1435: 0.2514611780643463\n",
      "Max gradient at step 1436: 0.3965137302875519\n",
      "Max gradient at step 1437: 0.1801309585571289\n",
      "Max gradient at step 1438: 0.0810592994093895\n",
      "Max gradient at step 1439: 0.1833934336900711\n",
      "Max gradient at step 1440: 0.11918966472148895\n",
      "Max gradient at step 1441: 0.21815834939479828\n",
      "Max gradient at step 1442: 0.1941833198070526\n",
      "Max gradient at step 1443: 0.13989323377609253\n",
      "Max gradient at step 1444: 0.05853460729122162\n",
      "Max gradient at step 1445: 0.20263642072677612\n",
      "Max gradient at step 1446: 0.11691731959581375\n",
      "Max gradient at step 1447: 0.19552747905254364\n",
      "Max gradient at step 1448: 0.20696164667606354\n",
      "Max gradient at step 1449: 0.15785658359527588\n",
      "Max gradient at step 1450: 0.09316813945770264\n",
      "Max gradient at step 1451: 0.2028946578502655\n",
      "Max gradient at step 1452: 0.13676950335502625\n",
      "Max gradient at step 1453: 0.19422832131385803\n",
      "Max gradient at step 1454: 0.20315024256706238\n",
      "Max gradient at step 1455: 0.1495324671268463\n",
      "Max gradient at step 1456: 0.12342047691345215\n",
      "Max gradient at step 1457: 0.20376351475715637\n",
      "Max gradient at step 1458: 0.17726731300354004\n",
      "Max gradient at step 1459: 0.2038479745388031\n",
      "Max gradient at step 1460: 0.14112316071987152\n",
      "Max gradient at step 1461: 0.31859904527664185\n",
      "Max gradient at step 1462: 0.14434824883937836\n",
      "Max gradient at step 1463: 0.2562344968318939\n",
      "Max gradient at step 1464: 0.12654510140419006\n",
      "Max gradient at step 1465: 0.1914980113506317\n",
      "Max gradient at step 1466: 0.11444708704948425\n",
      "Max gradient at step 1467: 0.24876023828983307\n",
      "Max gradient at step 1468: 0.16743038594722748\n",
      "Max gradient at step 1469: 0.25490182638168335\n",
      "Max gradient at step 1470: 0.2578795850276947\n",
      "Max gradient at step 1471: 0.22854776680469513\n",
      "Max gradient at step 1472: 0.23714323341846466\n",
      "Max gradient at step 1473: 0.1170310229063034\n",
      "Max gradient at step 1474: 0.155888631939888\n",
      "Max gradient at step 1475: 0.24997325241565704\n",
      "Max gradient at step 1476: 0.2921128571033478\n",
      "Max gradient at step 1477: 0.17234542965888977\n",
      "Max gradient at step 1478: 0.16849467158317566\n",
      "Max gradient at step 1479: 0.1160903200507164\n",
      "Max gradient at step 1480: 0.08524490892887115\n",
      "Max gradient at step 1481: 0.1158934161067009\n",
      "Max gradient at step 1482: 0.14165331423282623\n",
      "Max gradient at step 1483: 0.24236449599266052\n",
      "Max gradient at step 1484: 0.2866196036338806\n",
      "Max gradient at step 1485: 0.12641189992427826\n",
      "Max gradient at step 1486: 0.09733913093805313\n",
      "Max gradient at step 1487: 0.1569526046514511\n",
      "Max gradient at step 1488: 0.2045137584209442\n",
      "Max gradient at step 1489: 0.17648327350616455\n",
      "Max gradient at step 1490: 0.22896289825439453\n",
      "Max gradient at step 1491: 0.2170683741569519\n",
      "Max gradient at step 1492: 0.22708342969417572\n",
      "Max gradient at step 1493: 0.12009953707456589\n",
      "Max gradient at step 1494: 0.21107260882854462\n",
      "Max gradient at step 1495: 0.16743217408657074\n",
      "Max gradient at step 1496: 0.11314496397972107\n",
      "Max gradient at step 1497: 0.22790488600730896\n",
      "Max gradient at step 1498: 0.26291054487228394\n",
      "Max gradient at step 1499: 0.21002717316150665\n",
      "Max gradient at step 1500: 0.14075641334056854\n",
      "Training loss (for one batch) at step 1500: 0.1359\n",
      "Max gradient at step 1501: 0.1996690034866333\n",
      "Max gradient at step 1502: 0.1373482197523117\n",
      "Max gradient at step 1503: 0.22459855675697327\n",
      "Max gradient at step 1504: 0.1719592809677124\n",
      "Max gradient at step 1505: 0.22023987770080566\n",
      "Max gradient at step 1506: 0.3500109314918518\n",
      "Max gradient at step 1507: 0.34377795457839966\n",
      "Max gradient at step 1508: 0.16062837839126587\n",
      "Max gradient at step 1509: 0.1444697231054306\n",
      "Max gradient at step 1510: 0.27621686458587646\n",
      "Max gradient at step 1511: 0.19727906584739685\n",
      "Max gradient at step 1512: 0.08599122613668442\n",
      "Max gradient at step 1513: 0.20754499733448029\n",
      "Max gradient at step 1514: 0.30327194929122925\n",
      "Max gradient at step 1515: 0.12639421224594116\n",
      "Max gradient at step 1516: 0.18669632077217102\n",
      "Max gradient at step 1517: 0.13672898709774017\n",
      "Max gradient at step 1518: 0.38716477155685425\n",
      "Max gradient at step 1519: 0.23344746232032776\n",
      "Max gradient at step 1520: 0.13464805483818054\n",
      "Max gradient at step 1521: 0.156265988945961\n",
      "Max gradient at step 1522: 0.24614985287189484\n",
      "Max gradient at step 1523: 0.2148231565952301\n",
      "Max gradient at step 1524: 0.1680314689874649\n",
      "Max gradient at step 1525: 0.19331319630146027\n",
      "Max gradient at step 1526: 0.24440692365169525\n",
      "Max gradient at step 1527: 0.19234275817871094\n",
      "Max gradient at step 1528: 0.08969689905643463\n",
      "Max gradient at step 1529: 0.1650547981262207\n",
      "Max gradient at step 1530: 0.18853913247585297\n",
      "Max gradient at step 1531: 0.19064196944236755\n",
      "Max gradient at step 1532: 0.08487490564584732\n",
      "Max gradient at step 1533: 0.1844416856765747\n",
      "Max gradient at step 1534: 0.19341371953487396\n",
      "Max gradient at step 1535: 0.22311745584011078\n",
      "Max gradient at step 1536: 0.12146613746881485\n",
      "Max gradient at step 1537: 0.20139586925506592\n",
      "Max gradient at step 1538: 0.13572783768177032\n",
      "Max gradient at step 1539: 0.10160701721906662\n",
      "Max gradient at step 1540: 0.4520442485809326\n",
      "Max gradient at step 1541: 0.2858273684978485\n",
      "Max gradient at step 1542: 0.16871675848960876\n",
      "Max gradient at step 1543: 0.19842499494552612\n",
      "Max gradient at step 1544: 0.12130114436149597\n",
      "Max gradient at step 1545: 0.19840402901172638\n",
      "Max gradient at step 1546: 0.1259423941373825\n",
      "Max gradient at step 1547: 0.13638079166412354\n",
      "Max gradient at step 1548: 0.3188779652118683\n",
      "Max gradient at step 1549: 0.09405314922332764\n",
      "Max gradient at step 1550: 0.14295686781406403\n",
      "Max gradient at step 1551: 0.2491295486688614\n",
      "Max gradient at step 1552: 0.1517806053161621\n",
      "Max gradient at step 1553: 0.18140026926994324\n",
      "Max gradient at step 1554: 0.20847278833389282\n",
      "Max gradient at step 1555: 0.1265418380498886\n",
      "Max gradient at step 1556: 0.280015230178833\n",
      "Max gradient at step 1557: 0.1262553334236145\n",
      "Max gradient at step 1558: 0.10885972529649734\n",
      "Max gradient at step 1559: 0.1599847376346588\n",
      "Max gradient at step 1560: 0.2096513956785202\n",
      "Max gradient at step 1561: 0.2740863561630249\n",
      "Max gradient at step 1562: 0.35472390055656433\n",
      "Max gradient at step 1563: 0.24121138453483582\n",
      "Max gradient at step 1564: 0.12946470081806183\n",
      "Max gradient at step 1565: 0.13838428258895874\n",
      "Max gradient at step 1566: 0.13012436032295227\n",
      "Max gradient at step 1567: 0.225472554564476\n",
      "Max gradient at step 1568: 0.08517100661993027\n",
      "Max gradient at step 1569: 0.20655570924282074\n",
      "Max gradient at step 1570: 0.20002590119838715\n",
      "Max gradient at step 1571: 0.1510588824748993\n",
      "Max gradient at step 1572: 0.11949644237756729\n",
      "Max gradient at step 1573: 0.23839305341243744\n",
      "Max gradient at step 1574: 0.27880871295928955\n",
      "Max gradient at step 1575: 0.17796657979488373\n",
      "Max gradient at step 1576: 0.17109249532222748\n",
      "Max gradient at step 1577: 0.2178584337234497\n",
      "Max gradient at step 1578: 0.23715414106845856\n",
      "Max gradient at step 1579: 0.2236919105052948\n",
      "Max gradient at step 1580: 0.1709688901901245\n",
      "Max gradient at step 1581: 0.09876565635204315\n",
      "Max gradient at step 1582: 0.07697372138500214\n",
      "Max gradient at step 1583: 0.23629991710186005\n",
      "Max gradient at step 1584: 0.18016190826892853\n",
      "Max gradient at step 1585: 0.1576114296913147\n",
      "Max gradient at step 1586: 0.24270711839199066\n",
      "Max gradient at step 1587: 0.21252937614917755\n",
      "Max gradient at step 1588: 0.17827469110488892\n",
      "Max gradient at step 1589: 0.1675310730934143\n",
      "Max gradient at step 1590: 0.19288691878318787\n",
      "Max gradient at step 1591: 0.1693766862154007\n",
      "Max gradient at step 1592: 0.08789530396461487\n",
      "Max gradient at step 1593: 0.1152692511677742\n",
      "Max gradient at step 1594: 0.12225162982940674\n",
      "Max gradient at step 1595: 0.132632315158844\n",
      "Max gradient at step 1596: 0.13068611919879913\n",
      "Max gradient at step 1597: 0.13731753826141357\n",
      "Max gradient at step 1598: 0.4316594898700714\n",
      "Max gradient at step 1599: 0.1431441456079483\n",
      "Max gradient at step 1600: 0.1523684710264206\n",
      "Training loss (for one batch) at step 1600: 0.3113\n",
      "Max gradient at step 1601: 0.13545387983322144\n",
      "Max gradient at step 1602: 0.10097746551036835\n",
      "Max gradient at step 1603: 0.2958880066871643\n",
      "Max gradient at step 1604: 0.10656626522541046\n",
      "Max gradient at step 1605: 0.1838967353105545\n",
      "Max gradient at step 1606: 0.18691827356815338\n",
      "Max gradient at step 1607: 0.14655713737010956\n",
      "Max gradient at step 1608: 0.0785958543419838\n",
      "Max gradient at step 1609: 0.22328510880470276\n",
      "Max gradient at step 1610: 0.13352727890014648\n",
      "Max gradient at step 1611: 0.2151278704404831\n",
      "Max gradient at step 1612: 0.21650752425193787\n",
      "Max gradient at step 1613: 0.1398097276687622\n",
      "Max gradient at step 1614: 0.09987621754407883\n",
      "Max gradient at step 1615: 0.09545233845710754\n",
      "Max gradient at step 1616: 0.18858753144741058\n",
      "Max gradient at step 1617: 0.2873832881450653\n",
      "Max gradient at step 1618: 0.14149154722690582\n",
      "Max gradient at step 1619: 0.3445591628551483\n",
      "Max gradient at step 1620: 0.1589190810918808\n",
      "Max gradient at step 1621: 0.16578716039657593\n",
      "Max gradient at step 1622: 0.16231310367584229\n",
      "Max gradient at step 1623: 0.2800018787384033\n",
      "Max gradient at step 1624: 0.4201516807079315\n",
      "Max gradient at step 1625: 0.19380350410938263\n",
      "Max gradient at step 1626: 0.24423594772815704\n",
      "Max gradient at step 1627: 0.22219613194465637\n",
      "Max gradient at step 1628: 0.1666032373905182\n",
      "Max gradient at step 1629: 0.27252039313316345\n",
      "Max gradient at step 1630: 0.25874918699264526\n",
      "Max gradient at step 1631: 0.2064679116010666\n",
      "Max gradient at step 1632: 0.14067193865776062\n",
      "Max gradient at step 1633: 0.1875428706407547\n",
      "Max gradient at step 1634: 0.1797749251127243\n",
      "Max gradient at step 1635: 0.17845699191093445\n",
      "Max gradient at step 1636: 0.15326723456382751\n",
      "Max gradient at step 1637: 0.11972151696681976\n",
      "Max gradient at step 1638: 0.23667101562023163\n",
      "Max gradient at step 1639: 0.2268529236316681\n",
      "Max gradient at step 1640: 0.13889269530773163\n",
      "Max gradient at step 1641: 0.22397294640541077\n",
      "Max gradient at step 1642: 0.19622036814689636\n",
      "Max gradient at step 1643: 0.22840140759944916\n",
      "Max gradient at step 1644: 0.2565068006515503\n",
      "Max gradient at step 1645: 0.23760657012462616\n",
      "Max gradient at step 1646: 0.17186681926250458\n",
      "Max gradient at step 1647: 0.17679567635059357\n",
      "Max gradient at step 1648: 0.16633068025112152\n",
      "Max gradient at step 1649: 0.24183599650859833\n",
      "Max gradient at step 1650: 0.17334474623203278\n",
      "Max gradient at step 1651: 0.3090115487575531\n",
      "Max gradient at step 1652: 0.09223414212465286\n",
      "Max gradient at step 1653: 0.22971130907535553\n",
      "Max gradient at step 1654: 0.12269702553749084\n",
      "Max gradient at step 1655: 0.09822477400302887\n",
      "Max gradient at step 1656: 0.2746068835258484\n",
      "Max gradient at step 1657: 0.08374656736850739\n",
      "Max gradient at step 1658: 0.23922137916088104\n",
      "Max gradient at step 1659: 0.4075907766819\n",
      "Max gradient at step 1660: 0.13335582613945007\n",
      "Max gradient at step 1661: 0.11621455103158951\n",
      "Max gradient at step 1662: 0.13218873739242554\n",
      "Max gradient at step 1663: 0.1426590383052826\n",
      "Max gradient at step 1664: 0.16212677955627441\n",
      "Max gradient at step 1665: 0.3111869990825653\n",
      "Max gradient at step 1666: 0.2077476680278778\n",
      "Max gradient at step 1667: 0.2035708725452423\n",
      "Max gradient at step 1668: 0.07519122213125229\n",
      "Max gradient at step 1669: 0.08376532793045044\n",
      "Max gradient at step 1670: 0.14374586939811707\n",
      "Max gradient at step 1671: 0.1351253241300583\n",
      "Max gradient at step 1672: 0.14267770946025848\n",
      "Max gradient at step 1673: 0.19133450090885162\n",
      "Max gradient at step 1674: 0.2044648975133896\n",
      "Max gradient at step 1675: 0.14311444759368896\n",
      "Max gradient at step 1676: 0.13627350330352783\n",
      "Max gradient at step 1677: 0.19486524164676666\n",
      "Max gradient at step 1678: 0.254169225692749\n",
      "Max gradient at step 1679: 0.09771737456321716\n",
      "Max gradient at step 1680: 0.20100362598896027\n",
      "Max gradient at step 1681: 0.0816207006573677\n",
      "Max gradient at step 1682: 0.13354605436325073\n",
      "Max gradient at step 1683: 0.15857794880867004\n",
      "Max gradient at step 1684: 0.2587873637676239\n",
      "Max gradient at step 1685: 0.14503298699855804\n",
      "Max gradient at step 1686: 0.12858182191848755\n",
      "Max gradient at step 1687: 0.1580112874507904\n",
      "Max gradient at step 1688: 0.21433237195014954\n",
      "Max gradient at step 1689: 0.18078385293483734\n",
      "Max gradient at step 1690: 0.23925063014030457\n",
      "Max gradient at step 1691: 0.20553502440452576\n",
      "Max gradient at step 1692: 0.211542010307312\n",
      "Max gradient at step 1693: 0.19154049456119537\n",
      "Max gradient at step 1694: 0.15300394594669342\n",
      "Max gradient at step 1695: 0.23541057109832764\n",
      "Max gradient at step 1696: 0.2250770777463913\n",
      "Max gradient at step 1697: 0.2075241655111313\n",
      "Max gradient at step 1698: 0.23650260269641876\n",
      "Max gradient at step 1699: 0.31122878193855286\n",
      "Max gradient at step 1700: 0.21260236203670502\n",
      "Training loss (for one batch) at step 1700: 0.3949\n",
      "Max gradient at step 1701: 0.20850643515586853\n",
      "Max gradient at step 1702: 0.10724932700395584\n",
      "Max gradient at step 1703: 0.28239336609840393\n",
      "Max gradient at step 1704: 0.20703819394111633\n",
      "Max gradient at step 1705: 0.1450863778591156\n",
      "Max gradient at step 1706: 0.24839529395103455\n",
      "Max gradient at step 1707: 0.10574404150247574\n",
      "Max gradient at step 1708: 0.2147393524646759\n",
      "Max gradient at step 1709: 0.18049439787864685\n",
      "Max gradient at step 1710: 0.18336346745491028\n",
      "Max gradient at step 1711: 0.1344761699438095\n",
      "Max gradient at step 1712: 0.2519538700580597\n",
      "Max gradient at step 1713: 0.12190844863653183\n",
      "Max gradient at step 1714: 0.23931178450584412\n",
      "Max gradient at step 1715: 0.2990788221359253\n",
      "Max gradient at step 1716: 0.18637564778327942\n",
      "Max gradient at step 1717: 0.14450977742671967\n",
      "Max gradient at step 1718: 0.13104061782360077\n",
      "Max gradient at step 1719: 0.1072695180773735\n",
      "Max gradient at step 1720: 0.18764057755470276\n",
      "Max gradient at step 1721: 0.10613670945167542\n",
      "Max gradient at step 1722: 0.20769257843494415\n",
      "Max gradient at step 1723: 0.1525380164384842\n",
      "Max gradient at step 1724: 0.2823617458343506\n",
      "Max gradient at step 1725: 0.10288288444280624\n",
      "Max gradient at step 1726: 0.1553017944097519\n",
      "Max gradient at step 1727: 0.12290247529745102\n",
      "Max gradient at step 1728: 0.09258566051721573\n",
      "Max gradient at step 1729: 0.22214765846729279\n",
      "Max gradient at step 1730: 0.1481921374797821\n",
      "Max gradient at step 1731: 0.08952435106039047\n",
      "Max gradient at step 1732: 0.057841453701257706\n",
      "Max gradient at step 1733: 0.2271868884563446\n",
      "Max gradient at step 1734: 0.1767873764038086\n",
      "Max gradient at step 1735: 0.15968242287635803\n",
      "Max gradient at step 1736: 0.11719998717308044\n",
      "Max gradient at step 1737: 0.24989277124404907\n",
      "Max gradient at step 1738: 0.1974233090877533\n",
      "Max gradient at step 1739: 0.3705662786960602\n",
      "Max gradient at step 1740: 0.1689654439687729\n",
      "Max gradient at step 1741: 0.1733267903327942\n",
      "Max gradient at step 1742: 0.16594849526882172\n",
      "Max gradient at step 1743: 0.16207784414291382\n",
      "Max gradient at step 1744: 0.1806012988090515\n",
      "Max gradient at step 1745: 0.157063826918602\n",
      "Max gradient at step 1746: 0.13404977321624756\n",
      "Max gradient at step 1747: 0.08062276989221573\n",
      "Max gradient at step 1748: 0.15833322703838348\n",
      "Max gradient at step 1749: 0.18182991445064545\n",
      "Max gradient at step 1750: 0.15436865389347076\n",
      "Max gradient at step 1751: 0.1287614405155182\n",
      "Max gradient at step 1752: 0.12007827311754227\n",
      "Max gradient at step 1753: 0.2286204844713211\n",
      "Max gradient at step 1754: 0.14707857370376587\n",
      "Max gradient at step 1755: 0.13457852602005005\n",
      "Max gradient at step 1756: 0.22125059366226196\n",
      "Max gradient at step 1757: 0.14687831699848175\n",
      "Max gradient at step 1758: 0.14331787824630737\n",
      "Max gradient at step 1759: 0.11076349020004272\n",
      "Max gradient at step 1760: 0.139105886220932\n",
      "Max gradient at step 1761: 0.1192077100276947\n",
      "Max gradient at step 1762: 0.1951635777950287\n",
      "Max gradient at step 1763: 0.24018867313861847\n",
      "Max gradient at step 1764: 0.19193610548973083\n",
      "Max gradient at step 1765: 0.35322991013526917\n",
      "Max gradient at step 1766: 0.28970950841903687\n",
      "Max gradient at step 1767: 0.12546294927597046\n",
      "Max gradient at step 1768: 0.18488089740276337\n",
      "Max gradient at step 1769: 0.14721152186393738\n",
      "Max gradient at step 1770: 0.17412520945072174\n",
      "Max gradient at step 1771: 0.2605063021183014\n",
      "Max gradient at step 1772: 0.21531932055950165\n",
      "Max gradient at step 1773: 0.11130297929048538\n",
      "Max gradient at step 1774: 0.15004104375839233\n",
      "Max gradient at step 1775: 0.1286786049604416\n",
      "Max gradient at step 1776: 0.18940280377864838\n",
      "Max gradient at step 1777: 0.24716167151927948\n",
      "Max gradient at step 1778: 0.14790645241737366\n",
      "Max gradient at step 1779: 0.17171022295951843\n",
      "Max gradient at step 1780: 0.10702961683273315\n",
      "Max gradient at step 1781: 0.10537493973970413\n",
      "Max gradient at step 1782: 0.19313111901283264\n",
      "Max gradient at step 1783: 0.19127140939235687\n",
      "Max gradient at step 1784: 0.04628152400255203\n",
      "Max gradient at step 1785: 0.13803419470787048\n",
      "Max gradient at step 1786: 0.17654988169670105\n",
      "Max gradient at step 1787: 0.12546730041503906\n",
      "Max gradient at step 1788: 0.16706523299217224\n",
      "Max gradient at step 1789: 0.17316679656505585\n",
      "Max gradient at step 1790: 0.26249849796295166\n",
      "Max gradient at step 1791: 0.15031994879245758\n",
      "Max gradient at step 1792: 0.16264714300632477\n",
      "Max gradient at step 1793: 0.15519841015338898\n",
      "Max gradient at step 1794: 0.11757134646177292\n",
      "Max gradient at step 1795: 0.07623066753149033\n",
      "Max gradient at step 1796: 0.13842074573040009\n",
      "Max gradient at step 1797: 0.14473329484462738\n",
      "Max gradient at step 1798: 0.1599663496017456\n",
      "Max gradient at step 1799: 0.17882022261619568\n",
      "Max gradient at step 1800: 0.08413894474506378\n",
      "Training loss (for one batch) at step 1800: 0.0445\n",
      "Max gradient at step 1801: 0.20032057166099548\n",
      "Max gradient at step 1802: 0.07387629896402359\n",
      "Max gradient at step 1803: 0.2007736712694168\n",
      "Max gradient at step 1804: 0.15494734048843384\n",
      "Max gradient at step 1805: 0.20500600337982178\n",
      "Max gradient at step 1806: 0.18124201893806458\n",
      "Max gradient at step 1807: 0.15022152662277222\n",
      "Max gradient at step 1808: 0.1264815330505371\n",
      "Max gradient at step 1809: 0.10664784163236618\n",
      "Max gradient at step 1810: 0.12157967686653137\n",
      "Max gradient at step 1811: 0.1526077538728714\n",
      "Max gradient at step 1812: 0.08817915618419647\n",
      "Max gradient at step 1813: 0.26411083340644836\n",
      "Max gradient at step 1814: 0.17603717744350433\n",
      "Max gradient at step 1815: 0.12557163834571838\n",
      "Max gradient at step 1816: 0.1991576999425888\n",
      "Max gradient at step 1817: 0.1471312791109085\n",
      "Max gradient at step 1818: 0.13435441255569458\n",
      "Max gradient at step 1819: 0.31288477778434753\n",
      "Max gradient at step 1820: 0.043952759355306625\n",
      "Max gradient at step 1821: 0.11477049440145493\n",
      "Max gradient at step 1822: 0.07903415709733963\n",
      "Max gradient at step 1823: 0.06208730861544609\n",
      "Max gradient at step 1824: 0.1427568793296814\n",
      "Max gradient at step 1825: 0.2052723467350006\n",
      "Max gradient at step 1826: 0.122207410633564\n",
      "Max gradient at step 1827: 0.1329602301120758\n",
      "Max gradient at step 1828: 0.21752730011940002\n",
      "Max gradient at step 1829: 0.11355060338973999\n",
      "Max gradient at step 1830: 0.0946047231554985\n",
      "Max gradient at step 1831: 0.13668376207351685\n",
      "Max gradient at step 1832: 0.1141069233417511\n",
      "Max gradient at step 1833: 0.26301562786102295\n",
      "Max gradient at step 1834: 0.10468070954084396\n",
      "Max gradient at step 1835: 0.18084226548671722\n",
      "Max gradient at step 1836: 0.13603980839252472\n",
      "Max gradient at step 1837: 0.1521676927804947\n",
      "Max gradient at step 1838: 0.15151290595531464\n",
      "Max gradient at step 1839: 0.1461869776248932\n",
      "Max gradient at step 1840: 0.16152416169643402\n",
      "Max gradient at step 1841: 0.16467243432998657\n",
      "Max gradient at step 1842: 0.14558790624141693\n",
      "Max gradient at step 1843: 0.1182318702340126\n",
      "Max gradient at step 1844: 0.09947611391544342\n",
      "Max gradient at step 1845: 0.15028876066207886\n",
      "Max gradient at step 1846: 0.10387256741523743\n",
      "Max gradient at step 1847: 0.1672726571559906\n",
      "Max gradient at step 1848: 0.22363658249378204\n",
      "Max gradient at step 1849: 0.029550306499004364\n",
      "Max gradient at step 1850: 0.15073753893375397\n",
      "Max gradient at step 1851: 0.13275103271007538\n",
      "Max gradient at step 1852: 0.1368267983198166\n",
      "Max gradient at step 1853: 0.23279207944869995\n",
      "Max gradient at step 1854: 0.10691967606544495\n",
      "Max gradient at step 1855: 0.04500066488981247\n",
      "Max gradient at step 1856: 0.1525851935148239\n",
      "Max gradient at step 1857: 0.17851565778255463\n",
      "Max gradient at step 1858: 0.18752136826515198\n",
      "Max gradient at step 1859: 0.10295191407203674\n",
      "Max gradient at step 1860: 0.18733632564544678\n",
      "Max gradient at step 1861: 0.05917531996965408\n",
      "Max gradient at step 1862: 0.21356718242168427\n",
      "Max gradient at step 1863: 0.17853356897830963\n",
      "Max gradient at step 1864: 0.033442623913288116\n",
      "Max gradient at step 1865: 0.14767876267433167\n",
      "Max gradient at step 1866: 0.12785491347312927\n",
      "Max gradient at step 1867: 0.20134031772613525\n",
      "Max gradient at step 1868: 0.13985368609428406\n",
      "Max gradient at step 1869: 0.2639014720916748\n",
      "Max gradient at step 1870: 0.08043088018894196\n",
      "Max gradient at step 1871: 0.15303586423397064\n",
      "Max gradient at step 1872: 0.10196677595376968\n",
      "Max gradient at step 1873: 0.18006326258182526\n",
      "Max gradient at step 1874: 0.05291833356022835\n",
      "Epoch 3/4\n",
      "Max gradient at step 0: 0.2267845720052719\n",
      "Training loss (for one batch) at step 0: 0.7753\n",
      "Max gradient at step 1: 0.25387874245643616\n",
      "Max gradient at step 2: 0.22898077964782715\n",
      "Max gradient at step 3: 0.2067531943321228\n",
      "Max gradient at step 4: 0.15844173729419708\n",
      "Max gradient at step 5: 0.13071097433567047\n",
      "Max gradient at step 6: 0.24226775765419006\n",
      "Max gradient at step 7: 0.0919899120926857\n",
      "Max gradient at step 8: 0.2689931094646454\n",
      "Max gradient at step 9: 0.14989551901817322\n",
      "Max gradient at step 10: 0.16317348182201385\n",
      "Max gradient at step 11: 0.21268439292907715\n",
      "Max gradient at step 12: 0.1523427516222\n",
      "Max gradient at step 13: 0.23184214532375336\n",
      "Max gradient at step 14: 0.2183523178100586\n",
      "Max gradient at step 15: 0.19560478627681732\n",
      "Max gradient at step 16: 0.26252681016921997\n",
      "Max gradient at step 17: 0.17132321000099182\n",
      "Max gradient at step 18: 0.23519274592399597\n",
      "Max gradient at step 19: 0.11964205652475357\n",
      "Max gradient at step 20: 0.09627316892147064\n",
      "Max gradient at step 21: 0.19910629093647003\n",
      "Max gradient at step 22: 0.1758681833744049\n",
      "Max gradient at step 23: 0.10810592025518417\n",
      "Max gradient at step 24: 0.14863738417625427\n",
      "Max gradient at step 25: 0.1959119737148285\n",
      "Max gradient at step 26: 0.09756793081760406\n",
      "Max gradient at step 27: 0.1271153688430786\n",
      "Max gradient at step 28: 0.2580973207950592\n",
      "Max gradient at step 29: 0.2295481562614441\n",
      "Max gradient at step 30: 0.2696569561958313\n",
      "Max gradient at step 31: 0.1806042194366455\n",
      "Max gradient at step 32: 0.15256299078464508\n",
      "Max gradient at step 33: 0.1620551347732544\n",
      "Max gradient at step 34: 0.19995738565921783\n",
      "Max gradient at step 35: 0.16064149141311646\n",
      "Max gradient at step 36: 0.21674536168575287\n",
      "Max gradient at step 37: 0.08264052122831345\n",
      "Max gradient at step 38: 0.21745170652866364\n",
      "Max gradient at step 39: 0.22189711034297943\n",
      "Max gradient at step 40: 0.19504190981388092\n",
      "Max gradient at step 41: 0.15286897122859955\n",
      "Max gradient at step 42: 0.16624601185321808\n",
      "Max gradient at step 43: 0.16744883358478546\n",
      "Max gradient at step 44: 0.22035689651966095\n",
      "Max gradient at step 45: 0.20161747932434082\n",
      "Max gradient at step 46: 0.08102428168058395\n",
      "Max gradient at step 47: 0.19452320039272308\n",
      "Max gradient at step 48: 0.18549847602844238\n",
      "Max gradient at step 49: 0.13865435123443604\n",
      "Max gradient at step 50: 0.13298846781253815\n",
      "Max gradient at step 51: 0.3081912100315094\n",
      "Max gradient at step 52: 0.24284790456295013\n",
      "Max gradient at step 53: 0.2023230791091919\n",
      "Max gradient at step 54: 0.20714367926120758\n",
      "Max gradient at step 55: 0.2201511561870575\n",
      "Max gradient at step 56: 0.2048017680644989\n",
      "Max gradient at step 57: 0.08998547494411469\n",
      "Max gradient at step 58: 0.33432361483573914\n",
      "Max gradient at step 59: 0.22232171893119812\n",
      "Max gradient at step 60: 0.12200862914323807\n",
      "Max gradient at step 61: 0.1943673938512802\n",
      "Max gradient at step 62: 0.1599961370229721\n",
      "Max gradient at step 63: 0.20317165553569794\n",
      "Max gradient at step 64: 0.14584487676620483\n",
      "Max gradient at step 65: 0.13262644410133362\n",
      "Max gradient at step 66: 0.13093514740467072\n",
      "Max gradient at step 67: 0.22410523891448975\n",
      "Max gradient at step 68: 0.13025982677936554\n",
      "Max gradient at step 69: 0.1383044719696045\n",
      "Max gradient at step 70: 0.13283222913742065\n",
      "Max gradient at step 71: 0.11938014626502991\n",
      "Max gradient at step 72: 0.1332988142967224\n",
      "Max gradient at step 73: 0.14096121490001678\n",
      "Max gradient at step 74: 0.2285400927066803\n",
      "Max gradient at step 75: 0.1708996742963791\n",
      "Max gradient at step 76: 0.19535639882087708\n",
      "Max gradient at step 77: 0.1239955946803093\n",
      "Max gradient at step 78: 0.1522696167230606\n",
      "Max gradient at step 79: 0.1766750067472458\n",
      "Max gradient at step 80: 0.1386311650276184\n",
      "Max gradient at step 81: 0.11938992887735367\n",
      "Max gradient at step 82: 0.11640369892120361\n",
      "Max gradient at step 83: 0.19532641768455505\n",
      "Max gradient at step 84: 0.11259594559669495\n",
      "Max gradient at step 85: 0.14292553067207336\n",
      "Max gradient at step 86: 0.24431359767913818\n",
      "Max gradient at step 87: 0.14928235113620758\n",
      "Max gradient at step 88: 0.2271634191274643\n",
      "Max gradient at step 89: 0.20474861562252045\n",
      "Max gradient at step 90: 0.20616385340690613\n",
      "Max gradient at step 91: 0.26633650064468384\n",
      "Max gradient at step 92: 0.1641600877046585\n",
      "Max gradient at step 93: 0.1321944296360016\n",
      "Max gradient at step 94: 0.0703950747847557\n",
      "Max gradient at step 95: 0.12197674065828323\n",
      "Max gradient at step 96: 0.20087851583957672\n",
      "Max gradient at step 97: 0.15702210366725922\n",
      "Max gradient at step 98: 0.10361582040786743\n",
      "Max gradient at step 99: 0.25698795914649963\n",
      "Max gradient at step 100: 0.188246950507164\n",
      "Training loss (for one batch) at step 100: 0.4642\n",
      "Max gradient at step 101: 0.17166337370872498\n",
      "Max gradient at step 102: 0.10881318151950836\n",
      "Max gradient at step 103: 0.2590637803077698\n",
      "Max gradient at step 104: 0.171712264418602\n",
      "Max gradient at step 105: 0.05589795857667923\n",
      "Max gradient at step 106: 0.06536401063203812\n",
      "Max gradient at step 107: 0.2077416330575943\n",
      "Max gradient at step 108: 0.11241661757230759\n",
      "Max gradient at step 109: 0.16443410515785217\n",
      "Max gradient at step 110: 0.11427680402994156\n",
      "Max gradient at step 111: 0.21005898714065552\n",
      "Max gradient at step 112: 0.16563630104064941\n",
      "Max gradient at step 113: 0.1929091215133667\n",
      "Max gradient at step 114: 0.12329015880823135\n",
      "Max gradient at step 115: 0.14026494324207306\n",
      "Max gradient at step 116: 0.22773557901382446\n",
      "Max gradient at step 117: 0.2095426768064499\n",
      "Max gradient at step 118: 0.07198642939329147\n",
      "Max gradient at step 119: 0.12100672721862793\n",
      "Max gradient at step 120: 0.10459639877080917\n",
      "Max gradient at step 121: 0.23773248493671417\n",
      "Max gradient at step 122: 0.13181355595588684\n",
      "Max gradient at step 123: 0.1469379961490631\n",
      "Max gradient at step 124: 0.16490615904331207\n",
      "Max gradient at step 125: 0.2552574872970581\n",
      "Max gradient at step 126: 0.08584368973970413\n",
      "Max gradient at step 127: 0.06532523036003113\n",
      "Max gradient at step 128: 0.0812758058309555\n",
      "Max gradient at step 129: 0.15965940058231354\n",
      "Max gradient at step 130: 0.21356211602687836\n",
      "Max gradient at step 131: 0.25017431378364563\n",
      "Max gradient at step 132: 0.03706610947847366\n",
      "Max gradient at step 133: 0.3413951098918915\n",
      "Max gradient at step 134: 0.1933535486459732\n",
      "Max gradient at step 135: 0.2001197338104248\n",
      "Max gradient at step 136: 0.1257549375295639\n",
      "Max gradient at step 137: 0.19645129144191742\n",
      "Max gradient at step 138: 0.20329265296459198\n",
      "Max gradient at step 139: 0.26995593309402466\n",
      "Max gradient at step 140: 0.19416047632694244\n",
      "Max gradient at step 141: 0.030819430947303772\n",
      "Max gradient at step 142: 0.15649697184562683\n",
      "Max gradient at step 143: 0.1159290298819542\n",
      "Max gradient at step 144: 0.12186574935913086\n",
      "Max gradient at step 145: 0.152021124958992\n",
      "Max gradient at step 146: 0.1969730406999588\n",
      "Max gradient at step 147: 0.22643029689788818\n",
      "Max gradient at step 148: 0.095830038189888\n",
      "Max gradient at step 149: 0.15026025474071503\n",
      "Max gradient at step 150: 0.21572880446910858\n",
      "Max gradient at step 151: 0.12647272646427155\n",
      "Max gradient at step 152: 0.13519275188446045\n",
      "Max gradient at step 153: 0.14670056104660034\n",
      "Max gradient at step 154: 0.2800043821334839\n",
      "Max gradient at step 155: 0.1139461100101471\n",
      "Max gradient at step 156: 0.16123537719249725\n",
      "Max gradient at step 157: 0.06654373556375504\n",
      "Max gradient at step 158: 0.15433135628700256\n",
      "Max gradient at step 159: 0.24652868509292603\n",
      "Max gradient at step 160: 0.177579864859581\n",
      "Max gradient at step 161: 0.1735658198595047\n",
      "Max gradient at step 162: 0.17438244819641113\n",
      "Max gradient at step 163: 0.09240677207708359\n",
      "Max gradient at step 164: 0.19428890943527222\n",
      "Max gradient at step 165: 0.12856562435626984\n",
      "Max gradient at step 166: 0.02491665817797184\n",
      "Max gradient at step 167: 0.2025824934244156\n",
      "Max gradient at step 168: 0.22248180210590363\n",
      "Max gradient at step 169: 0.1638036072254181\n",
      "Max gradient at step 170: 0.14647851884365082\n",
      "Max gradient at step 171: 0.09425988048315048\n",
      "Max gradient at step 172: 0.052661407738924026\n",
      "Max gradient at step 173: 0.198179230093956\n",
      "Max gradient at step 174: 0.15894389152526855\n",
      "Max gradient at step 175: 0.17585448920726776\n",
      "Max gradient at step 176: 0.13025392591953278\n",
      "Max gradient at step 177: 0.24592219293117523\n",
      "Max gradient at step 178: 0.1726546585559845\n",
      "Max gradient at step 179: 0.141572043299675\n",
      "Max gradient at step 180: 0.12950070202350616\n",
      "Max gradient at step 181: 0.1132374033331871\n",
      "Max gradient at step 182: 0.09234708547592163\n",
      "Max gradient at step 183: 0.07329905033111572\n",
      "Max gradient at step 184: 0.06525170058012009\n",
      "Max gradient at step 185: 0.11404536664485931\n",
      "Max gradient at step 186: 0.16984207928180695\n",
      "Max gradient at step 187: 0.20124250650405884\n",
      "Max gradient at step 188: 0.20497597754001617\n",
      "Max gradient at step 189: 0.12120364606380463\n",
      "Max gradient at step 190: 0.13207213580608368\n",
      "Max gradient at step 191: 0.11317449063062668\n",
      "Max gradient at step 192: 0.11784970760345459\n",
      "Max gradient at step 193: 0.23955385386943817\n",
      "Max gradient at step 194: 0.17095370590686798\n",
      "Max gradient at step 195: 0.1862754225730896\n",
      "Max gradient at step 196: 0.16834203898906708\n",
      "Max gradient at step 197: 0.17495857179164886\n",
      "Max gradient at step 198: 0.19986337423324585\n",
      "Max gradient at step 199: 0.1265915483236313\n",
      "Max gradient at step 200: 0.19735875725746155\n",
      "Training loss (for one batch) at step 200: 0.5428\n",
      "Max gradient at step 201: 0.17478390038013458\n",
      "Max gradient at step 202: 0.24566222727298737\n",
      "Max gradient at step 203: 0.17165227234363556\n",
      "Max gradient at step 204: 0.09180671721696854\n",
      "Max gradient at step 205: 0.15756875276565552\n",
      "Max gradient at step 206: 0.14548839628696442\n",
      "Max gradient at step 207: 0.23230938613414764\n",
      "Max gradient at step 208: 0.19859246909618378\n",
      "Max gradient at step 209: 0.13381941616535187\n",
      "Max gradient at step 210: 0.08226744830608368\n",
      "Max gradient at step 211: 0.08755908906459808\n",
      "Max gradient at step 212: 0.22672918438911438\n",
      "Max gradient at step 213: 0.2967066466808319\n",
      "Max gradient at step 214: 0.10160860419273376\n",
      "Max gradient at step 215: 0.22607660293579102\n",
      "Max gradient at step 216: 0.13640855252742767\n",
      "Max gradient at step 217: 0.17975115776062012\n",
      "Max gradient at step 218: 0.2211628258228302\n",
      "Max gradient at step 219: 0.10416145622730255\n",
      "Max gradient at step 220: 0.19385772943496704\n",
      "Max gradient at step 221: 0.21545663475990295\n",
      "Max gradient at step 222: 0.23020246624946594\n",
      "Max gradient at step 223: 0.18559662997722626\n",
      "Max gradient at step 224: 0.11703626811504364\n",
      "Max gradient at step 225: 0.16948765516281128\n",
      "Max gradient at step 226: 0.17318475246429443\n",
      "Max gradient at step 227: 0.2022799253463745\n",
      "Max gradient at step 228: 0.2648765444755554\n",
      "Max gradient at step 229: 0.24365854263305664\n",
      "Max gradient at step 230: 0.1856287717819214\n",
      "Max gradient at step 231: 0.190608948469162\n",
      "Max gradient at step 232: 0.16393795609474182\n",
      "Max gradient at step 233: 0.14431647956371307\n",
      "Max gradient at step 234: 0.21466673910617828\n",
      "Max gradient at step 235: 0.18032807111740112\n",
      "Max gradient at step 236: 0.1339145004749298\n",
      "Max gradient at step 237: 0.14458656311035156\n",
      "Max gradient at step 238: 0.25339052081108093\n",
      "Max gradient at step 239: 0.21947064995765686\n",
      "Max gradient at step 240: 0.2405693084001541\n",
      "Max gradient at step 241: 0.1045222356915474\n",
      "Max gradient at step 242: 0.1652042120695114\n",
      "Max gradient at step 243: 0.21656450629234314\n",
      "Max gradient at step 244: 0.2627567946910858\n",
      "Max gradient at step 245: 0.17615388333797455\n",
      "Max gradient at step 246: 0.051965270191431046\n",
      "Max gradient at step 247: 0.20485980808734894\n",
      "Max gradient at step 248: 0.1422283798456192\n",
      "Max gradient at step 249: 0.19812409579753876\n",
      "Max gradient at step 250: 0.24757231771945953\n",
      "Max gradient at step 251: 0.151828333735466\n",
      "Max gradient at step 252: 0.19289281964302063\n",
      "Max gradient at step 253: 0.12256449460983276\n",
      "Max gradient at step 254: 0.3145901560783386\n",
      "Max gradient at step 255: 0.16452717781066895\n",
      "Max gradient at step 256: 0.29874444007873535\n",
      "Max gradient at step 257: 0.214352548122406\n",
      "Max gradient at step 258: 0.27425140142440796\n",
      "Max gradient at step 259: 0.140023335814476\n",
      "Max gradient at step 260: 0.29344499111175537\n",
      "Max gradient at step 261: 0.07479842007160187\n",
      "Max gradient at step 262: 0.164350226521492\n",
      "Max gradient at step 263: 0.12901802361011505\n",
      "Max gradient at step 264: 0.15860840678215027\n",
      "Max gradient at step 265: 0.1332692950963974\n",
      "Max gradient at step 266: 0.22729438543319702\n",
      "Max gradient at step 267: 0.1749878227710724\n",
      "Max gradient at step 268: 0.18891794979572296\n",
      "Max gradient at step 269: 0.1588212549686432\n",
      "Max gradient at step 270: 0.21260635554790497\n",
      "Max gradient at step 271: 0.2410479187965393\n",
      "Max gradient at step 272: 0.13850398361682892\n",
      "Max gradient at step 273: 0.21702809631824493\n",
      "Max gradient at step 274: 0.11192316561937332\n",
      "Max gradient at step 275: 0.2046705037355423\n",
      "Max gradient at step 276: 0.1846596747636795\n",
      "Max gradient at step 277: 0.17481635510921478\n",
      "Max gradient at step 278: 0.05644185468554497\n",
      "Max gradient at step 279: 0.1707959771156311\n",
      "Max gradient at step 280: 0.1713397353887558\n",
      "Max gradient at step 281: 0.09627234935760498\n",
      "Max gradient at step 282: 0.1163172721862793\n",
      "Max gradient at step 283: 0.1758648157119751\n",
      "Max gradient at step 284: 0.3919711410999298\n",
      "Max gradient at step 285: 0.12867264449596405\n",
      "Max gradient at step 286: 0.28568369150161743\n",
      "Max gradient at step 287: 0.28999099135398865\n",
      "Max gradient at step 288: 0.14752426743507385\n",
      "Max gradient at step 289: 0.17291241884231567\n",
      "Max gradient at step 290: 0.17724496126174927\n",
      "Max gradient at step 291: 0.2363194078207016\n",
      "Max gradient at step 292: 0.13207608461380005\n",
      "Max gradient at step 293: 0.16484032571315765\n",
      "Max gradient at step 294: 0.1346764862537384\n",
      "Max gradient at step 295: 0.16817696392536163\n",
      "Max gradient at step 296: 0.13912105560302734\n",
      "Max gradient at step 297: 0.13342085480690002\n",
      "Max gradient at step 298: 0.13373525440692902\n",
      "Max gradient at step 299: 0.45493608713150024\n",
      "Max gradient at step 300: 0.13022659718990326\n",
      "Training loss (for one batch) at step 300: 0.0810\n",
      "Max gradient at step 301: 0.18934589624404907\n",
      "Max gradient at step 302: 0.094894640147686\n",
      "Max gradient at step 303: 0.22529126703739166\n",
      "Max gradient at step 304: 0.21922332048416138\n",
      "Max gradient at step 305: 0.21827130019664764\n",
      "Max gradient at step 306: 0.3308224678039551\n",
      "Max gradient at step 307: 0.09461691230535507\n",
      "Max gradient at step 308: 0.17255021631717682\n",
      "Max gradient at step 309: 0.2243611067533493\n",
      "Max gradient at step 310: 0.1518758088350296\n",
      "Max gradient at step 311: 0.24399219453334808\n",
      "Max gradient at step 312: 0.0891108587384224\n",
      "Max gradient at step 313: 0.25111648440361023\n",
      "Max gradient at step 314: 0.26484766602516174\n",
      "Max gradient at step 315: 0.17796261608600616\n",
      "Max gradient at step 316: 0.17082475125789642\n",
      "Max gradient at step 317: 0.22624661028385162\n",
      "Max gradient at step 318: 0.14475049078464508\n",
      "Max gradient at step 319: 0.19446809589862823\n",
      "Max gradient at step 320: 0.06491595506668091\n",
      "Max gradient at step 321: 0.13018065690994263\n",
      "Max gradient at step 322: 0.29919371008872986\n",
      "Max gradient at step 323: 0.3548581600189209\n",
      "Max gradient at step 324: 0.16010813415050507\n",
      "Max gradient at step 325: 0.19376137852668762\n",
      "Max gradient at step 326: 0.19101691246032715\n",
      "Max gradient at step 327: 0.14345403015613556\n",
      "Max gradient at step 328: 0.18368837237358093\n",
      "Max gradient at step 329: 0.250480592250824\n",
      "Max gradient at step 330: 0.045476317405700684\n",
      "Max gradient at step 331: 0.17360743880271912\n",
      "Max gradient at step 332: 0.24380184710025787\n",
      "Max gradient at step 333: 0.1333010047674179\n",
      "Max gradient at step 334: 0.28603529930114746\n",
      "Max gradient at step 335: 0.15607044100761414\n",
      "Max gradient at step 336: 0.13495004177093506\n",
      "Max gradient at step 337: 0.07617031037807465\n",
      "Max gradient at step 338: 0.17182588577270508\n",
      "Max gradient at step 339: 0.1304832398891449\n",
      "Max gradient at step 340: 0.24466736614704132\n",
      "Max gradient at step 341: 0.13313190639019012\n",
      "Max gradient at step 342: 0.14993344247341156\n",
      "Max gradient at step 343: 0.1567910760641098\n",
      "Max gradient at step 344: 0.12649492919445038\n",
      "Max gradient at step 345: 0.22431610524654388\n",
      "Max gradient at step 346: 0.1715194433927536\n",
      "Max gradient at step 347: 0.16115888953208923\n",
      "Max gradient at step 348: 0.1991044282913208\n",
      "Max gradient at step 349: 0.27708590030670166\n",
      "Max gradient at step 350: 0.2002934217453003\n",
      "Max gradient at step 351: 0.25856515765190125\n",
      "Max gradient at step 352: 0.14753393828868866\n",
      "Max gradient at step 353: 0.13291732966899872\n",
      "Max gradient at step 354: 0.05707269161939621\n",
      "Max gradient at step 355: 0.17480634152889252\n",
      "Max gradient at step 356: 0.1354091316461563\n",
      "Max gradient at step 357: 0.12355180829763412\n",
      "Max gradient at step 358: 0.11581149697303772\n",
      "Max gradient at step 359: 0.27020201086997986\n",
      "Max gradient at step 360: 0.26196834444999695\n",
      "Max gradient at step 361: 0.20775733888149261\n",
      "Max gradient at step 362: 0.11752715706825256\n",
      "Max gradient at step 363: 0.1904408484697342\n",
      "Max gradient at step 364: 0.20962633192539215\n",
      "Max gradient at step 365: 0.2327136993408203\n",
      "Max gradient at step 366: 0.16089993715286255\n",
      "Max gradient at step 367: 0.2220410257577896\n",
      "Max gradient at step 368: 0.10214842110872269\n",
      "Max gradient at step 369: 0.19588862359523773\n",
      "Max gradient at step 370: 0.17351725697517395\n",
      "Max gradient at step 371: 0.13751725852489471\n",
      "Max gradient at step 372: 0.10854002833366394\n",
      "Max gradient at step 373: 0.37022122740745544\n",
      "Max gradient at step 374: 0.19932322204113007\n",
      "Max gradient at step 375: 0.21333101391792297\n",
      "Max gradient at step 376: 0.16831336915493011\n",
      "Max gradient at step 377: 0.08929368853569031\n",
      "Max gradient at step 378: 0.23828549683094025\n",
      "Max gradient at step 379: 0.18259869515895844\n",
      "Max gradient at step 380: 0.044687990099191666\n",
      "Max gradient at step 381: 0.16375333070755005\n",
      "Max gradient at step 382: 0.2241537868976593\n",
      "Max gradient at step 383: 0.18143001198768616\n",
      "Max gradient at step 384: 0.19915415346622467\n",
      "Max gradient at step 385: 0.1847219616174698\n",
      "Max gradient at step 386: 0.18555815517902374\n",
      "Max gradient at step 387: 0.1947474479675293\n",
      "Max gradient at step 388: 0.1833573281764984\n",
      "Max gradient at step 389: 0.16142086684703827\n",
      "Max gradient at step 390: 0.15169654786586761\n",
      "Max gradient at step 391: 0.37645530700683594\n",
      "Max gradient at step 392: 0.2485969215631485\n",
      "Max gradient at step 393: 0.16691900789737701\n",
      "Max gradient at step 394: 0.30799126625061035\n",
      "Max gradient at step 395: 0.19456148147583008\n",
      "Max gradient at step 396: 0.29622915387153625\n",
      "Max gradient at step 397: 0.1697438657283783\n",
      "Max gradient at step 398: 0.46925586462020874\n",
      "Max gradient at step 399: 0.3105304539203644\n",
      "Max gradient at step 400: 0.20273302495479584\n",
      "Training loss (for one batch) at step 400: 0.3748\n",
      "Max gradient at step 401: 0.2271360605955124\n",
      "Max gradient at step 402: 0.16798853874206543\n",
      "Max gradient at step 403: 0.13348130881786346\n",
      "Max gradient at step 404: 0.23130211234092712\n",
      "Max gradient at step 405: 0.1398583948612213\n",
      "Max gradient at step 406: 0.17713621258735657\n",
      "Max gradient at step 407: 0.13622558116912842\n",
      "Max gradient at step 408: 0.2321096658706665\n",
      "Max gradient at step 409: 0.1321670114994049\n",
      "Max gradient at step 410: 0.08312726020812988\n",
      "Max gradient at step 411: 0.27018776535987854\n",
      "Max gradient at step 412: 0.24807661771774292\n",
      "Max gradient at step 413: 0.1840600073337555\n",
      "Max gradient at step 414: 0.17922383546829224\n",
      "Max gradient at step 415: 0.2355441153049469\n",
      "Max gradient at step 416: 0.20852793753147125\n",
      "Max gradient at step 417: 0.20735692977905273\n",
      "Max gradient at step 418: 0.21836505830287933\n",
      "Max gradient at step 419: 0.2819861173629761\n",
      "Max gradient at step 420: 0.1483079344034195\n",
      "Max gradient at step 421: 0.16220681369304657\n",
      "Max gradient at step 422: 0.19896581768989563\n",
      "Max gradient at step 423: 0.22849330306053162\n",
      "Max gradient at step 424: 0.11506578326225281\n",
      "Max gradient at step 425: 0.17935219407081604\n",
      "Max gradient at step 426: 0.05861491337418556\n",
      "Max gradient at step 427: 0.16694703698158264\n",
      "Max gradient at step 428: 0.1863057166337967\n",
      "Max gradient at step 429: 0.17339390516281128\n",
      "Max gradient at step 430: 0.09824441373348236\n",
      "Max gradient at step 431: 0.19240394234657288\n",
      "Max gradient at step 432: 0.20628972351551056\n",
      "Max gradient at step 433: 0.14900285005569458\n",
      "Max gradient at step 434: 0.17678026854991913\n",
      "Max gradient at step 435: 0.14853520691394806\n",
      "Max gradient at step 436: 0.15625280141830444\n",
      "Max gradient at step 437: 0.20680631697177887\n",
      "Max gradient at step 438: 0.19999340176582336\n",
      "Max gradient at step 439: 0.14890368282794952\n",
      "Max gradient at step 440: 0.2015680968761444\n",
      "Max gradient at step 441: 0.25522470474243164\n",
      "Max gradient at step 442: 0.20373892784118652\n",
      "Max gradient at step 443: 0.26206013560295105\n",
      "Max gradient at step 444: 0.17737513780593872\n",
      "Max gradient at step 445: 0.1394275575876236\n",
      "Max gradient at step 446: 0.19043932855129242\n",
      "Max gradient at step 447: 0.17225846648216248\n",
      "Max gradient at step 448: 0.1792992651462555\n",
      "Max gradient at step 449: 0.18785704672336578\n",
      "Max gradient at step 450: 0.1052209883928299\n",
      "Max gradient at step 451: 0.15687072277069092\n",
      "Max gradient at step 452: 0.33642029762268066\n",
      "Max gradient at step 453: 0.21185137331485748\n",
      "Max gradient at step 454: 0.20974507927894592\n",
      "Max gradient at step 455: 0.15165556967258453\n",
      "Max gradient at step 456: 0.29731035232543945\n",
      "Max gradient at step 457: 0.28912168741226196\n",
      "Max gradient at step 458: 0.16380208730697632\n",
      "Max gradient at step 459: 0.1693418174982071\n",
      "Max gradient at step 460: 0.13067419826984406\n",
      "Max gradient at step 461: 0.26570022106170654\n",
      "Max gradient at step 462: 0.08999320864677429\n",
      "Max gradient at step 463: 0.18502669036388397\n",
      "Max gradient at step 464: 0.19255946576595306\n",
      "Max gradient at step 465: 0.15280751883983612\n",
      "Max gradient at step 466: 0.13117876648902893\n",
      "Max gradient at step 467: 0.17023085057735443\n",
      "Max gradient at step 468: 0.17033779621124268\n",
      "Max gradient at step 469: 0.16520392894744873\n",
      "Max gradient at step 470: 0.1310848444700241\n",
      "Max gradient at step 471: 0.22212499380111694\n",
      "Max gradient at step 472: 0.12178736925125122\n",
      "Max gradient at step 473: 0.36618858575820923\n",
      "Max gradient at step 474: 0.15463712811470032\n",
      "Max gradient at step 475: 0.35993343591690063\n",
      "Max gradient at step 476: 0.13578927516937256\n",
      "Max gradient at step 477: 0.10665895789861679\n",
      "Max gradient at step 478: 0.19389039278030396\n",
      "Max gradient at step 479: 0.1710405796766281\n",
      "Max gradient at step 480: 0.1693655252456665\n",
      "Max gradient at step 481: 0.15056803822517395\n",
      "Max gradient at step 482: 0.1363837569952011\n",
      "Max gradient at step 483: 0.18453368544578552\n",
      "Max gradient at step 484: 0.32998257875442505\n",
      "Max gradient at step 485: 0.2238956242799759\n",
      "Max gradient at step 486: 0.3895859122276306\n",
      "Max gradient at step 487: 0.19070938229560852\n",
      "Max gradient at step 488: 0.16394959390163422\n",
      "Max gradient at step 489: 0.1296563297510147\n",
      "Max gradient at step 490: 0.1359327733516693\n",
      "Max gradient at step 491: 0.18572083115577698\n",
      "Max gradient at step 492: 0.20183299481868744\n",
      "Max gradient at step 493: 0.11295011639595032\n",
      "Max gradient at step 494: 0.15432965755462646\n",
      "Max gradient at step 495: 0.18050581216812134\n",
      "Max gradient at step 496: 0.13649781048297882\n",
      "Max gradient at step 497: 0.13479913771152496\n",
      "Max gradient at step 498: 0.17623497545719147\n",
      "Max gradient at step 499: 0.15696381032466888\n",
      "Max gradient at step 500: 0.24172310531139374\n",
      "Training loss (for one batch) at step 500: 0.2758\n",
      "Max gradient at step 501: 0.17081280052661896\n",
      "Max gradient at step 502: 0.16432230174541473\n",
      "Max gradient at step 503: 0.23712582886219025\n",
      "Max gradient at step 504: 0.17767015099525452\n",
      "Max gradient at step 505: 0.2096691131591797\n",
      "Max gradient at step 506: 0.19210141897201538\n",
      "Max gradient at step 507: 0.18176902830600739\n",
      "Max gradient at step 508: 0.19596663117408752\n",
      "Max gradient at step 509: 0.14058105647563934\n",
      "Max gradient at step 510: 0.1337835043668747\n",
      "Max gradient at step 511: 0.12072187662124634\n",
      "Max gradient at step 512: 0.14471296966075897\n",
      "Max gradient at step 513: 0.10630624741315842\n",
      "Max gradient at step 514: 0.12055744230747223\n",
      "Max gradient at step 515: 0.13472014665603638\n",
      "Max gradient at step 516: 0.17025116086006165\n",
      "Max gradient at step 517: 0.16731539368629456\n",
      "Max gradient at step 518: 0.21231621503829956\n",
      "Max gradient at step 519: 0.11434466391801834\n",
      "Max gradient at step 520: 0.06766509264707565\n",
      "Max gradient at step 521: 0.2674213647842407\n",
      "Max gradient at step 522: 0.12628227472305298\n",
      "Max gradient at step 523: 0.19075100123882294\n",
      "Max gradient at step 524: 0.21278007328510284\n",
      "Max gradient at step 525: 0.0813131108880043\n",
      "Max gradient at step 526: 0.17880161106586456\n",
      "Max gradient at step 527: 0.2018909752368927\n",
      "Max gradient at step 528: 0.15796399116516113\n",
      "Max gradient at step 529: 0.15420746803283691\n",
      "Max gradient at step 530: 0.3924984931945801\n",
      "Max gradient at step 531: 0.2243742197751999\n",
      "Max gradient at step 532: 0.1157870963215828\n",
      "Max gradient at step 533: 0.20673835277557373\n",
      "Max gradient at step 534: 0.2852683961391449\n",
      "Max gradient at step 535: 0.19703824818134308\n",
      "Max gradient at step 536: 0.1814487874507904\n",
      "Max gradient at step 537: 0.15871211886405945\n",
      "Max gradient at step 538: 0.08095470815896988\n",
      "Max gradient at step 539: 0.1618873029947281\n",
      "Max gradient at step 540: 0.22746247053146362\n",
      "Max gradient at step 541: 0.0786479264497757\n",
      "Max gradient at step 542: 0.20479784905910492\n",
      "Max gradient at step 543: 0.19750386476516724\n",
      "Max gradient at step 544: 0.20114189386367798\n",
      "Max gradient at step 545: 0.16605070233345032\n",
      "Max gradient at step 546: 0.1993308961391449\n",
      "Max gradient at step 547: 0.38145843148231506\n",
      "Max gradient at step 548: 0.06559980660676956\n",
      "Max gradient at step 549: 0.14476248621940613\n",
      "Max gradient at step 550: 0.33883172273635864\n",
      "Max gradient at step 551: 0.16435043513774872\n",
      "Max gradient at step 552: 0.13513435423374176\n",
      "Max gradient at step 553: 0.2685166895389557\n",
      "Max gradient at step 554: 0.11117705702781677\n",
      "Max gradient at step 555: 0.21791911125183105\n",
      "Max gradient at step 556: 0.13933923840522766\n",
      "Max gradient at step 557: 0.13211849331855774\n",
      "Max gradient at step 558: 0.158121258020401\n",
      "Max gradient at step 559: 0.23282787203788757\n",
      "Max gradient at step 560: 0.1376153975725174\n",
      "Max gradient at step 561: 0.100238136947155\n",
      "Max gradient at step 562: 0.13268370926380157\n",
      "Max gradient at step 563: 0.15365247428417206\n",
      "Max gradient at step 564: 0.13925643265247345\n",
      "Max gradient at step 565: 0.14884938299655914\n",
      "Max gradient at step 566: 0.04475787281990051\n",
      "Max gradient at step 567: 0.1762113869190216\n",
      "Max gradient at step 568: 0.13427239656448364\n",
      "Max gradient at step 569: 0.10657548904418945\n",
      "Max gradient at step 570: 0.22302505373954773\n",
      "Max gradient at step 571: 0.225175142288208\n",
      "Max gradient at step 572: 0.11401116847991943\n",
      "Max gradient at step 573: 0.10709843039512634\n",
      "Max gradient at step 574: 0.24419814348220825\n",
      "Max gradient at step 575: 0.2420186847448349\n",
      "Max gradient at step 576: 0.17609244585037231\n",
      "Max gradient at step 577: 0.1320863515138626\n",
      "Max gradient at step 578: 0.27786099910736084\n",
      "Max gradient at step 579: 0.09515777230262756\n",
      "Max gradient at step 580: 0.18588252365589142\n",
      "Max gradient at step 581: 0.200413778424263\n",
      "Max gradient at step 582: 0.2221653163433075\n",
      "Max gradient at step 583: 0.20094408094882965\n",
      "Max gradient at step 584: 0.15166494250297546\n",
      "Max gradient at step 585: 0.12958551943302155\n",
      "Max gradient at step 586: 0.14147955179214478\n",
      "Max gradient at step 587: 0.19660817086696625\n",
      "Max gradient at step 588: 0.0776953175663948\n",
      "Max gradient at step 589: 0.23424667119979858\n",
      "Max gradient at step 590: 0.16073854267597198\n",
      "Max gradient at step 591: 0.17339208722114563\n",
      "Max gradient at step 592: 0.1324164718389511\n",
      "Max gradient at step 593: 0.18519504368305206\n",
      "Max gradient at step 594: 0.18528568744659424\n",
      "Max gradient at step 595: 0.16278129816055298\n",
      "Max gradient at step 596: 0.17236077785491943\n",
      "Max gradient at step 597: 0.10949420928955078\n",
      "Max gradient at step 598: 0.2946501672267914\n",
      "Max gradient at step 599: 0.21346333622932434\n",
      "Max gradient at step 600: 0.15182572603225708\n",
      "Training loss (for one batch) at step 600: 0.1817\n",
      "Max gradient at step 601: 0.15821866691112518\n",
      "Max gradient at step 602: 0.20836809277534485\n",
      "Max gradient at step 603: 0.17380958795547485\n",
      "Max gradient at step 604: 0.23543639481067657\n",
      "Max gradient at step 605: 0.10457497835159302\n",
      "Max gradient at step 606: 0.25418636202812195\n",
      "Max gradient at step 607: 0.23565773665905\n",
      "Max gradient at step 608: 0.2270222306251526\n",
      "Max gradient at step 609: 0.07175220549106598\n",
      "Max gradient at step 610: 0.14992690086364746\n",
      "Max gradient at step 611: 0.11714771389961243\n",
      "Max gradient at step 612: 0.16678503155708313\n",
      "Max gradient at step 613: 0.12317510694265366\n",
      "Max gradient at step 614: 0.21352273225784302\n",
      "Max gradient at step 615: 0.14290182292461395\n",
      "Max gradient at step 616: 0.15859617292881012\n",
      "Max gradient at step 617: 0.2802978456020355\n",
      "Max gradient at step 618: 0.20794880390167236\n",
      "Max gradient at step 619: 0.14305371046066284\n",
      "Max gradient at step 620: 0.22635935246944427\n",
      "Max gradient at step 621: 0.13325048983097076\n",
      "Max gradient at step 622: 0.16218294203281403\n",
      "Max gradient at step 623: 0.1628558337688446\n",
      "Max gradient at step 624: 0.14956435561180115\n",
      "Max gradient at step 625: 0.22015924751758575\n",
      "Max gradient at step 626: 0.26484057307243347\n",
      "Max gradient at step 627: 0.15332913398742676\n",
      "Max gradient at step 628: 0.220597043633461\n",
      "Max gradient at step 629: 0.17034664750099182\n",
      "Max gradient at step 630: 0.03437657654285431\n",
      "Max gradient at step 631: 0.1611272692680359\n",
      "Max gradient at step 632: 0.19686073064804077\n",
      "Max gradient at step 633: 0.1284331977367401\n",
      "Max gradient at step 634: 0.21760469675064087\n",
      "Max gradient at step 635: 0.22728309035301208\n",
      "Max gradient at step 636: 0.18976068496704102\n",
      "Max gradient at step 637: 0.1275220662355423\n",
      "Max gradient at step 638: 0.15330328047275543\n",
      "Max gradient at step 639: 0.2011786252260208\n",
      "Max gradient at step 640: 0.13560588657855988\n",
      "Max gradient at step 641: 0.2237812727689743\n",
      "Max gradient at step 642: 0.17797091603279114\n",
      "Max gradient at step 643: 0.20270077884197235\n",
      "Max gradient at step 644: 0.1175135150551796\n",
      "Max gradient at step 645: 0.13826125860214233\n",
      "Max gradient at step 646: 0.08914853632450104\n",
      "Max gradient at step 647: 0.19123636186122894\n",
      "Max gradient at step 648: 0.11860664933919907\n",
      "Max gradient at step 649: 0.13482367992401123\n",
      "Max gradient at step 650: 0.14094476401805878\n",
      "Max gradient at step 651: 0.2608673572540283\n",
      "Max gradient at step 652: 0.18567174673080444\n",
      "Max gradient at step 653: 0.2231501042842865\n",
      "Max gradient at step 654: 0.32404273748397827\n",
      "Max gradient at step 655: 0.18697276711463928\n",
      "Max gradient at step 656: 0.18407104909420013\n",
      "Max gradient at step 657: 0.22029423713684082\n",
      "Max gradient at step 658: 0.211642324924469\n",
      "Max gradient at step 659: 0.06590781360864639\n",
      "Max gradient at step 660: 0.21826232969760895\n",
      "Max gradient at step 661: 0.1699547916650772\n",
      "Max gradient at step 662: 0.1732238233089447\n",
      "Max gradient at step 663: 0.20042458176612854\n",
      "Max gradient at step 664: 0.10939933359622955\n",
      "Max gradient at step 665: 0.3028290271759033\n",
      "Max gradient at step 666: 0.24429383873939514\n",
      "Max gradient at step 667: 0.11491385847330093\n",
      "Max gradient at step 668: 0.14928250014781952\n",
      "Max gradient at step 669: 0.17838644981384277\n",
      "Max gradient at step 670: 0.14470967650413513\n",
      "Max gradient at step 671: 0.14293240010738373\n",
      "Max gradient at step 672: 0.07980046421289444\n",
      "Max gradient at step 673: 0.16051827371120453\n",
      "Max gradient at step 674: 0.214482381939888\n",
      "Max gradient at step 675: 0.18893815577030182\n",
      "Max gradient at step 676: 0.17920146882534027\n",
      "Max gradient at step 677: 0.11372332274913788\n",
      "Max gradient at step 678: 0.1649138331413269\n",
      "Max gradient at step 679: 0.2099693864583969\n",
      "Max gradient at step 680: 0.1743270456790924\n",
      "Max gradient at step 681: 0.12222591787576675\n",
      "Max gradient at step 682: 0.16231268644332886\n",
      "Max gradient at step 683: 0.1969805508852005\n",
      "Max gradient at step 684: 0.15408207476139069\n",
      "Max gradient at step 685: 0.09302586317062378\n",
      "Max gradient at step 686: 0.08840446174144745\n",
      "Max gradient at step 687: 0.15436233580112457\n",
      "Max gradient at step 688: 0.17611148953437805\n",
      "Max gradient at step 689: 0.1681281477212906\n",
      "Max gradient at step 690: 0.12185455113649368\n",
      "Max gradient at step 691: 0.25120851397514343\n",
      "Max gradient at step 692: 0.16157791018486023\n",
      "Max gradient at step 693: 0.20720355212688446\n",
      "Max gradient at step 694: 0.20942160487174988\n",
      "Max gradient at step 695: 0.1621086597442627\n",
      "Max gradient at step 696: 0.11152547597885132\n",
      "Max gradient at step 697: 0.11488700658082962\n",
      "Max gradient at step 698: 0.13962620496749878\n",
      "Max gradient at step 699: 0.2710268497467041\n",
      "Max gradient at step 700: 0.11245059221982956\n",
      "Training loss (for one batch) at step 700: 0.2054\n",
      "Max gradient at step 701: 0.14030081033706665\n",
      "Max gradient at step 702: 0.15308278799057007\n",
      "Max gradient at step 703: 0.1859273910522461\n",
      "Max gradient at step 704: 0.11010030657052994\n",
      "Max gradient at step 705: 0.14367474615573883\n",
      "Max gradient at step 706: 0.11573149263858795\n",
      "Max gradient at step 707: 0.12576670944690704\n",
      "Max gradient at step 708: 0.22073523700237274\n",
      "Max gradient at step 709: 0.14775747060775757\n",
      "Max gradient at step 710: 0.2301667034626007\n",
      "Max gradient at step 711: 0.34431135654449463\n",
      "Max gradient at step 712: 0.12001749128103256\n",
      "Max gradient at step 713: 0.13101114332675934\n",
      "Max gradient at step 714: 0.26017701625823975\n",
      "Max gradient at step 715: 0.1658659428358078\n",
      "Max gradient at step 716: 0.1236988976597786\n",
      "Max gradient at step 717: 0.15364551544189453\n",
      "Max gradient at step 718: 0.1790362298488617\n",
      "Max gradient at step 719: 0.14332816004753113\n",
      "Max gradient at step 720: 0.16413140296936035\n",
      "Max gradient at step 721: 0.13662205636501312\n",
      "Max gradient at step 722: 0.21334214508533478\n",
      "Max gradient at step 723: 0.09234274178743362\n",
      "Max gradient at step 724: 0.13620445132255554\n",
      "Max gradient at step 725: 0.24157820641994476\n",
      "Max gradient at step 726: 0.26745226979255676\n",
      "Max gradient at step 727: 0.2174655795097351\n",
      "Max gradient at step 728: 0.19080635905265808\n",
      "Max gradient at step 729: 0.11027733236551285\n",
      "Max gradient at step 730: 0.13708758354187012\n",
      "Max gradient at step 731: 0.11648394167423248\n",
      "Max gradient at step 732: 0.11765126883983612\n",
      "Max gradient at step 733: 0.24434159696102142\n",
      "Max gradient at step 734: 0.22807562351226807\n",
      "Max gradient at step 735: 0.4664728045463562\n",
      "Max gradient at step 736: 0.28275495767593384\n",
      "Max gradient at step 737: 0.12767264246940613\n",
      "Max gradient at step 738: 0.32360658049583435\n",
      "Max gradient at step 739: 0.3558995723724365\n",
      "Max gradient at step 740: 0.17539386451244354\n",
      "Max gradient at step 741: 0.18477962911128998\n",
      "Max gradient at step 742: 0.20291009545326233\n",
      "Max gradient at step 743: 0.14283005893230438\n",
      "Max gradient at step 744: 0.261627197265625\n",
      "Max gradient at step 745: 0.3108040988445282\n",
      "Max gradient at step 746: 0.1439792960882187\n",
      "Max gradient at step 747: 0.1125628873705864\n",
      "Max gradient at step 748: 0.2266279011964798\n",
      "Max gradient at step 749: 0.22490039467811584\n",
      "Max gradient at step 750: 0.2627565562725067\n",
      "Max gradient at step 751: 0.0999746322631836\n",
      "Max gradient at step 752: 0.16233199834823608\n",
      "Max gradient at step 753: 0.17296917736530304\n",
      "Max gradient at step 754: 0.09927836060523987\n",
      "Max gradient at step 755: 0.10407372564077377\n",
      "Max gradient at step 756: 0.13822492957115173\n",
      "Max gradient at step 757: 0.21525530517101288\n",
      "Max gradient at step 758: 0.17985883355140686\n",
      "Max gradient at step 759: 0.14124585688114166\n",
      "Max gradient at step 760: 0.1263100802898407\n",
      "Max gradient at step 761: 0.14210256934165955\n",
      "Max gradient at step 762: 0.10288068652153015\n",
      "Max gradient at step 763: 0.19140398502349854\n",
      "Max gradient at step 764: 0.10877249389886856\n",
      "Max gradient at step 765: 0.2666759490966797\n",
      "Max gradient at step 766: 0.1380724161863327\n",
      "Max gradient at step 767: 0.24098214507102966\n",
      "Max gradient at step 768: 0.10624449700117111\n",
      "Max gradient at step 769: 0.10668718814849854\n",
      "Max gradient at step 770: 0.10390639305114746\n",
      "Max gradient at step 771: 0.20374858379364014\n",
      "Max gradient at step 772: 0.26143431663513184\n",
      "Max gradient at step 773: 0.23523077368736267\n",
      "Max gradient at step 774: 0.1014205738902092\n",
      "Max gradient at step 775: 0.07464545965194702\n",
      "Max gradient at step 776: 0.128549724817276\n",
      "Max gradient at step 777: 0.19177423417568207\n",
      "Max gradient at step 778: 0.23582755029201508\n",
      "Max gradient at step 779: 0.14789210259914398\n",
      "Max gradient at step 780: 0.13795951008796692\n",
      "Max gradient at step 781: 0.25797176361083984\n",
      "Max gradient at step 782: 0.1341022104024887\n",
      "Max gradient at step 783: 0.25575488805770874\n",
      "Max gradient at step 784: 0.12227203696966171\n",
      "Max gradient at step 785: 0.12454048544168472\n",
      "Max gradient at step 786: 0.18954536318778992\n",
      "Max gradient at step 787: 0.13421815633773804\n",
      "Max gradient at step 788: 0.11839629709720612\n",
      "Max gradient at step 789: 0.22273984551429749\n",
      "Max gradient at step 790: 0.1610780507326126\n",
      "Max gradient at step 791: 0.09689141809940338\n",
      "Max gradient at step 792: 0.16480529308319092\n",
      "Max gradient at step 793: 0.24867074191570282\n",
      "Max gradient at step 794: 0.16539925336837769\n",
      "Max gradient at step 795: 0.14756061136722565\n",
      "Max gradient at step 796: 0.20045466721057892\n",
      "Max gradient at step 797: 0.2484321892261505\n",
      "Max gradient at step 798: 0.14185351133346558\n",
      "Max gradient at step 799: 0.07550878822803497\n",
      "Max gradient at step 800: 0.2631847858428955\n",
      "Training loss (for one batch) at step 800: 0.4287\n",
      "Max gradient at step 801: 0.16497768461704254\n",
      "Max gradient at step 802: 0.11835207045078278\n",
      "Max gradient at step 803: 0.19580048322677612\n",
      "Max gradient at step 804: 0.182498037815094\n",
      "Max gradient at step 805: 0.3150785565376282\n",
      "Max gradient at step 806: 0.09849844872951508\n",
      "Max gradient at step 807: 0.23078198730945587\n",
      "Max gradient at step 808: 0.2906058430671692\n",
      "Max gradient at step 809: 0.18522784113883972\n",
      "Max gradient at step 810: 0.2771903872489929\n",
      "Max gradient at step 811: 0.1926230937242508\n",
      "Max gradient at step 812: 0.2441243976354599\n",
      "Max gradient at step 813: 0.21752849221229553\n",
      "Max gradient at step 814: 0.13868087530136108\n",
      "Max gradient at step 815: 0.19185484945774078\n",
      "Max gradient at step 816: 0.08673329651355743\n",
      "Max gradient at step 817: 0.155669167637825\n",
      "Max gradient at step 818: 0.15126416087150574\n",
      "Max gradient at step 819: 0.19421516358852386\n",
      "Max gradient at step 820: 0.1327175498008728\n",
      "Max gradient at step 821: 0.1577076017856598\n",
      "Max gradient at step 822: 0.21032017469406128\n",
      "Max gradient at step 823: 0.08044792711734772\n",
      "Max gradient at step 824: 0.1275334358215332\n",
      "Max gradient at step 825: 0.12477993220090866\n",
      "Max gradient at step 826: 0.13950395584106445\n",
      "Max gradient at step 827: 0.17428342998027802\n",
      "Max gradient at step 828: 0.2219916135072708\n",
      "Max gradient at step 829: 0.19317656755447388\n",
      "Max gradient at step 830: 0.15290287137031555\n",
      "Max gradient at step 831: 0.24465614557266235\n",
      "Max gradient at step 832: 0.1638059914112091\n",
      "Max gradient at step 833: 0.1448347121477127\n",
      "Max gradient at step 834: 0.1655467450618744\n",
      "Max gradient at step 835: 0.17485035955905914\n",
      "Max gradient at step 836: 0.17206741869449615\n",
      "Max gradient at step 837: 0.18354664742946625\n",
      "Max gradient at step 838: 0.15929238498210907\n",
      "Max gradient at step 839: 0.25354722142219543\n",
      "Max gradient at step 840: 0.11233451962471008\n",
      "Max gradient at step 841: 0.1785939633846283\n",
      "Max gradient at step 842: 0.14467784762382507\n",
      "Max gradient at step 843: 0.1265379637479782\n",
      "Max gradient at step 844: 0.09290452301502228\n",
      "Max gradient at step 845: 0.11326796561479568\n",
      "Max gradient at step 846: 0.17431385815143585\n",
      "Max gradient at step 847: 0.20435287058353424\n",
      "Max gradient at step 848: 0.23060739040374756\n",
      "Max gradient at step 849: 0.13345710933208466\n",
      "Max gradient at step 850: 0.20457950234413147\n",
      "Max gradient at step 851: 0.06452056020498276\n",
      "Max gradient at step 852: 0.08956437557935715\n",
      "Max gradient at step 853: 0.14339084923267365\n",
      "Max gradient at step 854: 0.27461400628089905\n",
      "Max gradient at step 855: 0.1362805962562561\n",
      "Max gradient at step 856: 0.1538563072681427\n",
      "Max gradient at step 857: 0.10841195285320282\n",
      "Max gradient at step 858: 0.14896781742572784\n",
      "Max gradient at step 859: 0.0656098797917366\n",
      "Max gradient at step 860: 0.1811356246471405\n",
      "Max gradient at step 861: 0.07571602612733841\n",
      "Max gradient at step 862: 0.22952893376350403\n",
      "Max gradient at step 863: 0.22117961943149567\n",
      "Max gradient at step 864: 0.13803307712078094\n",
      "Max gradient at step 865: 0.12921099364757538\n",
      "Max gradient at step 866: 0.1896367222070694\n",
      "Max gradient at step 867: 0.06243613734841347\n",
      "Max gradient at step 868: 0.22977350652217865\n",
      "Max gradient at step 869: 0.19057247042655945\n",
      "Max gradient at step 870: 0.20338982343673706\n",
      "Max gradient at step 871: 0.09274359047412872\n",
      "Max gradient at step 872: 0.11244696378707886\n",
      "Max gradient at step 873: 0.18672297894954681\n",
      "Max gradient at step 874: 0.1968887448310852\n",
      "Max gradient at step 875: 0.161864772439003\n",
      "Max gradient at step 876: 0.2530687749385834\n",
      "Max gradient at step 877: 0.12443351000547409\n",
      "Max gradient at step 878: 0.16973306238651276\n",
      "Max gradient at step 879: 0.30112507939338684\n",
      "Max gradient at step 880: 0.11846792697906494\n",
      "Max gradient at step 881: 0.18109674751758575\n",
      "Max gradient at step 882: 0.11477739363908768\n",
      "Max gradient at step 883: 0.14965270459651947\n",
      "Max gradient at step 884: 0.22226107120513916\n",
      "Max gradient at step 885: 0.2930688261985779\n",
      "Max gradient at step 886: 0.1668393611907959\n",
      "Max gradient at step 887: 0.16205839812755585\n",
      "Max gradient at step 888: 0.1048307865858078\n",
      "Max gradient at step 889: 0.2581382989883423\n",
      "Max gradient at step 890: 0.12134762853384018\n",
      "Max gradient at step 891: 0.2618614137172699\n",
      "Max gradient at step 892: 0.15233853459358215\n",
      "Max gradient at step 893: 0.24700365960597992\n",
      "Max gradient at step 894: 0.08916622400283813\n",
      "Max gradient at step 895: 0.08060064911842346\n",
      "Max gradient at step 896: 0.19176973402500153\n",
      "Max gradient at step 897: 0.11920025199651718\n",
      "Max gradient at step 898: 0.22886809706687927\n",
      "Max gradient at step 899: 0.21896116435527802\n",
      "Max gradient at step 900: 0.17184042930603027\n",
      "Training loss (for one batch) at step 900: 0.3485\n",
      "Max gradient at step 901: 0.127531960606575\n",
      "Max gradient at step 902: 0.10913456976413727\n",
      "Max gradient at step 903: 0.17765121161937714\n",
      "Max gradient at step 904: 0.1419670283794403\n",
      "Max gradient at step 905: 0.26174196600914\n",
      "Max gradient at step 906: 0.21313248574733734\n",
      "Max gradient at step 907: 0.17609158158302307\n",
      "Max gradient at step 908: 0.17366854846477509\n",
      "Max gradient at step 909: 0.27246782183647156\n",
      "Max gradient at step 910: 0.23004016280174255\n",
      "Max gradient at step 911: 0.0724964588880539\n",
      "Max gradient at step 912: 0.2172684371471405\n",
      "Max gradient at step 913: 0.09943096339702606\n",
      "Max gradient at step 914: 0.09485352039337158\n",
      "Max gradient at step 915: 0.10557946562767029\n",
      "Max gradient at step 916: 0.24143195152282715\n",
      "Max gradient at step 917: 0.2562597692012787\n",
      "Max gradient at step 918: 0.14659887552261353\n",
      "Max gradient at step 919: 0.2279375046491623\n",
      "Max gradient at step 920: 0.1710217148065567\n",
      "Max gradient at step 921: 0.1944708377122879\n",
      "Max gradient at step 922: 0.15217827260494232\n",
      "Max gradient at step 923: 0.2063273936510086\n",
      "Max gradient at step 924: 0.1963399350643158\n",
      "Max gradient at step 925: 0.19088003039360046\n",
      "Max gradient at step 926: 0.22624337673187256\n",
      "Max gradient at step 927: 0.18686331808567047\n",
      "Max gradient at step 928: 0.14185671508312225\n",
      "Max gradient at step 929: 0.15604686737060547\n",
      "Max gradient at step 930: 0.21512427926063538\n",
      "Max gradient at step 931: 0.17708322405815125\n",
      "Max gradient at step 932: 0.07277841120958328\n",
      "Max gradient at step 933: 0.21987947821617126\n",
      "Max gradient at step 934: 0.1478867530822754\n",
      "Max gradient at step 935: 0.17106187343597412\n",
      "Max gradient at step 936: 0.20732687413692474\n",
      "Max gradient at step 937: 0.14973221719264984\n",
      "Max gradient at step 938: 0.14589206874370575\n",
      "Max gradient at step 939: 0.20515500009059906\n",
      "Max gradient at step 940: 0.2026968151330948\n",
      "Max gradient at step 941: 0.09145417809486389\n",
      "Max gradient at step 942: 0.21137312054634094\n",
      "Max gradient at step 943: 0.15358081459999084\n",
      "Max gradient at step 944: 0.1592205911874771\n",
      "Max gradient at step 945: 0.1729927361011505\n",
      "Max gradient at step 946: 0.18915732204914093\n",
      "Max gradient at step 947: 0.13866063952445984\n",
      "Max gradient at step 948: 0.14311431348323822\n",
      "Max gradient at step 949: 0.17944517731666565\n",
      "Max gradient at step 950: 0.1605580598115921\n",
      "Max gradient at step 951: 0.23071718215942383\n",
      "Max gradient at step 952: 0.2527346909046173\n",
      "Max gradient at step 953: 0.3814951181411743\n",
      "Max gradient at step 954: 0.2282360941171646\n",
      "Max gradient at step 955: 0.3392779529094696\n",
      "Max gradient at step 956: 0.16565775871276855\n",
      "Max gradient at step 957: 0.12170296162366867\n",
      "Max gradient at step 958: 0.1258893609046936\n",
      "Max gradient at step 959: 0.18611033260822296\n",
      "Max gradient at step 960: 0.09688327461481094\n",
      "Max gradient at step 961: 0.14739422500133514\n",
      "Max gradient at step 962: 0.16940899193286896\n",
      "Max gradient at step 963: 0.1181316003203392\n",
      "Max gradient at step 964: 0.10638696700334549\n",
      "Max gradient at step 965: 0.23024867475032806\n",
      "Max gradient at step 966: 0.171974778175354\n",
      "Max gradient at step 967: 0.2196674644947052\n",
      "Max gradient at step 968: 0.12480324506759644\n",
      "Max gradient at step 969: 0.21792326867580414\n",
      "Max gradient at step 970: 0.19209221005439758\n",
      "Max gradient at step 971: 0.13109195232391357\n",
      "Max gradient at step 972: 0.3028942048549652\n",
      "Max gradient at step 973: 0.26063719391822815\n",
      "Max gradient at step 974: 0.1287759691476822\n",
      "Max gradient at step 975: 0.1840512901544571\n",
      "Max gradient at step 976: 0.2148146778345108\n",
      "Max gradient at step 977: 0.265959233045578\n",
      "Max gradient at step 978: 0.2599150836467743\n",
      "Max gradient at step 979: 0.20045964419841766\n",
      "Max gradient at step 980: 0.12290451675653458\n",
      "Max gradient at step 981: 0.24279743432998657\n",
      "Max gradient at step 982: 0.1773996651172638\n",
      "Max gradient at step 983: 0.171306312084198\n",
      "Max gradient at step 984: 0.19737838208675385\n",
      "Max gradient at step 985: 0.22188273072242737\n",
      "Max gradient at step 986: 0.12189211696386337\n",
      "Max gradient at step 987: 0.16488458216190338\n",
      "Max gradient at step 988: 0.16046138107776642\n",
      "Max gradient at step 989: 0.1307169795036316\n",
      "Max gradient at step 990: 0.18553099036216736\n",
      "Max gradient at step 991: 0.1277010291814804\n",
      "Max gradient at step 992: 0.14879241585731506\n",
      "Max gradient at step 993: 0.15232528746128082\n",
      "Max gradient at step 994: 0.280056893825531\n",
      "Max gradient at step 995: 0.14081071317195892\n",
      "Max gradient at step 996: 0.17730335891246796\n",
      "Max gradient at step 997: 0.1834186464548111\n",
      "Max gradient at step 998: 0.2148614078760147\n",
      "Max gradient at step 999: 0.30957096815109253\n",
      "Max gradient at step 1000: 0.2865443229675293\n",
      "Training loss (for one batch) at step 1000: 0.2830\n",
      "Max gradient at step 1001: 0.14269906282424927\n",
      "Max gradient at step 1002: 0.11974114924669266\n",
      "Max gradient at step 1003: 0.16988810896873474\n",
      "Max gradient at step 1004: 0.16068559885025024\n",
      "Max gradient at step 1005: 0.20566383004188538\n",
      "Max gradient at step 1006: 0.19439229369163513\n",
      "Max gradient at step 1007: 0.14227667450904846\n",
      "Max gradient at step 1008: 0.2333754152059555\n",
      "Max gradient at step 1009: 0.17765381932258606\n",
      "Max gradient at step 1010: 0.15550440549850464\n",
      "Max gradient at step 1011: 0.1346021592617035\n",
      "Max gradient at step 1012: 0.22811363637447357\n",
      "Max gradient at step 1013: 0.1327877938747406\n",
      "Max gradient at step 1014: 0.17890594899654388\n",
      "Max gradient at step 1015: 0.13305090367794037\n",
      "Max gradient at step 1016: 0.2064318209886551\n",
      "Max gradient at step 1017: 0.14862613379955292\n",
      "Max gradient at step 1018: 0.1035088300704956\n",
      "Max gradient at step 1019: 0.22646531462669373\n",
      "Max gradient at step 1020: 0.18100428581237793\n",
      "Max gradient at step 1021: 0.17482957243919373\n",
      "Max gradient at step 1022: 0.1280006468296051\n",
      "Max gradient at step 1023: 0.09382780641317368\n",
      "Max gradient at step 1024: 0.3288743197917938\n",
      "Max gradient at step 1025: 0.19986577332019806\n",
      "Max gradient at step 1026: 0.16346809267997742\n",
      "Max gradient at step 1027: 0.12972790002822876\n",
      "Max gradient at step 1028: 0.12618334591388702\n",
      "Max gradient at step 1029: 0.23216193914413452\n",
      "Max gradient at step 1030: 0.14844846725463867\n",
      "Max gradient at step 1031: 0.12368351966142654\n",
      "Max gradient at step 1032: 0.1445547491312027\n",
      "Max gradient at step 1033: 0.11319983750581741\n",
      "Max gradient at step 1034: 0.2615310251712799\n",
      "Max gradient at step 1035: 0.2663835883140564\n",
      "Max gradient at step 1036: 0.19078215956687927\n",
      "Max gradient at step 1037: 0.29053226113319397\n",
      "Max gradient at step 1038: 0.13086622953414917\n",
      "Max gradient at step 1039: 0.1406070441007614\n",
      "Max gradient at step 1040: 0.22821873426437378\n",
      "Max gradient at step 1041: 0.11376725137233734\n",
      "Max gradient at step 1042: 0.20341172814369202\n",
      "Max gradient at step 1043: 0.18802767992019653\n",
      "Max gradient at step 1044: 0.26659587025642395\n",
      "Max gradient at step 1045: 0.10790219902992249\n",
      "Max gradient at step 1046: 0.24070972204208374\n",
      "Max gradient at step 1047: 0.12428797781467438\n",
      "Max gradient at step 1048: 0.3216498792171478\n",
      "Max gradient at step 1049: 0.20156599581241608\n",
      "Max gradient at step 1050: 0.15166567265987396\n",
      "Max gradient at step 1051: 0.1402844488620758\n",
      "Max gradient at step 1052: 0.42757928371429443\n",
      "Max gradient at step 1053: 0.142687126994133\n",
      "Max gradient at step 1054: 0.1289074420928955\n",
      "Max gradient at step 1055: 0.22935061156749725\n",
      "Max gradient at step 1056: 0.20267602801322937\n",
      "Max gradient at step 1057: 0.17044709622859955\n",
      "Max gradient at step 1058: 0.28617602586746216\n",
      "Max gradient at step 1059: 0.29719361662864685\n",
      "Max gradient at step 1060: 0.22291241586208344\n",
      "Max gradient at step 1061: 0.08400570601224899\n",
      "Max gradient at step 1062: 0.2847662568092346\n",
      "Max gradient at step 1063: 0.17751160264015198\n",
      "Max gradient at step 1064: 0.14855043590068817\n",
      "Max gradient at step 1065: 0.13686901330947876\n",
      "Max gradient at step 1066: 0.3030812442302704\n",
      "Max gradient at step 1067: 0.12265416234731674\n",
      "Max gradient at step 1068: 0.19329573214054108\n",
      "Max gradient at step 1069: 0.12435116618871689\n",
      "Max gradient at step 1070: 0.13746647536754608\n",
      "Max gradient at step 1071: 0.0899081602692604\n",
      "Max gradient at step 1072: 0.1364915370941162\n",
      "Max gradient at step 1073: 0.18681789934635162\n",
      "Max gradient at step 1074: 0.12262817472219467\n",
      "Max gradient at step 1075: 0.09740711748600006\n",
      "Max gradient at step 1076: 0.25948619842529297\n",
      "Max gradient at step 1077: 0.17343571782112122\n",
      "Max gradient at step 1078: 0.1831931471824646\n",
      "Max gradient at step 1079: 0.14228646457195282\n",
      "Max gradient at step 1080: 0.15157456696033478\n",
      "Max gradient at step 1081: 0.21224844455718994\n",
      "Max gradient at step 1082: 0.11411246657371521\n",
      "Max gradient at step 1083: 0.15753377974033356\n",
      "Max gradient at step 1084: 0.1974814087152481\n",
      "Max gradient at step 1085: 0.16984623670578003\n",
      "Max gradient at step 1086: 0.26534485816955566\n",
      "Max gradient at step 1087: 0.11368519067764282\n",
      "Max gradient at step 1088: 0.2672976851463318\n",
      "Max gradient at step 1089: 0.31120267510414124\n",
      "Max gradient at step 1090: 0.07468249648809433\n",
      "Max gradient at step 1091: 0.1388978213071823\n",
      "Max gradient at step 1092: 0.2268337607383728\n",
      "Max gradient at step 1093: 0.18448150157928467\n",
      "Max gradient at step 1094: 0.20541168749332428\n",
      "Max gradient at step 1095: 0.15100514888763428\n",
      "Max gradient at step 1096: 0.19932986795902252\n",
      "Max gradient at step 1097: 0.12356628477573395\n",
      "Max gradient at step 1098: 0.24494542181491852\n",
      "Max gradient at step 1099: 0.09204375743865967\n",
      "Max gradient at step 1100: 0.19001127779483795\n",
      "Training loss (for one batch) at step 1100: 0.1955\n",
      "Max gradient at step 1101: 0.057309869676828384\n",
      "Max gradient at step 1102: 0.20821742713451385\n",
      "Max gradient at step 1103: 0.1657121479511261\n",
      "Max gradient at step 1104: 0.22945784032344818\n",
      "Max gradient at step 1105: 0.21821321547031403\n",
      "Max gradient at step 1106: 0.1871003806591034\n",
      "Max gradient at step 1107: 0.1654120236635208\n",
      "Max gradient at step 1108: 0.23753637075424194\n",
      "Max gradient at step 1109: 0.16768792271614075\n",
      "Max gradient at step 1110: 0.08619215339422226\n",
      "Max gradient at step 1111: 0.07603316009044647\n",
      "Max gradient at step 1112: 0.18108750879764557\n",
      "Max gradient at step 1113: 0.05726473405957222\n",
      "Max gradient at step 1114: 0.09667519479990005\n",
      "Max gradient at step 1115: 0.18873146176338196\n",
      "Max gradient at step 1116: 0.1646263301372528\n",
      "Max gradient at step 1117: 0.16911226511001587\n",
      "Max gradient at step 1118: 0.26393213868141174\n",
      "Max gradient at step 1119: 0.24504022300243378\n",
      "Max gradient at step 1120: 0.14555981755256653\n",
      "Max gradient at step 1121: 0.1399335116147995\n",
      "Max gradient at step 1122: 0.11214214563369751\n",
      "Max gradient at step 1123: 0.23764517903327942\n",
      "Max gradient at step 1124: 0.1744047850370407\n",
      "Max gradient at step 1125: 0.19280458986759186\n",
      "Max gradient at step 1126: 0.13544316589832306\n",
      "Max gradient at step 1127: 0.1675768941640854\n",
      "Max gradient at step 1128: 0.23105913400650024\n",
      "Max gradient at step 1129: 0.14489562809467316\n",
      "Max gradient at step 1130: 0.2138506919145584\n",
      "Max gradient at step 1131: 0.19748079776763916\n",
      "Max gradient at step 1132: 0.19060514867305756\n",
      "Max gradient at step 1133: 0.1735066920518875\n",
      "Max gradient at step 1134: 0.17364181578159332\n",
      "Max gradient at step 1135: 0.12362825870513916\n",
      "Max gradient at step 1136: 0.2660762071609497\n",
      "Max gradient at step 1137: 0.10795177519321442\n",
      "Max gradient at step 1138: 0.19131653010845184\n",
      "Max gradient at step 1139: 0.2577977776527405\n",
      "Max gradient at step 1140: 0.20075133442878723\n",
      "Max gradient at step 1141: 0.45884668827056885\n",
      "Max gradient at step 1142: 0.28198859095573425\n",
      "Max gradient at step 1143: 0.1079336404800415\n",
      "Max gradient at step 1144: 0.16000792384147644\n",
      "Max gradient at step 1145: 0.2641541361808777\n",
      "Max gradient at step 1146: 0.24088801443576813\n",
      "Max gradient at step 1147: 0.2759783864021301\n",
      "Max gradient at step 1148: 0.23168624937534332\n",
      "Max gradient at step 1149: 0.19180254638195038\n",
      "Max gradient at step 1150: 0.12121963500976562\n",
      "Max gradient at step 1151: 0.23093225061893463\n",
      "Max gradient at step 1152: 0.15276139974594116\n",
      "Max gradient at step 1153: 0.1379580944776535\n",
      "Max gradient at step 1154: 0.15036223828792572\n",
      "Max gradient at step 1155: 0.17612293362617493\n",
      "Max gradient at step 1156: 0.15179838240146637\n",
      "Max gradient at step 1157: 0.06259556859731674\n",
      "Max gradient at step 1158: 0.20685146749019623\n",
      "Max gradient at step 1159: 0.1496470421552658\n",
      "Max gradient at step 1160: 0.18155282735824585\n",
      "Max gradient at step 1161: 0.12332942336797714\n",
      "Max gradient at step 1162: 0.16521763801574707\n",
      "Max gradient at step 1163: 0.11871358007192612\n",
      "Max gradient at step 1164: 0.16918155550956726\n",
      "Max gradient at step 1165: 0.1687776893377304\n",
      "Max gradient at step 1166: 0.18941672146320343\n",
      "Max gradient at step 1167: 0.30196377635002136\n",
      "Max gradient at step 1168: 0.2477983832359314\n",
      "Max gradient at step 1169: 0.18404576182365417\n",
      "Max gradient at step 1170: 0.25361719727516174\n",
      "Max gradient at step 1171: 0.11916711181402206\n",
      "Max gradient at step 1172: 0.25829267501831055\n",
      "Max gradient at step 1173: 0.21619847416877747\n",
      "Max gradient at step 1174: 0.17686396837234497\n",
      "Max gradient at step 1175: 0.11238417029380798\n",
      "Max gradient at step 1176: 0.13861490786075592\n",
      "Max gradient at step 1177: 0.16797952353954315\n",
      "Max gradient at step 1178: 0.11386774480342865\n",
      "Max gradient at step 1179: 0.16046656668186188\n",
      "Max gradient at step 1180: 0.22227288782596588\n",
      "Max gradient at step 1181: 0.16733983159065247\n",
      "Max gradient at step 1182: 0.21982157230377197\n",
      "Max gradient at step 1183: 0.1708136796951294\n",
      "Max gradient at step 1184: 0.20050673186779022\n",
      "Max gradient at step 1185: 0.15318965911865234\n",
      "Max gradient at step 1186: 0.16198992729187012\n",
      "Max gradient at step 1187: 0.12598614394664764\n",
      "Max gradient at step 1188: 0.12450547516345978\n",
      "Max gradient at step 1189: 0.16804882884025574\n",
      "Max gradient at step 1190: 0.23115624487400055\n",
      "Max gradient at step 1191: 0.1652287095785141\n",
      "Max gradient at step 1192: 0.11072969436645508\n",
      "Max gradient at step 1193: 0.0736175924539566\n",
      "Max gradient at step 1194: 0.22515197098255157\n",
      "Max gradient at step 1195: 0.20889973640441895\n",
      "Max gradient at step 1196: 0.1795452982187271\n",
      "Max gradient at step 1197: 0.13955272734165192\n",
      "Max gradient at step 1198: 0.23322413861751556\n",
      "Max gradient at step 1199: 0.13915970921516418\n",
      "Max gradient at step 1200: 0.14621375501155853\n",
      "Training loss (for one batch) at step 1200: 0.4836\n",
      "Max gradient at step 1201: 0.13502834737300873\n",
      "Max gradient at step 1202: 0.14432817697525024\n",
      "Max gradient at step 1203: 0.1506694108247757\n",
      "Max gradient at step 1204: 0.13718505203723907\n",
      "Max gradient at step 1205: 0.150641068816185\n",
      "Max gradient at step 1206: 0.21339765191078186\n",
      "Max gradient at step 1207: 0.1841391772031784\n",
      "Max gradient at step 1208: 0.20587944984436035\n",
      "Max gradient at step 1209: 0.1889794021844864\n",
      "Max gradient at step 1210: 0.12452839314937592\n",
      "Max gradient at step 1211: 0.09920328110456467\n",
      "Max gradient at step 1212: 0.1800336390733719\n",
      "Max gradient at step 1213: 0.20961356163024902\n",
      "Max gradient at step 1214: 0.2802373766899109\n",
      "Max gradient at step 1215: 0.19181859493255615\n",
      "Max gradient at step 1216: 0.11789846420288086\n",
      "Max gradient at step 1217: 0.17894615232944489\n",
      "Max gradient at step 1218: 0.18964210152626038\n",
      "Max gradient at step 1219: 0.16211764514446259\n",
      "Max gradient at step 1220: 0.08809265494346619\n",
      "Max gradient at step 1221: 0.3105897009372711\n",
      "Max gradient at step 1222: 0.1078936755657196\n",
      "Max gradient at step 1223: 0.1693420261144638\n",
      "Max gradient at step 1224: 0.18072509765625\n",
      "Max gradient at step 1225: 0.1372833400964737\n",
      "Max gradient at step 1226: 0.08979123830795288\n",
      "Max gradient at step 1227: 0.13800694048404694\n",
      "Max gradient at step 1228: 0.13509930670261383\n",
      "Max gradient at step 1229: 0.2663328945636749\n",
      "Max gradient at step 1230: 0.15472173690795898\n",
      "Max gradient at step 1231: 0.14151979982852936\n",
      "Max gradient at step 1232: 0.15956537425518036\n",
      "Max gradient at step 1233: 0.1659521758556366\n",
      "Max gradient at step 1234: 0.2486523538827896\n",
      "Max gradient at step 1235: 0.20073972642421722\n",
      "Max gradient at step 1236: 0.2571021318435669\n",
      "Max gradient at step 1237: 0.22910313308238983\n",
      "Max gradient at step 1238: 0.12332746386528015\n",
      "Max gradient at step 1239: 0.15103062987327576\n",
      "Max gradient at step 1240: 0.2511296570301056\n",
      "Max gradient at step 1241: 0.19290535151958466\n",
      "Max gradient at step 1242: 0.08362983167171478\n",
      "Max gradient at step 1243: 0.2071407437324524\n",
      "Max gradient at step 1244: 0.1774929016828537\n",
      "Max gradient at step 1245: 0.1264217644929886\n",
      "Max gradient at step 1246: 0.13781113922595978\n",
      "Max gradient at step 1247: 0.13894519209861755\n",
      "Max gradient at step 1248: 0.13315744698047638\n",
      "Max gradient at step 1249: 0.14663611352443695\n",
      "Max gradient at step 1250: 0.19823101162910461\n",
      "Max gradient at step 1251: 0.10566801577806473\n",
      "Max gradient at step 1252: 0.14444631338119507\n",
      "Max gradient at step 1253: 0.18968133628368378\n",
      "Max gradient at step 1254: 0.20814307034015656\n",
      "Max gradient at step 1255: 0.08928809314966202\n",
      "Max gradient at step 1256: 0.1953810453414917\n",
      "Max gradient at step 1257: 0.13398580253124237\n",
      "Max gradient at step 1258: 0.11230701953172684\n",
      "Max gradient at step 1259: 0.1423269361257553\n",
      "Max gradient at step 1260: 0.07891838252544403\n",
      "Max gradient at step 1261: 0.07296525686979294\n",
      "Max gradient at step 1262: 0.13393941521644592\n",
      "Max gradient at step 1263: 0.1621895134449005\n",
      "Max gradient at step 1264: 0.2093377411365509\n",
      "Max gradient at step 1265: 0.14487013220787048\n",
      "Max gradient at step 1266: 0.10833269357681274\n",
      "Max gradient at step 1267: 0.1408427506685257\n",
      "Max gradient at step 1268: 0.278070330619812\n",
      "Max gradient at step 1269: 0.1500854343175888\n",
      "Max gradient at step 1270: 0.18461939692497253\n",
      "Max gradient at step 1271: 0.22364425659179688\n",
      "Max gradient at step 1272: 0.21288633346557617\n",
      "Max gradient at step 1273: 0.34170404076576233\n",
      "Max gradient at step 1274: 0.1632460057735443\n",
      "Max gradient at step 1275: 0.23895038664340973\n",
      "Max gradient at step 1276: 0.3309113681316376\n",
      "Max gradient at step 1277: 0.15937933325767517\n",
      "Max gradient at step 1278: 0.18226221203804016\n",
      "Max gradient at step 1279: 0.10598156601190567\n",
      "Max gradient at step 1280: 0.10307516157627106\n",
      "Max gradient at step 1281: 0.1361197829246521\n",
      "Max gradient at step 1282: 0.15102806687355042\n",
      "Max gradient at step 1283: 0.20660102367401123\n",
      "Max gradient at step 1284: 0.06966860592365265\n",
      "Max gradient at step 1285: 0.17546486854553223\n",
      "Max gradient at step 1286: 0.19494296610355377\n",
      "Max gradient at step 1287: 0.19107788801193237\n",
      "Max gradient at step 1288: 0.21234653890132904\n",
      "Max gradient at step 1289: 0.22707921266555786\n",
      "Max gradient at step 1290: 0.29380613565444946\n",
      "Max gradient at step 1291: 0.3434574604034424\n",
      "Max gradient at step 1292: 0.1542760729789734\n",
      "Max gradient at step 1293: 0.12451894581317902\n",
      "Max gradient at step 1294: 0.21107901632785797\n",
      "Max gradient at step 1295: 0.2225014716386795\n",
      "Max gradient at step 1296: 0.2624511122703552\n",
      "Max gradient at step 1297: 0.2744494676589966\n",
      "Max gradient at step 1298: 0.029392404481768608\n",
      "Max gradient at step 1299: 0.2567392885684967\n",
      "Max gradient at step 1300: 0.4406929910182953\n",
      "Training loss (for one batch) at step 1300: 0.7868\n",
      "Max gradient at step 1301: 0.22100332379341125\n",
      "Max gradient at step 1302: 0.1664569228887558\n",
      "Max gradient at step 1303: 0.1799534410238266\n",
      "Max gradient at step 1304: 0.14100715517997742\n",
      "Max gradient at step 1305: 0.10781800746917725\n",
      "Max gradient at step 1306: 0.24756665527820587\n",
      "Max gradient at step 1307: 0.214529886841774\n",
      "Max gradient at step 1308: 0.24215927720069885\n",
      "Max gradient at step 1309: 0.23431876301765442\n",
      "Max gradient at step 1310: 0.11308076977729797\n",
      "Max gradient at step 1311: 0.2125413566827774\n",
      "Max gradient at step 1312: 0.18718206882476807\n",
      "Max gradient at step 1313: 0.26143959164619446\n",
      "Max gradient at step 1314: 0.27317672967910767\n",
      "Max gradient at step 1315: 0.14469164609909058\n",
      "Max gradient at step 1316: 0.3230498433113098\n",
      "Max gradient at step 1317: 0.11678344756364822\n",
      "Max gradient at step 1318: 0.316413015127182\n",
      "Max gradient at step 1319: 0.1293899267911911\n",
      "Max gradient at step 1320: 0.2710309624671936\n",
      "Max gradient at step 1321: 0.22232185304164886\n",
      "Max gradient at step 1322: 0.16815738379955292\n",
      "Max gradient at step 1323: 0.19233284890651703\n",
      "Max gradient at step 1324: 0.1476716548204422\n",
      "Max gradient at step 1325: 0.07906795293092728\n",
      "Max gradient at step 1326: 0.194801464676857\n",
      "Max gradient at step 1327: 0.1125175878405571\n",
      "Max gradient at step 1328: 0.20849289000034332\n",
      "Max gradient at step 1329: 0.1951427459716797\n",
      "Max gradient at step 1330: 0.2431415468454361\n",
      "Max gradient at step 1331: 0.15901145339012146\n",
      "Max gradient at step 1332: 0.2311771959066391\n",
      "Max gradient at step 1333: 0.08162404596805573\n",
      "Max gradient at step 1334: 0.16747693717479706\n",
      "Max gradient at step 1335: 0.30646029114723206\n",
      "Max gradient at step 1336: 0.1536485105752945\n",
      "Max gradient at step 1337: 0.05546033754944801\n",
      "Max gradient at step 1338: 0.18486841022968292\n",
      "Max gradient at step 1339: 0.21453051269054413\n",
      "Max gradient at step 1340: 0.17008927464485168\n",
      "Max gradient at step 1341: 0.16441231966018677\n",
      "Max gradient at step 1342: 0.1382555216550827\n",
      "Max gradient at step 1343: 0.15778911113739014\n",
      "Max gradient at step 1344: 0.21565937995910645\n",
      "Max gradient at step 1345: 0.20497211813926697\n",
      "Max gradient at step 1346: 0.2193560153245926\n",
      "Max gradient at step 1347: 0.08683619648218155\n",
      "Max gradient at step 1348: 0.14856408536434174\n",
      "Max gradient at step 1349: 0.1663092076778412\n",
      "Max gradient at step 1350: 0.15453554689884186\n",
      "Max gradient at step 1351: 0.10525225847959518\n",
      "Max gradient at step 1352: 0.18859080970287323\n",
      "Max gradient at step 1353: 0.16997967660427094\n",
      "Max gradient at step 1354: 0.09187200665473938\n",
      "Max gradient at step 1355: 0.21900758147239685\n",
      "Max gradient at step 1356: 0.1317114531993866\n",
      "Max gradient at step 1357: 0.36923137307167053\n",
      "Max gradient at step 1358: 0.16236020624637604\n",
      "Max gradient at step 1359: 0.12000555545091629\n",
      "Max gradient at step 1360: 0.36789169907569885\n",
      "Max gradient at step 1361: 0.09850169718265533\n",
      "Max gradient at step 1362: 0.12614953517913818\n",
      "Max gradient at step 1363: 0.08602678775787354\n",
      "Max gradient at step 1364: 0.18667086958885193\n",
      "Max gradient at step 1365: 0.18997912108898163\n",
      "Max gradient at step 1366: 0.11632788181304932\n",
      "Max gradient at step 1367: 0.3118031919002533\n",
      "Max gradient at step 1368: 0.20997627079486847\n",
      "Max gradient at step 1369: 0.14112257957458496\n",
      "Max gradient at step 1370: 0.09056674689054489\n",
      "Max gradient at step 1371: 0.19708670675754547\n",
      "Max gradient at step 1372: 0.19137102365493774\n",
      "Max gradient at step 1373: 0.16203932464122772\n",
      "Max gradient at step 1374: 0.12543147802352905\n",
      "Max gradient at step 1375: 0.3748610019683838\n",
      "Max gradient at step 1376: 0.18431413173675537\n",
      "Max gradient at step 1377: 0.06154939904808998\n",
      "Max gradient at step 1378: 0.23208610713481903\n",
      "Max gradient at step 1379: 0.26971304416656494\n",
      "Max gradient at step 1380: 0.2180667370557785\n",
      "Max gradient at step 1381: 0.17007791996002197\n",
      "Max gradient at step 1382: 0.21510526537895203\n",
      "Max gradient at step 1383: 0.07975982129573822\n",
      "Max gradient at step 1384: 0.1985516995191574\n",
      "Max gradient at step 1385: 0.0973266214132309\n",
      "Max gradient at step 1386: 0.14179296791553497\n",
      "Max gradient at step 1387: 0.13425059616565704\n",
      "Max gradient at step 1388: 0.17802681028842926\n",
      "Max gradient at step 1389: 0.2211674004793167\n",
      "Max gradient at step 1390: 0.14828968048095703\n",
      "Max gradient at step 1391: 0.2227223515510559\n",
      "Max gradient at step 1392: 0.22817468643188477\n",
      "Max gradient at step 1393: 0.09801120311021805\n",
      "Max gradient at step 1394: 0.14371003210544586\n",
      "Max gradient at step 1395: 0.3139132261276245\n",
      "Max gradient at step 1396: 0.11840313673019409\n",
      "Max gradient at step 1397: 0.22194582223892212\n",
      "Max gradient at step 1398: 0.16710956394672394\n",
      "Max gradient at step 1399: 0.1841859668493271\n",
      "Max gradient at step 1400: 0.1078450009226799\n",
      "Training loss (for one batch) at step 1400: 0.1398\n",
      "Max gradient at step 1401: 0.15744315087795258\n",
      "Max gradient at step 1402: 0.2840996980667114\n",
      "Max gradient at step 1403: 0.2754053473472595\n",
      "Max gradient at step 1404: 0.28489699959754944\n",
      "Max gradient at step 1405: 0.14313386380672455\n",
      "Max gradient at step 1406: 0.31280407309532166\n",
      "Max gradient at step 1407: 0.2060340940952301\n",
      "Max gradient at step 1408: 0.1813197284936905\n",
      "Max gradient at step 1409: 0.1176786944270134\n",
      "Max gradient at step 1410: 0.285203754901886\n",
      "Max gradient at step 1411: 0.16840018332004547\n",
      "Max gradient at step 1412: 0.1544579714536667\n",
      "Max gradient at step 1413: 0.07184315472841263\n",
      "Max gradient at step 1414: 0.15680140256881714\n",
      "Max gradient at step 1415: 0.43226557970046997\n",
      "Max gradient at step 1416: 0.11536410450935364\n",
      "Max gradient at step 1417: 0.18194763362407684\n",
      "Max gradient at step 1418: 0.19668439030647278\n",
      "Max gradient at step 1419: 0.23624591529369354\n",
      "Max gradient at step 1420: 0.16931569576263428\n",
      "Max gradient at step 1421: 0.35421204566955566\n",
      "Max gradient at step 1422: 0.13231569528579712\n",
      "Max gradient at step 1423: 0.32506370544433594\n",
      "Max gradient at step 1424: 0.16908833384513855\n",
      "Max gradient at step 1425: 0.15508779883384705\n",
      "Max gradient at step 1426: 0.18024122714996338\n",
      "Max gradient at step 1427: 0.17981427907943726\n",
      "Max gradient at step 1428: 0.14392873644828796\n",
      "Max gradient at step 1429: 0.18322357535362244\n",
      "Max gradient at step 1430: 0.2715282738208771\n",
      "Max gradient at step 1431: 0.07916190475225449\n",
      "Max gradient at step 1432: 0.10322088748216629\n",
      "Max gradient at step 1433: 0.20629416406154633\n",
      "Max gradient at step 1434: 0.18766315281391144\n",
      "Max gradient at step 1435: 0.2988086938858032\n",
      "Max gradient at step 1436: 0.1897030621767044\n",
      "Max gradient at step 1437: 0.1679573357105255\n",
      "Max gradient at step 1438: 0.26496079564094543\n",
      "Max gradient at step 1439: 0.1336006373167038\n",
      "Max gradient at step 1440: 0.15421843528747559\n",
      "Max gradient at step 1441: 0.14358685910701752\n",
      "Max gradient at step 1442: 0.1710491180419922\n",
      "Max gradient at step 1443: 0.09499560296535492\n",
      "Max gradient at step 1444: 0.2402818351984024\n",
      "Max gradient at step 1445: 0.20878121256828308\n",
      "Max gradient at step 1446: 0.13882403075695038\n",
      "Max gradient at step 1447: 0.17914815247058868\n",
      "Max gradient at step 1448: 0.20440295338630676\n",
      "Max gradient at step 1449: 0.14666317403316498\n",
      "Max gradient at step 1450: 0.2156619280576706\n",
      "Max gradient at step 1451: 0.1520669311285019\n",
      "Max gradient at step 1452: 0.08639717847108841\n",
      "Max gradient at step 1453: 0.2823818624019623\n",
      "Max gradient at step 1454: 0.12931188941001892\n",
      "Max gradient at step 1455: 0.1759083867073059\n",
      "Max gradient at step 1456: 0.16071242094039917\n",
      "Max gradient at step 1457: 0.2859284579753876\n",
      "Max gradient at step 1458: 0.13464106619358063\n",
      "Max gradient at step 1459: 0.1255042999982834\n",
      "Max gradient at step 1460: 0.17232903838157654\n",
      "Max gradient at step 1461: 0.16234257817268372\n",
      "Max gradient at step 1462: 0.16999803483486176\n",
      "Max gradient at step 1463: 0.19952407479286194\n",
      "Max gradient at step 1464: 0.09993404150009155\n",
      "Max gradient at step 1465: 0.17204612493515015\n",
      "Max gradient at step 1466: 0.2968023121356964\n",
      "Max gradient at step 1467: 0.2753312587738037\n",
      "Max gradient at step 1468: 0.0977678894996643\n",
      "Max gradient at step 1469: 0.2172645926475525\n",
      "Max gradient at step 1470: 0.19210845232009888\n",
      "Max gradient at step 1471: 0.2579113245010376\n",
      "Max gradient at step 1472: 0.3480202257633209\n",
      "Max gradient at step 1473: 0.3021801710128784\n",
      "Max gradient at step 1474: 0.285919189453125\n",
      "Max gradient at step 1475: 0.19238877296447754\n",
      "Max gradient at step 1476: 0.1340956836938858\n",
      "Max gradient at step 1477: 0.23992732167243958\n",
      "Max gradient at step 1478: 0.09669096767902374\n",
      "Max gradient at step 1479: 0.19033941626548767\n",
      "Max gradient at step 1480: 0.13871915638446808\n",
      "Max gradient at step 1481: 0.1278495192527771\n",
      "Max gradient at step 1482: 0.15391063690185547\n",
      "Max gradient at step 1483: 0.1935940980911255\n",
      "Max gradient at step 1484: 0.14156103134155273\n",
      "Max gradient at step 1485: 0.12591902911663055\n",
      "Max gradient at step 1486: 0.18380025029182434\n",
      "Max gradient at step 1487: 0.12636785209178925\n",
      "Max gradient at step 1488: 0.11665402352809906\n",
      "Max gradient at step 1489: 0.18952377140522003\n",
      "Max gradient at step 1490: 0.10967900604009628\n",
      "Max gradient at step 1491: 0.13715653121471405\n",
      "Max gradient at step 1492: 0.3338489234447479\n",
      "Max gradient at step 1493: 0.1654585599899292\n",
      "Max gradient at step 1494: 0.15164834260940552\n",
      "Max gradient at step 1495: 0.1257796287536621\n",
      "Max gradient at step 1496: 0.24547600746154785\n",
      "Max gradient at step 1497: 0.35453349351882935\n",
      "Max gradient at step 1498: 0.13687890768051147\n",
      "Max gradient at step 1499: 0.08396738767623901\n",
      "Max gradient at step 1500: 0.26285889744758606\n",
      "Training loss (for one batch) at step 1500: 0.2273\n",
      "Max gradient at step 1501: 0.14995142817497253\n",
      "Max gradient at step 1502: 0.09555061161518097\n",
      "Max gradient at step 1503: 0.2425897717475891\n",
      "Max gradient at step 1504: 0.21235239505767822\n",
      "Max gradient at step 1505: 0.18560431897640228\n",
      "Max gradient at step 1506: 0.13992242515087128\n",
      "Max gradient at step 1507: 0.1418391913175583\n",
      "Max gradient at step 1508: 0.18951401114463806\n",
      "Max gradient at step 1509: 0.2270679622888565\n",
      "Max gradient at step 1510: 0.1624448448419571\n",
      "Max gradient at step 1511: 0.24570101499557495\n",
      "Max gradient at step 1512: 0.15901021659374237\n",
      "Max gradient at step 1513: 0.22944827377796173\n",
      "Max gradient at step 1514: 0.1272761970758438\n",
      "Max gradient at step 1515: 0.18151849508285522\n",
      "Max gradient at step 1516: 0.14256225526332855\n",
      "Max gradient at step 1517: 0.18562555313110352\n",
      "Max gradient at step 1518: 0.20436584949493408\n",
      "Max gradient at step 1519: 0.13728690147399902\n",
      "Max gradient at step 1520: 0.1973697990179062\n",
      "Max gradient at step 1521: 0.17030970752239227\n",
      "Max gradient at step 1522: 0.23173882067203522\n",
      "Max gradient at step 1523: 0.1522148996591568\n",
      "Max gradient at step 1524: 0.16581009328365326\n",
      "Max gradient at step 1525: 0.17689040303230286\n",
      "Max gradient at step 1526: 0.11822225898504257\n",
      "Max gradient at step 1527: 0.0818304643034935\n",
      "Max gradient at step 1528: 0.17900216579437256\n",
      "Max gradient at step 1529: 0.18739038705825806\n",
      "Max gradient at step 1530: 0.10202524065971375\n",
      "Max gradient at step 1531: 0.20486417412757874\n",
      "Max gradient at step 1532: 0.5059028267860413\n",
      "Max gradient at step 1533: 0.14057449996471405\n",
      "Max gradient at step 1534: 0.17687685787677765\n",
      "Max gradient at step 1535: 0.30502668023109436\n",
      "Max gradient at step 1536: 0.18957170844078064\n",
      "Max gradient at step 1537: 0.11693315953016281\n",
      "Max gradient at step 1538: 0.23582203686237335\n",
      "Max gradient at step 1539: 0.1494496613740921\n",
      "Max gradient at step 1540: 0.2762397825717926\n",
      "Max gradient at step 1541: 0.18824318051338196\n",
      "Max gradient at step 1542: 0.10923311859369278\n",
      "Max gradient at step 1543: 0.1408868432044983\n",
      "Max gradient at step 1544: 0.1706295907497406\n",
      "Max gradient at step 1545: 0.1402115821838379\n",
      "Max gradient at step 1546: 0.1537858098745346\n",
      "Max gradient at step 1547: 0.18194738030433655\n",
      "Max gradient at step 1548: 0.27147766947746277\n",
      "Max gradient at step 1549: 0.1807853728532791\n",
      "Max gradient at step 1550: 0.14892907440662384\n",
      "Max gradient at step 1551: 0.09478697180747986\n",
      "Max gradient at step 1552: 0.15195180475711823\n",
      "Max gradient at step 1553: 0.1844145953655243\n",
      "Max gradient at step 1554: 0.26375812292099\n",
      "Max gradient at step 1555: 0.18759849667549133\n",
      "Max gradient at step 1556: 0.18075966835021973\n",
      "Max gradient at step 1557: 0.13522450625896454\n",
      "Max gradient at step 1558: 0.25361570715904236\n",
      "Max gradient at step 1559: 0.14270295202732086\n",
      "Max gradient at step 1560: 0.13412544131278992\n",
      "Max gradient at step 1561: 0.12110541760921478\n",
      "Max gradient at step 1562: 0.20133692026138306\n",
      "Max gradient at step 1563: 0.14839933812618256\n",
      "Max gradient at step 1564: 0.2077745795249939\n",
      "Max gradient at step 1565: 0.1955617070198059\n",
      "Max gradient at step 1566: 0.20217697322368622\n",
      "Max gradient at step 1567: 0.20423993468284607\n",
      "Max gradient at step 1568: 0.1551675647497177\n",
      "Max gradient at step 1569: 0.13916803896427155\n",
      "Max gradient at step 1570: 0.12217161804437637\n",
      "Max gradient at step 1571: 0.17744891345500946\n",
      "Max gradient at step 1572: 0.16124626994132996\n",
      "Max gradient at step 1573: 0.18219833076000214\n",
      "Max gradient at step 1574: 0.2282225340604782\n",
      "Max gradient at step 1575: 0.15248966217041016\n",
      "Max gradient at step 1576: 0.15890629589557648\n",
      "Max gradient at step 1577: 0.181550532579422\n",
      "Max gradient at step 1578: 0.17668333649635315\n",
      "Max gradient at step 1579: 0.20692628622055054\n",
      "Max gradient at step 1580: 0.13692761957645416\n",
      "Max gradient at step 1581: 0.143083855509758\n",
      "Max gradient at step 1582: 0.14500820636749268\n",
      "Max gradient at step 1583: 0.13075530529022217\n",
      "Max gradient at step 1584: 0.1680067628622055\n",
      "Max gradient at step 1585: 0.08601977676153183\n",
      "Max gradient at step 1586: 0.15225936472415924\n",
      "Max gradient at step 1587: 0.1750846654176712\n",
      "Max gradient at step 1588: 0.09395775943994522\n",
      "Max gradient at step 1589: 0.32185474038124084\n",
      "Max gradient at step 1590: 0.1755533069372177\n",
      "Max gradient at step 1591: 0.16698049008846283\n",
      "Max gradient at step 1592: 0.1898229867219925\n",
      "Max gradient at step 1593: 0.45591992139816284\n",
      "Max gradient at step 1594: 0.1985119730234146\n",
      "Max gradient at step 1595: 0.21866872906684875\n",
      "Max gradient at step 1596: 0.11072107404470444\n",
      "Max gradient at step 1597: 0.24332979321479797\n",
      "Max gradient at step 1598: 0.2358102649450302\n",
      "Max gradient at step 1599: 0.08440037071704865\n",
      "Max gradient at step 1600: 0.16271436214447021\n",
      "Training loss (for one batch) at step 1600: 0.3705\n",
      "Max gradient at step 1601: 0.15733425319194794\n",
      "Max gradient at step 1602: 0.2594286799430847\n",
      "Max gradient at step 1603: 0.14451277256011963\n",
      "Max gradient at step 1604: 0.13528454303741455\n",
      "Max gradient at step 1605: 0.1848098486661911\n",
      "Max gradient at step 1606: 0.12006661295890808\n",
      "Max gradient at step 1607: 0.21288159489631653\n",
      "Max gradient at step 1608: 0.19184757769107819\n",
      "Max gradient at step 1609: 0.2067933827638626\n",
      "Max gradient at step 1610: 0.20868933200836182\n",
      "Max gradient at step 1611: 0.033028144389390945\n",
      "Max gradient at step 1612: 0.14770424365997314\n",
      "Max gradient at step 1613: 0.14756536483764648\n",
      "Max gradient at step 1614: 0.27795836329460144\n",
      "Max gradient at step 1615: 0.21919798851013184\n",
      "Max gradient at step 1616: 0.20203466713428497\n",
      "Max gradient at step 1617: 0.18362680077552795\n",
      "Max gradient at step 1618: 0.19889315962791443\n",
      "Max gradient at step 1619: 0.22724968194961548\n",
      "Max gradient at step 1620: 0.06980761885643005\n",
      "Max gradient at step 1621: 0.07388339191675186\n",
      "Max gradient at step 1622: 0.14658412337303162\n",
      "Max gradient at step 1623: 0.19131706655025482\n",
      "Max gradient at step 1624: 0.2644473910331726\n",
      "Max gradient at step 1625: 0.26636818051338196\n",
      "Max gradient at step 1626: 0.121075339615345\n",
      "Max gradient at step 1627: 0.23918397724628448\n",
      "Max gradient at step 1628: 0.19233430922031403\n",
      "Max gradient at step 1629: 0.26282545924186707\n",
      "Max gradient at step 1630: 0.18442904949188232\n",
      "Max gradient at step 1631: 0.14337942004203796\n",
      "Max gradient at step 1632: 0.2221742570400238\n",
      "Max gradient at step 1633: 0.14653068780899048\n",
      "Max gradient at step 1634: 0.13652721047401428\n",
      "Max gradient at step 1635: 0.12710897624492645\n",
      "Max gradient at step 1636: 0.2510754466056824\n",
      "Max gradient at step 1637: 0.10826173424720764\n",
      "Max gradient at step 1638: 0.29383131861686707\n",
      "Max gradient at step 1639: 0.15704387426376343\n",
      "Max gradient at step 1640: 0.10046950727701187\n",
      "Max gradient at step 1641: 0.1609179675579071\n",
      "Max gradient at step 1642: 0.05897482484579086\n",
      "Max gradient at step 1643: 0.17137199640274048\n",
      "Max gradient at step 1644: 0.1539389193058014\n",
      "Max gradient at step 1645: 0.13557180762290955\n",
      "Max gradient at step 1646: 0.1196923479437828\n",
      "Max gradient at step 1647: 0.22685648500919342\n",
      "Max gradient at step 1648: 0.17472955584526062\n",
      "Max gradient at step 1649: 0.2278498262166977\n",
      "Max gradient at step 1650: 0.15871021151542664\n",
      "Max gradient at step 1651: 0.10785610973834991\n",
      "Max gradient at step 1652: 0.11892949044704437\n",
      "Max gradient at step 1653: 0.2586197555065155\n",
      "Max gradient at step 1654: 0.12474479526281357\n",
      "Max gradient at step 1655: 0.1412716805934906\n",
      "Max gradient at step 1656: 0.08281411975622177\n",
      "Max gradient at step 1657: 0.11533444374799728\n",
      "Max gradient at step 1658: 0.3541402220726013\n",
      "Max gradient at step 1659: 0.35555997490882874\n",
      "Max gradient at step 1660: 0.32002508640289307\n",
      "Max gradient at step 1661: 0.22133049368858337\n",
      "Max gradient at step 1662: 0.23071864247322083\n",
      "Max gradient at step 1663: 0.15187875926494598\n",
      "Max gradient at step 1664: 0.18655739724636078\n",
      "Max gradient at step 1665: 0.1981908231973648\n",
      "Max gradient at step 1666: 0.149186372756958\n",
      "Max gradient at step 1667: 0.1876996010541916\n",
      "Max gradient at step 1668: 0.1965073198080063\n",
      "Max gradient at step 1669: 0.14768147468566895\n",
      "Max gradient at step 1670: 0.1712058037519455\n",
      "Max gradient at step 1671: 0.21039709448814392\n",
      "Max gradient at step 1672: 0.23243659734725952\n",
      "Max gradient at step 1673: 0.1815246343612671\n",
      "Max gradient at step 1674: 0.19257588684558868\n",
      "Max gradient at step 1675: 0.22858402132987976\n",
      "Max gradient at step 1676: 0.13605132699012756\n",
      "Max gradient at step 1677: 0.11956370621919632\n",
      "Max gradient at step 1678: 0.15214702486991882\n",
      "Max gradient at step 1679: 0.24049459397792816\n",
      "Max gradient at step 1680: 0.19313274323940277\n",
      "Max gradient at step 1681: 0.13948442041873932\n",
      "Max gradient at step 1682: 0.07489188015460968\n",
      "Max gradient at step 1683: 0.12401160597801208\n",
      "Max gradient at step 1684: 0.07986924797296524\n",
      "Max gradient at step 1685: 0.23085717856884003\n",
      "Max gradient at step 1686: 0.1634690761566162\n",
      "Max gradient at step 1687: 0.23497478663921356\n",
      "Max gradient at step 1688: 0.3377726972103119\n",
      "Max gradient at step 1689: 0.18032048642635345\n",
      "Max gradient at step 1690: 0.1517719328403473\n",
      "Max gradient at step 1691: 0.06677485257387161\n",
      "Max gradient at step 1692: 0.23292966187000275\n",
      "Max gradient at step 1693: 0.23154939711093903\n",
      "Max gradient at step 1694: 0.22965650260448456\n",
      "Max gradient at step 1695: 0.13717526197433472\n",
      "Max gradient at step 1696: 0.19997701048851013\n",
      "Max gradient at step 1697: 0.12169425189495087\n",
      "Max gradient at step 1698: 0.14277000725269318\n",
      "Max gradient at step 1699: 0.2640814781188965\n",
      "Max gradient at step 1700: 0.2044987678527832\n",
      "Training loss (for one batch) at step 1700: 0.2837\n",
      "Max gradient at step 1701: 0.12101882696151733\n",
      "Max gradient at step 1702: 0.18258562684059143\n",
      "Max gradient at step 1703: 0.18735793232917786\n",
      "Max gradient at step 1704: 0.227909117937088\n",
      "Max gradient at step 1705: 0.1314927339553833\n",
      "Max gradient at step 1706: 0.19654369354248047\n",
      "Max gradient at step 1707: 0.09361092746257782\n",
      "Max gradient at step 1708: 0.22462676465511322\n",
      "Max gradient at step 1709: 0.24961183965206146\n",
      "Max gradient at step 1710: 0.1770199090242386\n",
      "Max gradient at step 1711: 0.1174069195985794\n",
      "Max gradient at step 1712: 0.2288038283586502\n",
      "Max gradient at step 1713: 0.1294051706790924\n",
      "Max gradient at step 1714: 0.11981358379125595\n",
      "Max gradient at step 1715: 0.07981094717979431\n",
      "Max gradient at step 1716: 0.19313721358776093\n",
      "Max gradient at step 1717: 0.14105314016342163\n",
      "Max gradient at step 1718: 0.2148486226797104\n",
      "Max gradient at step 1719: 0.2883061468601227\n",
      "Max gradient at step 1720: 0.14919087290763855\n",
      "Max gradient at step 1721: 0.10710630565881729\n",
      "Max gradient at step 1722: 0.15574632585048676\n",
      "Max gradient at step 1723: 0.13170041143894196\n",
      "Max gradient at step 1724: 0.1495724469423294\n",
      "Max gradient at step 1725: 0.14279742538928986\n",
      "Max gradient at step 1726: 0.07583513855934143\n",
      "Max gradient at step 1727: 0.19287602603435516\n",
      "Max gradient at step 1728: 0.1527201235294342\n",
      "Max gradient at step 1729: 0.10449401289224625\n",
      "Max gradient at step 1730: 0.12224501371383667\n",
      "Max gradient at step 1731: 0.14848163723945618\n",
      "Max gradient at step 1732: 0.27496573328971863\n",
      "Max gradient at step 1733: 0.17875973880290985\n",
      "Max gradient at step 1734: 0.17977048456668854\n",
      "Max gradient at step 1735: 0.14192628860473633\n",
      "Max gradient at step 1736: 0.1431041806936264\n",
      "Max gradient at step 1737: 0.13877567648887634\n",
      "Max gradient at step 1738: 0.11960156261920929\n",
      "Max gradient at step 1739: 0.10248962044715881\n",
      "Max gradient at step 1740: 0.1555476188659668\n",
      "Max gradient at step 1741: 0.31160247325897217\n",
      "Max gradient at step 1742: 0.2020338475704193\n",
      "Max gradient at step 1743: 0.0666736513376236\n",
      "Max gradient at step 1744: 0.1849447637796402\n",
      "Max gradient at step 1745: 0.05826403945684433\n",
      "Max gradient at step 1746: 0.2197868376970291\n",
      "Max gradient at step 1747: 0.26430806517601013\n",
      "Max gradient at step 1748: 0.23624448478221893\n",
      "Max gradient at step 1749: 0.06851868331432343\n",
      "Max gradient at step 1750: 0.2513345181941986\n",
      "Max gradient at step 1751: 0.24988269805908203\n",
      "Max gradient at step 1752: 0.15008561313152313\n",
      "Max gradient at step 1753: 0.13455353677272797\n",
      "Max gradient at step 1754: 0.1154228001832962\n",
      "Max gradient at step 1755: 0.25368010997772217\n",
      "Max gradient at step 1756: 0.07629167288541794\n",
      "Max gradient at step 1757: 0.21115972101688385\n",
      "Max gradient at step 1758: 0.12087921053171158\n",
      "Max gradient at step 1759: 0.1625925898551941\n",
      "Max gradient at step 1760: 0.08046144992113113\n",
      "Max gradient at step 1761: 0.17883460223674774\n",
      "Max gradient at step 1762: 0.3303365409374237\n",
      "Max gradient at step 1763: 0.15044817328453064\n",
      "Max gradient at step 1764: 0.10552386939525604\n",
      "Max gradient at step 1765: 0.12725408375263214\n",
      "Max gradient at step 1766: 0.14470578730106354\n",
      "Max gradient at step 1767: 0.12736128270626068\n",
      "Max gradient at step 1768: 0.3650769293308258\n",
      "Max gradient at step 1769: 0.25183776021003723\n",
      "Max gradient at step 1770: 0.2159271091222763\n",
      "Max gradient at step 1771: 0.15434756875038147\n",
      "Max gradient at step 1772: 0.10419508069753647\n",
      "Max gradient at step 1773: 0.10927087813615799\n",
      "Max gradient at step 1774: 0.1158841922879219\n",
      "Max gradient at step 1775: 0.11565196514129639\n",
      "Max gradient at step 1776: 0.15303680300712585\n",
      "Max gradient at step 1777: 0.13581416010856628\n",
      "Max gradient at step 1778: 0.14625704288482666\n",
      "Max gradient at step 1779: 0.14380083978176117\n",
      "Max gradient at step 1780: 0.11256155371665955\n",
      "Max gradient at step 1781: 0.30135834217071533\n",
      "Max gradient at step 1782: 0.08153720200061798\n",
      "Max gradient at step 1783: 0.19723990559577942\n",
      "Max gradient at step 1784: 0.13758355379104614\n",
      "Max gradient at step 1785: 0.12628412246704102\n",
      "Max gradient at step 1786: 0.18976597487926483\n",
      "Max gradient at step 1787: 0.20388783514499664\n",
      "Max gradient at step 1788: 0.3180675208568573\n",
      "Max gradient at step 1789: 0.13570564985275269\n",
      "Max gradient at step 1790: 0.13205228745937347\n",
      "Max gradient at step 1791: 0.10382804274559021\n",
      "Max gradient at step 1792: 0.09282630681991577\n",
      "Max gradient at step 1793: 0.13549621403217316\n",
      "Max gradient at step 1794: 0.18715649843215942\n",
      "Max gradient at step 1795: 0.1096804141998291\n",
      "Max gradient at step 1796: 0.12190020084381104\n",
      "Max gradient at step 1797: 0.12619717419147491\n",
      "Max gradient at step 1798: 0.14881481230258942\n",
      "Max gradient at step 1799: 0.09020281583070755\n",
      "Max gradient at step 1800: 0.17750464379787445\n",
      "Training loss (for one batch) at step 1800: 0.1821\n",
      "Max gradient at step 1801: 0.186078742146492\n",
      "Max gradient at step 1802: 0.13853606581687927\n",
      "Max gradient at step 1803: 0.12125542014837265\n",
      "Max gradient at step 1804: 0.08005918562412262\n",
      "Max gradient at step 1805: 0.06021494045853615\n",
      "Max gradient at step 1806: 0.12425840646028519\n",
      "Max gradient at step 1807: 0.12827274203300476\n",
      "Max gradient at step 1808: 0.09966528415679932\n",
      "Max gradient at step 1809: 0.1955031156539917\n",
      "Max gradient at step 1810: 0.1292192041873932\n",
      "Max gradient at step 1811: 0.08335541188716888\n",
      "Max gradient at step 1812: 0.2351534515619278\n",
      "Max gradient at step 1813: 0.04002068564295769\n",
      "Max gradient at step 1814: 0.25546708703041077\n",
      "Max gradient at step 1815: 0.08781026303768158\n",
      "Max gradient at step 1816: 0.1827772706747055\n",
      "Max gradient at step 1817: 0.17781707644462585\n",
      "Max gradient at step 1818: 0.13525040447711945\n",
      "Max gradient at step 1819: 0.1175348088145256\n",
      "Max gradient at step 1820: 0.12209773063659668\n",
      "Max gradient at step 1821: 0.13620588183403015\n",
      "Max gradient at step 1822: 0.09641175717115402\n",
      "Max gradient at step 1823: 0.11262195557355881\n",
      "Max gradient at step 1824: 0.185085728764534\n",
      "Max gradient at step 1825: 0.1891801804304123\n",
      "Max gradient at step 1826: 0.178569957613945\n",
      "Max gradient at step 1827: 0.17829668521881104\n",
      "Max gradient at step 1828: 0.11173903942108154\n",
      "Max gradient at step 1829: 0.20496796071529388\n",
      "Max gradient at step 1830: 0.16409499943256378\n",
      "Max gradient at step 1831: 0.14483408629894257\n",
      "Max gradient at step 1832: 0.0897010862827301\n",
      "Max gradient at step 1833: 0.1598232388496399\n",
      "Max gradient at step 1834: 0.09779221564531326\n",
      "Max gradient at step 1835: 0.12844350934028625\n",
      "Max gradient at step 1836: 0.09437587857246399\n",
      "Max gradient at step 1837: 0.28097081184387207\n",
      "Max gradient at step 1838: 0.07135914266109467\n",
      "Max gradient at step 1839: 0.06239521875977516\n",
      "Max gradient at step 1840: 0.17756396532058716\n",
      "Max gradient at step 1841: 0.13476048409938812\n",
      "Max gradient at step 1842: 0.11448048055171967\n",
      "Max gradient at step 1843: 0.025618791580200195\n",
      "Max gradient at step 1844: 0.2548978328704834\n",
      "Max gradient at step 1845: 0.1606650948524475\n",
      "Max gradient at step 1846: 0.22308765351772308\n",
      "Max gradient at step 1847: 0.22164814174175262\n",
      "Max gradient at step 1848: 0.12754623591899872\n",
      "Max gradient at step 1849: 0.16677334904670715\n",
      "Max gradient at step 1850: 0.15482576191425323\n",
      "Max gradient at step 1851: 0.11793671548366547\n",
      "Max gradient at step 1852: 0.1612868458032608\n",
      "Max gradient at step 1853: 0.1268242448568344\n",
      "Max gradient at step 1854: 0.2575489282608032\n",
      "Max gradient at step 1855: 0.20516172051429749\n",
      "Max gradient at step 1856: 0.26954180002212524\n",
      "Max gradient at step 1857: 0.18163242936134338\n",
      "Max gradient at step 1858: 0.1795041859149933\n",
      "Max gradient at step 1859: 0.1450434923171997\n",
      "Max gradient at step 1860: 0.34933969378471375\n",
      "Max gradient at step 1861: 0.1457752287387848\n",
      "Max gradient at step 1862: 0.08735687285661697\n",
      "Max gradient at step 1863: 0.12590642273426056\n",
      "Max gradient at step 1864: 0.20444002747535706\n",
      "Max gradient at step 1865: 0.10799470543861389\n",
      "Max gradient at step 1866: 0.025065088644623756\n",
      "Max gradient at step 1867: 0.2685289978981018\n",
      "Max gradient at step 1868: 0.15751466155052185\n",
      "Max gradient at step 1869: 0.10743021219968796\n",
      "Max gradient at step 1870: 0.12542711198329926\n",
      "Max gradient at step 1871: 0.2152751237154007\n",
      "Max gradient at step 1872: 0.21706368029117584\n",
      "Max gradient at step 1873: 0.06242360174655914\n",
      "Max gradient at step 1874: 0.12535127997398376\n",
      "Epoch 4/4\n",
      "Max gradient at step 0: 0.17118613421916962\n",
      "Training loss (for one batch) at step 0: 0.3884\n",
      "Max gradient at step 1: 0.156911700963974\n",
      "Max gradient at step 2: 0.18875710666179657\n",
      "Max gradient at step 3: 0.18128228187561035\n",
      "Max gradient at step 4: 0.11677198857069016\n",
      "Max gradient at step 5: 0.1262381374835968\n",
      "Max gradient at step 6: 0.14226080477237701\n",
      "Max gradient at step 7: 0.17384664714336395\n",
      "Max gradient at step 8: 0.30824241042137146\n",
      "Max gradient at step 9: 0.131969153881073\n",
      "Max gradient at step 10: 0.19237668812274933\n",
      "Max gradient at step 11: 0.12098421901464462\n",
      "Max gradient at step 12: 0.3461194336414337\n",
      "Max gradient at step 13: 0.24694564938545227\n",
      "Max gradient at step 14: 0.1549696922302246\n",
      "Max gradient at step 15: 0.12695297598838806\n",
      "Max gradient at step 16: 0.1805637776851654\n",
      "Max gradient at step 17: 0.14020799100399017\n",
      "Max gradient at step 18: 0.12361419200897217\n",
      "Max gradient at step 19: 0.22079020738601685\n",
      "Max gradient at step 20: 0.12377183139324188\n",
      "Max gradient at step 21: 0.18351627886295319\n",
      "Max gradient at step 22: 0.24414587020874023\n",
      "Max gradient at step 23: 0.15050120651721954\n",
      "Max gradient at step 24: 0.1019921749830246\n",
      "Max gradient at step 25: 0.14538708329200745\n",
      "Max gradient at step 26: 0.19325096905231476\n",
      "Max gradient at step 27: 0.1848616749048233\n",
      "Max gradient at step 28: 0.17856498062610626\n",
      "Max gradient at step 29: 0.22084589302539825\n",
      "Max gradient at step 30: 0.23313941061496735\n",
      "Max gradient at step 31: 0.25211969017982483\n",
      "Max gradient at step 32: 0.19442348182201385\n",
      "Max gradient at step 33: 0.12051132321357727\n",
      "Max gradient at step 34: 0.15609413385391235\n",
      "Max gradient at step 35: 0.24528345465660095\n",
      "Max gradient at step 36: 0.14047104120254517\n",
      "Max gradient at step 37: 0.15823478996753693\n",
      "Max gradient at step 38: 0.19154004752635956\n",
      "Max gradient at step 39: 0.13328690826892853\n",
      "Max gradient at step 40: 0.14391957223415375\n",
      "Max gradient at step 41: 0.08338304609060287\n",
      "Max gradient at step 42: 0.24046169221401215\n",
      "Max gradient at step 43: 0.138871431350708\n",
      "Max gradient at step 44: 0.16185913980007172\n",
      "Max gradient at step 45: 0.1363186091184616\n",
      "Max gradient at step 46: 0.1376047432422638\n",
      "Max gradient at step 47: 0.10631448030471802\n",
      "Max gradient at step 48: 0.10814208537340164\n",
      "Max gradient at step 49: 0.14620503783226013\n",
      "Max gradient at step 50: 0.14983698725700378\n",
      "Max gradient at step 51: 0.0986151173710823\n",
      "Max gradient at step 52: 0.14009042084217072\n",
      "Max gradient at step 53: 0.1644144505262375\n",
      "Max gradient at step 54: 0.10398253798484802\n",
      "Max gradient at step 55: 0.15811747312545776\n",
      "Max gradient at step 56: 0.2084522545337677\n",
      "Max gradient at step 57: 0.16562938690185547\n",
      "Max gradient at step 58: 0.27915191650390625\n",
      "Max gradient at step 59: 0.12851011753082275\n",
      "Max gradient at step 60: 0.12273906171321869\n",
      "Max gradient at step 61: 0.21312344074249268\n",
      "Max gradient at step 62: 0.12417980283498764\n",
      "Max gradient at step 63: 0.1774510145187378\n",
      "Max gradient at step 64: 0.10637691617012024\n",
      "Max gradient at step 65: 0.08316289633512497\n",
      "Max gradient at step 66: 0.09651369601488113\n",
      "Max gradient at step 67: 0.13406254351139069\n",
      "Max gradient at step 68: 0.049902644008398056\n",
      "Max gradient at step 69: 0.11077523231506348\n",
      "Max gradient at step 70: 0.13666005432605743\n",
      "Max gradient at step 71: 0.04592267796397209\n",
      "Max gradient at step 72: 0.25005990266799927\n",
      "Max gradient at step 73: 0.1139831393957138\n",
      "Max gradient at step 74: 0.1187625601887703\n",
      "Max gradient at step 75: 0.14931614696979523\n",
      "Max gradient at step 76: 0.26746830344200134\n",
      "Max gradient at step 77: 0.12257307767868042\n",
      "Max gradient at step 78: 0.22409945726394653\n",
      "Max gradient at step 79: 0.2542197108268738\n",
      "Max gradient at step 80: 0.14464838802814484\n",
      "Max gradient at step 81: 0.14679951965808868\n",
      "Max gradient at step 82: 0.2074298858642578\n",
      "Max gradient at step 83: 0.2514791786670685\n",
      "Max gradient at step 84: 0.11282144486904144\n",
      "Max gradient at step 85: 0.18676075339317322\n",
      "Max gradient at step 86: 0.16100898385047913\n",
      "Max gradient at step 87: 0.21070441603660583\n",
      "Max gradient at step 88: 0.22749988734722137\n",
      "Max gradient at step 89: 0.0413898266851902\n",
      "Max gradient at step 90: 0.2500551640987396\n",
      "Max gradient at step 91: 0.09974900633096695\n",
      "Max gradient at step 92: 0.19806717336177826\n",
      "Max gradient at step 93: 0.28420984745025635\n",
      "Max gradient at step 94: 0.15597787499427795\n",
      "Max gradient at step 95: 0.06083644926548004\n",
      "Max gradient at step 96: 0.06318806111812592\n",
      "Max gradient at step 97: 0.13436150550842285\n",
      "Max gradient at step 98: 0.12206901609897614\n",
      "Max gradient at step 99: 0.23987165093421936\n",
      "Max gradient at step 100: 0.1631249338388443\n",
      "Training loss (for one batch) at step 100: 0.2958\n",
      "Max gradient at step 101: 0.13727742433547974\n",
      "Max gradient at step 102: 0.14092057943344116\n",
      "Max gradient at step 103: 0.17724104225635529\n",
      "Max gradient at step 104: 0.22059068083763123\n",
      "Max gradient at step 105: 0.2341068685054779\n",
      "Max gradient at step 106: 0.16233213245868683\n",
      "Max gradient at step 107: 0.18358539044857025\n",
      "Max gradient at step 108: 0.11844328790903091\n",
      "Max gradient at step 109: 0.13271887600421906\n",
      "Max gradient at step 110: 0.18199603259563446\n",
      "Max gradient at step 111: 0.16363205015659332\n",
      "Max gradient at step 112: 0.14186978340148926\n",
      "Max gradient at step 113: 0.0603136271238327\n",
      "Max gradient at step 114: 0.15002672374248505\n",
      "Max gradient at step 115: 0.1517540067434311\n",
      "Max gradient at step 116: 0.12097632884979248\n",
      "Max gradient at step 117: 0.10649403184652328\n",
      "Max gradient at step 118: 0.2178489863872528\n",
      "Max gradient at step 119: 0.11860952526330948\n",
      "Max gradient at step 120: 0.08393090218305588\n",
      "Max gradient at step 121: 0.15992754697799683\n",
      "Max gradient at step 122: 0.12536248564720154\n",
      "Max gradient at step 123: 0.24187512695789337\n",
      "Max gradient at step 124: 0.1327614188194275\n",
      "Max gradient at step 125: 0.1262490451335907\n",
      "Max gradient at step 126: 0.17463962733745575\n",
      "Max gradient at step 127: 0.12405718117952347\n",
      "Max gradient at step 128: 0.15403562784194946\n",
      "Max gradient at step 129: 0.15666848421096802\n",
      "Max gradient at step 130: 0.20892417430877686\n",
      "Max gradient at step 131: 0.14046820998191833\n",
      "Max gradient at step 132: 0.09806868433952332\n",
      "Max gradient at step 133: 0.13383308053016663\n",
      "Max gradient at step 134: 0.3121488094329834\n",
      "Max gradient at step 135: 0.21097156405448914\n",
      "Max gradient at step 136: 0.14144420623779297\n",
      "Max gradient at step 137: 0.14261536300182343\n",
      "Max gradient at step 138: 0.17442931234836578\n",
      "Max gradient at step 139: 0.03062591888010502\n",
      "Max gradient at step 140: 0.15242506563663483\n",
      "Max gradient at step 141: 0.1295546144247055\n",
      "Max gradient at step 142: 0.08652637898921967\n",
      "Max gradient at step 143: 0.23053376376628876\n",
      "Max gradient at step 144: 0.20786318182945251\n",
      "Max gradient at step 145: 0.07735367864370346\n",
      "Max gradient at step 146: 0.08368022739887238\n",
      "Max gradient at step 147: 0.18735074996948242\n",
      "Max gradient at step 148: 0.15432508289813995\n",
      "Max gradient at step 149: 0.3068063259124756\n",
      "Max gradient at step 150: 0.1435055136680603\n",
      "Max gradient at step 151: 0.28262367844581604\n",
      "Max gradient at step 152: 0.09202192723751068\n",
      "Max gradient at step 153: 0.10518019646406174\n",
      "Max gradient at step 154: 0.17489466071128845\n",
      "Max gradient at step 155: 0.18415553867816925\n",
      "Max gradient at step 156: 0.19966721534729004\n",
      "Max gradient at step 157: 0.241178497672081\n",
      "Max gradient at step 158: 0.30536511540412903\n",
      "Max gradient at step 159: 0.13138534128665924\n",
      "Max gradient at step 160: 0.13765347003936768\n",
      "Max gradient at step 161: 0.22497527301311493\n",
      "Max gradient at step 162: 0.20633965730667114\n",
      "Max gradient at step 163: 0.04038650169968605\n",
      "Max gradient at step 164: 0.17870280146598816\n",
      "Max gradient at step 165: 0.13906919956207275\n",
      "Max gradient at step 166: 0.14038582146167755\n",
      "Max gradient at step 167: 0.13565847277641296\n",
      "Max gradient at step 168: 0.20036423206329346\n",
      "Max gradient at step 169: 0.18332353234291077\n",
      "Max gradient at step 170: 0.21153709292411804\n",
      "Max gradient at step 171: 0.17220062017440796\n",
      "Max gradient at step 172: 0.15434983372688293\n",
      "Max gradient at step 173: 0.12548227608203888\n",
      "Max gradient at step 174: 0.13408620655536652\n",
      "Max gradient at step 175: 0.08394208550453186\n",
      "Max gradient at step 176: 0.1248001977801323\n",
      "Max gradient at step 177: 0.2334437072277069\n",
      "Max gradient at step 178: 0.2760716378688812\n",
      "Max gradient at step 179: 0.15951965749263763\n",
      "Max gradient at step 180: 0.180652916431427\n",
      "Max gradient at step 181: 0.11730888485908508\n",
      "Max gradient at step 182: 0.15670372545719147\n",
      "Max gradient at step 183: 0.13858336210250854\n",
      "Max gradient at step 184: 0.13550491631031036\n",
      "Max gradient at step 185: 0.2843092083930969\n",
      "Max gradient at step 186: 0.16762173175811768\n",
      "Max gradient at step 187: 0.09310010075569153\n",
      "Max gradient at step 188: 0.23433293402194977\n",
      "Max gradient at step 189: 0.13668642938137054\n",
      "Max gradient at step 190: 0.09093553572893143\n",
      "Max gradient at step 191: 0.23576922714710236\n",
      "Max gradient at step 192: 0.10362566262483597\n",
      "Max gradient at step 193: 0.23552855849266052\n",
      "Max gradient at step 194: 0.16515100002288818\n",
      "Max gradient at step 195: 0.14773990213871002\n",
      "Max gradient at step 196: 0.2058340162038803\n",
      "Max gradient at step 197: 0.1768432855606079\n",
      "Max gradient at step 198: 0.13973717391490936\n",
      "Max gradient at step 199: 0.2309499830007553\n",
      "Max gradient at step 200: 0.1850135624408722\n",
      "Training loss (for one batch) at step 200: 0.4329\n",
      "Max gradient at step 201: 0.18882527947425842\n",
      "Max gradient at step 202: 0.12494225054979324\n",
      "Max gradient at step 203: 0.06355675309896469\n",
      "Max gradient at step 204: 0.22296269237995148\n",
      "Max gradient at step 205: 0.12735727429389954\n",
      "Max gradient at step 206: 0.19815406203269958\n",
      "Max gradient at step 207: 0.0903412252664566\n",
      "Max gradient at step 208: 0.14818212389945984\n",
      "Max gradient at step 209: 0.14690867066383362\n",
      "Max gradient at step 210: 0.10979966074228287\n",
      "Max gradient at step 211: 0.06394398212432861\n",
      "Max gradient at step 212: 0.14785656332969666\n",
      "Max gradient at step 213: 0.19773109257221222\n",
      "Max gradient at step 214: 0.10348770022392273\n",
      "Max gradient at step 215: 0.13233357667922974\n",
      "Max gradient at step 216: 0.11715950071811676\n",
      "Max gradient at step 217: 0.11288733035326004\n",
      "Max gradient at step 218: 0.2078350931406021\n",
      "Max gradient at step 219: 0.15124662220478058\n",
      "Max gradient at step 220: 0.173345685005188\n",
      "Max gradient at step 221: 0.16271989047527313\n",
      "Max gradient at step 222: 0.19672836363315582\n",
      "Max gradient at step 223: 0.20885343849658966\n",
      "Max gradient at step 224: 0.26337137818336487\n",
      "Max gradient at step 225: 0.19298185408115387\n",
      "Max gradient at step 226: 0.1500152349472046\n",
      "Max gradient at step 227: 0.14985539019107819\n",
      "Max gradient at step 228: 0.21596777439117432\n",
      "Max gradient at step 229: 0.1464766412973404\n",
      "Max gradient at step 230: 0.2805302143096924\n",
      "Max gradient at step 231: 0.11977151781320572\n",
      "Max gradient at step 232: 0.17161354422569275\n",
      "Max gradient at step 233: 0.22500069439411163\n",
      "Max gradient at step 234: 0.12192189693450928\n",
      "Max gradient at step 235: 0.14611607789993286\n",
      "Max gradient at step 236: 0.09879158437252045\n",
      "Max gradient at step 237: 0.22254939377307892\n",
      "Max gradient at step 238: 0.11246950179338455\n",
      "Max gradient at step 239: 0.12562841176986694\n",
      "Max gradient at step 240: 0.1649324893951416\n",
      "Max gradient at step 241: 0.24232229590415955\n",
      "Max gradient at step 242: 0.21637307107448578\n",
      "Max gradient at step 243: 0.22653250396251678\n",
      "Max gradient at step 244: 0.1756773740053177\n",
      "Max gradient at step 245: 0.27245911955833435\n",
      "Max gradient at step 246: 0.21658456325531006\n",
      "Max gradient at step 247: 0.08361058682203293\n",
      "Max gradient at step 248: 0.23879757523536682\n",
      "Max gradient at step 249: 0.2463051974773407\n",
      "Max gradient at step 250: 0.3243621289730072\n",
      "Max gradient at step 251: 0.17313449084758759\n",
      "Max gradient at step 252: 0.1559433788061142\n",
      "Max gradient at step 253: 0.33057138323783875\n",
      "Max gradient at step 254: 0.37561649084091187\n",
      "Max gradient at step 255: 0.2067844718694687\n",
      "Max gradient at step 256: 0.16216439008712769\n",
      "Max gradient at step 257: 0.22620521485805511\n",
      "Max gradient at step 258: 0.13748422265052795\n",
      "Max gradient at step 259: 0.1708860844373703\n",
      "Max gradient at step 260: 0.1544412076473236\n",
      "Max gradient at step 261: 0.20893695950508118\n",
      "Max gradient at step 262: 0.2896280288696289\n",
      "Max gradient at step 263: 0.1417873501777649\n",
      "Max gradient at step 264: 0.14473646879196167\n",
      "Max gradient at step 265: 0.1418362259864807\n",
      "Max gradient at step 266: 0.2112157940864563\n",
      "Max gradient at step 267: 0.289628803730011\n",
      "Max gradient at step 268: 0.1555824726819992\n",
      "Max gradient at step 269: 0.10734176635742188\n",
      "Max gradient at step 270: 0.11396379768848419\n",
      "Max gradient at step 271: 0.11523625254631042\n",
      "Max gradient at step 272: 0.1745269000530243\n",
      "Max gradient at step 273: 0.15342874825000763\n",
      "Max gradient at step 274: 0.18623526394367218\n",
      "Max gradient at step 275: 0.3205430805683136\n",
      "Max gradient at step 276: 0.10640080273151398\n",
      "Max gradient at step 277: 0.09634514898061752\n",
      "Max gradient at step 278: 0.12580208480358124\n",
      "Max gradient at step 279: 0.2077835500240326\n",
      "Max gradient at step 280: 0.30698010325431824\n",
      "Max gradient at step 281: 0.10920526832342148\n",
      "Max gradient at step 282: 0.23173978924751282\n",
      "Max gradient at step 283: 0.14583104848861694\n",
      "Max gradient at step 284: 0.233005091547966\n",
      "Max gradient at step 285: 0.17409247159957886\n",
      "Max gradient at step 286: 0.15796847641468048\n",
      "Max gradient at step 287: 0.12237647920846939\n",
      "Max gradient at step 288: 0.2577568292617798\n",
      "Max gradient at step 289: 0.20164217054843903\n",
      "Max gradient at step 290: 0.18485836684703827\n",
      "Max gradient at step 291: 0.14580190181732178\n",
      "Max gradient at step 292: 0.14010916650295258\n",
      "Max gradient at step 293: 0.1680162400007248\n",
      "Max gradient at step 294: 0.2205055207014084\n",
      "Max gradient at step 295: 0.20190876722335815\n",
      "Max gradient at step 296: 0.19548895955085754\n",
      "Max gradient at step 297: 0.16772513091564178\n",
      "Max gradient at step 298: 0.16340357065200806\n",
      "Max gradient at step 299: 0.070318803191185\n",
      "Max gradient at step 300: 0.18654818832874298\n",
      "Training loss (for one batch) at step 300: 0.2387\n",
      "Max gradient at step 301: 0.14993587136268616\n",
      "Max gradient at step 302: 0.17133323848247528\n",
      "Max gradient at step 303: 0.1457739919424057\n",
      "Max gradient at step 304: 0.5288994908332825\n",
      "Max gradient at step 305: 0.31358689069747925\n",
      "Max gradient at step 306: 0.12450617551803589\n",
      "Max gradient at step 307: 0.15630508959293365\n",
      "Max gradient at step 308: 0.1358502060174942\n",
      "Max gradient at step 309: 0.12355463206768036\n",
      "Max gradient at step 310: 0.21219050884246826\n",
      "Max gradient at step 311: 0.16095104813575745\n",
      "Max gradient at step 312: 0.13405172526836395\n",
      "Max gradient at step 313: 0.12336587905883789\n",
      "Max gradient at step 314: 0.1712643802165985\n",
      "Max gradient at step 315: 0.10680053383111954\n",
      "Max gradient at step 316: 0.10885290056467056\n",
      "Max gradient at step 317: 0.10102459043264389\n",
      "Max gradient at step 318: 0.13369522988796234\n",
      "Max gradient at step 319: 0.21498145163059235\n",
      "Max gradient at step 320: 0.1694219708442688\n",
      "Max gradient at step 321: 0.1416286677122116\n",
      "Max gradient at step 322: 0.08795804530382156\n",
      "Max gradient at step 323: 0.05797409638762474\n",
      "Max gradient at step 324: 0.15001364052295685\n",
      "Max gradient at step 325: 0.10383980721235275\n",
      "Max gradient at step 326: 0.1806483119726181\n",
      "Max gradient at step 327: 0.10467741638422012\n",
      "Max gradient at step 328: 0.1865711361169815\n",
      "Max gradient at step 329: 0.14125855267047882\n",
      "Max gradient at step 330: 0.14929404854774475\n",
      "Max gradient at step 331: 0.12720534205436707\n",
      "Max gradient at step 332: 0.16283956170082092\n",
      "Max gradient at step 333: 0.14176501333713531\n",
      "Max gradient at step 334: 0.19498509168624878\n",
      "Max gradient at step 335: 0.1650722622871399\n",
      "Max gradient at step 336: 0.15601927042007446\n",
      "Max gradient at step 337: 0.16654574871063232\n",
      "Max gradient at step 338: 0.21677903831005096\n",
      "Max gradient at step 339: 0.15187010169029236\n",
      "Max gradient at step 340: 0.13493844866752625\n",
      "Max gradient at step 341: 0.3193504512310028\n",
      "Max gradient at step 342: 0.14395438134670258\n",
      "Max gradient at step 343: 0.1701667159795761\n",
      "Max gradient at step 344: 0.15747934579849243\n",
      "Max gradient at step 345: 0.13483574986457825\n",
      "Max gradient at step 346: 0.26609912514686584\n",
      "Max gradient at step 347: 0.1640390306711197\n",
      "Max gradient at step 348: 0.15855711698532104\n",
      "Max gradient at step 349: 0.14971230924129486\n",
      "Max gradient at step 350: 0.15814536809921265\n",
      "Max gradient at step 351: 0.04843645915389061\n",
      "Max gradient at step 352: 0.15255311131477356\n",
      "Max gradient at step 353: 0.09183871001005173\n",
      "Max gradient at step 354: 0.15512894093990326\n",
      "Max gradient at step 355: 0.16190476715564728\n",
      "Max gradient at step 356: 0.13232390582561493\n",
      "Max gradient at step 357: 0.17489801347255707\n",
      "Max gradient at step 358: 0.17052122950553894\n",
      "Max gradient at step 359: 0.27135157585144043\n",
      "Max gradient at step 360: 0.21130257844924927\n",
      "Max gradient at step 361: 0.1857079267501831\n",
      "Max gradient at step 362: 0.04215143248438835\n",
      "Max gradient at step 363: 0.28786465525627136\n",
      "Max gradient at step 364: 0.1206742376089096\n",
      "Max gradient at step 365: 0.3581090569496155\n",
      "Max gradient at step 366: 0.24013471603393555\n",
      "Max gradient at step 367: 0.1352619230747223\n",
      "Max gradient at step 368: 0.28042203187942505\n",
      "Max gradient at step 369: 0.10968846827745438\n",
      "Max gradient at step 370: 0.1972772181034088\n",
      "Max gradient at step 371: 0.2653122842311859\n",
      "Max gradient at step 372: 0.2429608553647995\n",
      "Max gradient at step 373: 0.2904476523399353\n",
      "Max gradient at step 374: 0.2704799473285675\n",
      "Max gradient at step 375: 0.14934390783309937\n",
      "Max gradient at step 376: 0.12183616310358047\n",
      "Max gradient at step 377: 0.13666608929634094\n",
      "Max gradient at step 378: 0.14085125923156738\n",
      "Max gradient at step 379: 0.0912756696343422\n",
      "Max gradient at step 380: 0.25709056854248047\n",
      "Max gradient at step 381: 0.2948106527328491\n",
      "Max gradient at step 382: 0.19440406560897827\n",
      "Max gradient at step 383: 0.33597397804260254\n",
      "Max gradient at step 384: 0.04468928650021553\n",
      "Max gradient at step 385: 0.11916530132293701\n",
      "Max gradient at step 386: 0.1894177943468094\n",
      "Max gradient at step 387: 0.24163365364074707\n",
      "Max gradient at step 388: 0.06221543997526169\n",
      "Max gradient at step 389: 0.10506895184516907\n",
      "Max gradient at step 390: 0.25667890906333923\n",
      "Max gradient at step 391: 0.24103260040283203\n",
      "Max gradient at step 392: 0.135079026222229\n",
      "Max gradient at step 393: 0.22803333401679993\n",
      "Max gradient at step 394: 0.09345562756061554\n",
      "Max gradient at step 395: 0.1562395989894867\n",
      "Max gradient at step 396: 0.22123044729232788\n",
      "Max gradient at step 397: 0.09785912185907364\n",
      "Max gradient at step 398: 0.2133409082889557\n",
      "Max gradient at step 399: 0.1861218810081482\n",
      "Max gradient at step 400: 0.3500171899795532\n",
      "Training loss (for one batch) at step 400: 0.2232\n",
      "Max gradient at step 401: 0.10676995664834976\n",
      "Max gradient at step 402: 0.291851669549942\n",
      "Max gradient at step 403: 0.23360174894332886\n",
      "Max gradient at step 404: 0.2136872112751007\n",
      "Max gradient at step 405: 0.08226944506168365\n",
      "Max gradient at step 406: 0.27538326382637024\n",
      "Max gradient at step 407: 0.26483651995658875\n",
      "Max gradient at step 408: 0.38733363151550293\n",
      "Max gradient at step 409: 0.2204943746328354\n",
      "Max gradient at step 410: 0.2011929154396057\n",
      "Max gradient at step 411: 0.10316397249698639\n",
      "Max gradient at step 412: 0.14313913881778717\n",
      "Max gradient at step 413: 0.2300904542207718\n",
      "Max gradient at step 414: 0.2312956005334854\n",
      "Max gradient at step 415: 0.24714848399162292\n",
      "Max gradient at step 416: 0.13780716061592102\n",
      "Max gradient at step 417: 0.1354963630437851\n",
      "Max gradient at step 418: 0.18313635885715485\n",
      "Max gradient at step 419: 0.11085382848978043\n",
      "Max gradient at step 420: 0.20847322046756744\n",
      "Max gradient at step 421: 0.16971580684185028\n",
      "Max gradient at step 422: 0.21566249430179596\n",
      "Max gradient at step 423: 0.20381426811218262\n",
      "Max gradient at step 424: 0.08917760848999023\n",
      "Max gradient at step 425: 0.12275703996419907\n",
      "Max gradient at step 426: 0.2657788395881653\n",
      "Max gradient at step 427: 0.2528556287288666\n",
      "Max gradient at step 428: 0.16121192276477814\n",
      "Max gradient at step 429: 0.13770009577274323\n",
      "Max gradient at step 430: 0.23949187994003296\n",
      "Max gradient at step 431: 0.1795944720506668\n",
      "Max gradient at step 432: 0.27161484956741333\n",
      "Max gradient at step 433: 0.19512304663658142\n",
      "Max gradient at step 434: 0.16116584837436676\n",
      "Max gradient at step 435: 0.13376390933990479\n",
      "Max gradient at step 436: 0.157592311501503\n",
      "Max gradient at step 437: 0.3617553412914276\n",
      "Max gradient at step 438: 0.22598490118980408\n",
      "Max gradient at step 439: 0.32536032795906067\n",
      "Max gradient at step 440: 0.14732176065444946\n",
      "Max gradient at step 441: 0.29614076018333435\n",
      "Max gradient at step 442: 0.24689514935016632\n",
      "Max gradient at step 443: 0.37236618995666504\n",
      "Max gradient at step 444: 0.2541722059249878\n",
      "Max gradient at step 445: 0.12321765720844269\n",
      "Max gradient at step 446: 0.12686683237552643\n",
      "Max gradient at step 447: 0.12400966137647629\n",
      "Max gradient at step 448: 0.12277802079916\n",
      "Max gradient at step 449: 0.20711909234523773\n",
      "Max gradient at step 450: 0.23227708041667938\n",
      "Max gradient at step 451: 0.14593391120433807\n",
      "Max gradient at step 452: 0.24629995226860046\n",
      "Max gradient at step 453: 0.15831336379051208\n",
      "Max gradient at step 454: 0.1855970025062561\n",
      "Max gradient at step 455: 0.11541009694337845\n",
      "Max gradient at step 456: 0.13632605969905853\n",
      "Max gradient at step 457: 0.28046277165412903\n",
      "Max gradient at step 458: 0.16945864260196686\n",
      "Max gradient at step 459: 0.2376966029405594\n",
      "Max gradient at step 460: 0.1520073115825653\n",
      "Max gradient at step 461: 0.15219105780124664\n",
      "Max gradient at step 462: 0.23615695536136627\n",
      "Max gradient at step 463: 0.24116899073123932\n",
      "Max gradient at step 464: 0.10783413797616959\n",
      "Max gradient at step 465: 0.2054152637720108\n",
      "Max gradient at step 466: 0.13797162473201752\n",
      "Max gradient at step 467: 0.2145071178674698\n",
      "Max gradient at step 468: 0.09982777386903763\n",
      "Max gradient at step 469: 0.3501124978065491\n",
      "Max gradient at step 470: 0.1784871518611908\n",
      "Max gradient at step 471: 0.12626248598098755\n",
      "Max gradient at step 472: 0.1644917130470276\n",
      "Max gradient at step 473: 0.10989020764827728\n",
      "Max gradient at step 474: 0.11987470090389252\n",
      "Max gradient at step 475: 0.1841503232717514\n",
      "Max gradient at step 476: 0.11356525123119354\n",
      "Max gradient at step 477: 0.10883725434541702\n",
      "Max gradient at step 478: 0.12946490943431854\n",
      "Max gradient at step 479: 0.2657707929611206\n",
      "Max gradient at step 480: 0.1758277416229248\n",
      "Max gradient at step 481: 0.31597664952278137\n",
      "Max gradient at step 482: 0.20148193836212158\n",
      "Max gradient at step 483: 0.3375658690929413\n",
      "Max gradient at step 484: 0.12665829062461853\n",
      "Max gradient at step 485: 0.11970718204975128\n",
      "Max gradient at step 486: 0.22378814220428467\n",
      "Max gradient at step 487: 0.21408024430274963\n",
      "Max gradient at step 488: 0.2566278576850891\n",
      "Max gradient at step 489: 0.1375703513622284\n",
      "Max gradient at step 490: 0.19525651633739471\n",
      "Max gradient at step 491: 0.2575613558292389\n",
      "Max gradient at step 492: 0.2004719078540802\n",
      "Max gradient at step 493: 0.24045006930828094\n",
      "Max gradient at step 494: 0.07243705540895462\n",
      "Max gradient at step 495: 0.23308894038200378\n",
      "Max gradient at step 496: 0.17695091664791107\n",
      "Max gradient at step 497: 0.31237971782684326\n",
      "Max gradient at step 498: 0.16577130556106567\n",
      "Max gradient at step 499: 0.15524181723594666\n",
      "Max gradient at step 500: 0.12315686047077179\n",
      "Training loss (for one batch) at step 500: 0.1592\n",
      "Max gradient at step 501: 0.1747623234987259\n",
      "Max gradient at step 502: 0.08098749816417694\n",
      "Max gradient at step 503: 0.10838323831558228\n",
      "Max gradient at step 504: 0.21782013773918152\n",
      "Max gradient at step 505: 0.16732418537139893\n",
      "Max gradient at step 506: 0.26163873076438904\n",
      "Max gradient at step 507: 0.11111174523830414\n",
      "Max gradient at step 508: 0.23509961366653442\n",
      "Max gradient at step 509: 0.25067803263664246\n",
      "Max gradient at step 510: 0.1466526985168457\n",
      "Max gradient at step 511: 0.23595932126045227\n",
      "Max gradient at step 512: 0.35268649458885193\n",
      "Max gradient at step 513: 0.2222052365541458\n",
      "Max gradient at step 514: 0.2766067385673523\n",
      "Max gradient at step 515: 0.29792869091033936\n",
      "Max gradient at step 516: 0.13910606503486633\n",
      "Max gradient at step 517: 0.156408429145813\n",
      "Max gradient at step 518: 0.13544568419456482\n",
      "Max gradient at step 519: 0.12380944937467575\n",
      "Max gradient at step 520: 0.11937300115823746\n",
      "Max gradient at step 521: 0.1636514514684677\n",
      "Max gradient at step 522: 0.289798766374588\n",
      "Max gradient at step 523: 0.18171103298664093\n",
      "Max gradient at step 524: 0.16220442950725555\n",
      "Max gradient at step 525: 0.14287151396274567\n",
      "Max gradient at step 526: 0.1840195506811142\n",
      "Max gradient at step 527: 0.1547292023897171\n",
      "Max gradient at step 528: 0.1094176396727562\n",
      "Max gradient at step 529: 0.31045112013816833\n",
      "Max gradient at step 530: 0.2584483325481415\n",
      "Max gradient at step 531: 0.2770596742630005\n",
      "Max gradient at step 532: 0.19291013479232788\n",
      "Max gradient at step 533: 0.2339116930961609\n",
      "Max gradient at step 534: 0.11542602628469467\n",
      "Max gradient at step 535: 0.14162978529930115\n",
      "Max gradient at step 536: 0.18188229203224182\n",
      "Max gradient at step 537: 0.15409091114997864\n",
      "Max gradient at step 538: 0.26346156001091003\n",
      "Max gradient at step 539: 0.2760525047779083\n",
      "Max gradient at step 540: 0.07636471837759018\n",
      "Max gradient at step 541: 0.1588137447834015\n",
      "Max gradient at step 542: 0.10843734443187714\n",
      "Max gradient at step 543: 0.20746004581451416\n",
      "Max gradient at step 544: 0.11628440022468567\n",
      "Max gradient at step 545: 0.1467144638299942\n",
      "Max gradient at step 546: 0.0966147631406784\n",
      "Max gradient at step 547: 0.18985067307949066\n",
      "Max gradient at step 548: 0.18350574374198914\n",
      "Max gradient at step 549: 0.2100808173418045\n",
      "Max gradient at step 550: 0.1529300957918167\n",
      "Max gradient at step 551: 0.33865123987197876\n",
      "Max gradient at step 552: 0.17980492115020752\n",
      "Max gradient at step 553: 0.13815705478191376\n",
      "Max gradient at step 554: 0.14870189130306244\n",
      "Max gradient at step 555: 0.1222810298204422\n",
      "Max gradient at step 556: 0.16134697198867798\n",
      "Max gradient at step 557: 0.16224955022335052\n",
      "Max gradient at step 558: 0.24133673310279846\n",
      "Max gradient at step 559: 0.2566510736942291\n",
      "Max gradient at step 560: 0.12354757636785507\n",
      "Max gradient at step 561: 0.13183771073818207\n",
      "Max gradient at step 562: 0.05931917205452919\n",
      "Max gradient at step 563: 0.11251179873943329\n",
      "Max gradient at step 564: 0.12902531027793884\n",
      "Max gradient at step 565: 0.1998305767774582\n",
      "Max gradient at step 566: 0.15730951726436615\n",
      "Max gradient at step 567: 0.17761091887950897\n",
      "Max gradient at step 568: 0.1568690687417984\n",
      "Max gradient at step 569: 0.19823335111141205\n",
      "Max gradient at step 570: 0.08304278552532196\n",
      "Max gradient at step 571: 0.24740980565547943\n",
      "Max gradient at step 572: 0.09335263818502426\n",
      "Max gradient at step 573: 0.15455178916454315\n",
      "Max gradient at step 574: 0.14951512217521667\n",
      "Max gradient at step 575: 0.17915377020835876\n",
      "Max gradient at step 576: 0.18700367212295532\n",
      "Max gradient at step 577: 0.16011342406272888\n",
      "Max gradient at step 578: 0.20447051525115967\n",
      "Max gradient at step 579: 0.09174751490354538\n",
      "Max gradient at step 580: 0.21691127121448517\n",
      "Max gradient at step 581: 0.1332203596830368\n",
      "Max gradient at step 582: 0.15163956582546234\n",
      "Max gradient at step 583: 0.2165406048297882\n",
      "Max gradient at step 584: 0.23251838982105255\n",
      "Max gradient at step 585: 0.1757982224225998\n",
      "Max gradient at step 586: 0.13281413912773132\n",
      "Max gradient at step 587: 0.15765438973903656\n",
      "Max gradient at step 588: 0.1513524055480957\n",
      "Max gradient at step 589: 0.18286007642745972\n",
      "Max gradient at step 590: 0.15771324932575226\n",
      "Max gradient at step 591: 0.12688042223453522\n",
      "Max gradient at step 592: 0.12247554957866669\n",
      "Max gradient at step 593: 0.15470999479293823\n",
      "Max gradient at step 594: 0.15444763004779816\n",
      "Max gradient at step 595: 0.2204434722661972\n",
      "Max gradient at step 596: 0.2668013274669647\n",
      "Max gradient at step 597: 0.0350017175078392\n",
      "Max gradient at step 598: 0.14499874413013458\n",
      "Max gradient at step 599: 0.2782662808895111\n",
      "Max gradient at step 600: 0.2801832854747772\n",
      "Training loss (for one batch) at step 600: 0.5611\n",
      "Max gradient at step 601: 0.2509828805923462\n",
      "Max gradient at step 602: 0.19556556642055511\n",
      "Max gradient at step 603: 0.1438903659582138\n",
      "Max gradient at step 604: 0.11275187879800797\n",
      "Max gradient at step 605: 0.1692337691783905\n",
      "Max gradient at step 606: 0.11999310553073883\n",
      "Max gradient at step 607: 0.11557313799858093\n",
      "Max gradient at step 608: 0.23587723076343536\n",
      "Max gradient at step 609: 0.14286766946315765\n",
      "Max gradient at step 610: 0.10457335412502289\n",
      "Max gradient at step 611: 0.15984191000461578\n",
      "Max gradient at step 612: 0.1602814644575119\n",
      "Max gradient at step 613: 0.1579846441745758\n",
      "Max gradient at step 614: 0.11815405637025833\n",
      "Max gradient at step 615: 0.14356602728366852\n",
      "Max gradient at step 616: 0.16131989657878876\n",
      "Max gradient at step 617: 0.2315317541360855\n",
      "Max gradient at step 618: 0.12710674107074738\n",
      "Max gradient at step 619: 0.18870525062084198\n",
      "Max gradient at step 620: 0.13395638763904572\n",
      "Max gradient at step 621: 0.15318329632282257\n",
      "Max gradient at step 622: 0.14003917574882507\n",
      "Max gradient at step 623: 0.16025030612945557\n",
      "Max gradient at step 624: 0.060635946691036224\n",
      "Max gradient at step 625: 0.2995208501815796\n",
      "Max gradient at step 626: 0.09632648527622223\n",
      "Max gradient at step 627: 0.16424554586410522\n",
      "Max gradient at step 628: 0.1657068133354187\n",
      "Max gradient at step 629: 0.07986976951360703\n",
      "Max gradient at step 630: 0.15807156264781952\n",
      "Max gradient at step 631: 0.1680397391319275\n",
      "Max gradient at step 632: 0.17454852163791656\n",
      "Max gradient at step 633: 0.20199254155158997\n",
      "Max gradient at step 634: 0.24949181079864502\n",
      "Max gradient at step 635: 0.17570525407791138\n",
      "Max gradient at step 636: 0.2756180465221405\n",
      "Max gradient at step 637: 0.17441178858280182\n",
      "Max gradient at step 638: 0.23474180698394775\n",
      "Max gradient at step 639: 0.1537601351737976\n",
      "Max gradient at step 640: 0.2706618309020996\n",
      "Max gradient at step 641: 0.10862399637699127\n",
      "Max gradient at step 642: 0.1322566419839859\n",
      "Max gradient at step 643: 0.19929607212543488\n",
      "Max gradient at step 644: 0.11170289665460587\n",
      "Max gradient at step 645: 0.14325182139873505\n",
      "Max gradient at step 646: 0.23958931863307953\n",
      "Max gradient at step 647: 0.34518319368362427\n",
      "Max gradient at step 648: 0.1509583294391632\n",
      "Max gradient at step 649: 0.16854652762413025\n",
      "Max gradient at step 650: 0.21961145102977753\n",
      "Max gradient at step 651: 0.1585364043712616\n",
      "Max gradient at step 652: 0.25616157054901123\n",
      "Max gradient at step 653: 0.1542321741580963\n",
      "Max gradient at step 654: 0.18908172845840454\n",
      "Max gradient at step 655: 0.11182572692632675\n",
      "Max gradient at step 656: 0.14983762800693512\n",
      "Max gradient at step 657: 0.26788273453712463\n",
      "Max gradient at step 658: 0.25830575823783875\n",
      "Max gradient at step 659: 0.13837075233459473\n",
      "Max gradient at step 660: 0.1201273575425148\n",
      "Max gradient at step 661: 0.14685924351215363\n",
      "Max gradient at step 662: 0.1329723596572876\n",
      "Max gradient at step 663: 0.193131685256958\n",
      "Max gradient at step 664: 0.3216225504875183\n",
      "Max gradient at step 665: 0.13104701042175293\n",
      "Max gradient at step 666: 0.2301994264125824\n",
      "Max gradient at step 667: 0.1252511590719223\n",
      "Max gradient at step 668: 0.1408468335866928\n",
      "Max gradient at step 669: 0.13060076534748077\n",
      "Max gradient at step 670: 0.056242685765028\n",
      "Max gradient at step 671: 0.21678109467029572\n",
      "Max gradient at step 672: 0.19432151317596436\n",
      "Max gradient at step 673: 0.08916356414556503\n",
      "Max gradient at step 674: 0.2764090299606323\n",
      "Max gradient at step 675: 0.36231088638305664\n",
      "Max gradient at step 676: 0.23133395612239838\n",
      "Max gradient at step 677: 0.3592908978462219\n",
      "Max gradient at step 678: 0.20188865065574646\n",
      "Max gradient at step 679: 0.2021128386259079\n",
      "Max gradient at step 680: 0.1267917901277542\n",
      "Max gradient at step 681: 0.13493287563323975\n",
      "Max gradient at step 682: 0.10131758451461792\n",
      "Max gradient at step 683: 0.13522855937480927\n",
      "Max gradient at step 684: 0.18670573830604553\n",
      "Max gradient at step 685: 0.25103655457496643\n",
      "Max gradient at step 686: 0.22111856937408447\n",
      "Max gradient at step 687: 0.17322319746017456\n",
      "Max gradient at step 688: 0.08651234209537506\n",
      "Max gradient at step 689: 0.1375075727701187\n",
      "Max gradient at step 690: 0.20638766884803772\n",
      "Max gradient at step 691: 0.20950232446193695\n",
      "Max gradient at step 692: 0.1731354296207428\n",
      "Max gradient at step 693: 0.23145516216754913\n",
      "Max gradient at step 694: 0.23371700942516327\n",
      "Max gradient at step 695: 0.24031388759613037\n",
      "Max gradient at step 696: 0.24322082102298737\n",
      "Max gradient at step 697: 0.14434891939163208\n",
      "Max gradient at step 698: 0.15276312828063965\n",
      "Max gradient at step 699: 0.09535844624042511\n",
      "Max gradient at step 700: 0.2924954891204834\n",
      "Training loss (for one batch) at step 700: 0.1737\n",
      "Max gradient at step 701: 0.14976643025875092\n",
      "Max gradient at step 702: 0.10757283866405487\n",
      "Max gradient at step 703: 0.06799285858869553\n",
      "Max gradient at step 704: 0.183632493019104\n",
      "Max gradient at step 705: 0.24319542944431305\n",
      "Max gradient at step 706: 0.1695263534784317\n",
      "Max gradient at step 707: 0.13320809602737427\n",
      "Max gradient at step 708: 0.12506277859210968\n",
      "Max gradient at step 709: 0.20537704229354858\n",
      "Max gradient at step 710: 0.14663678407669067\n",
      "Max gradient at step 711: 0.1544257402420044\n",
      "Max gradient at step 712: 0.21625396609306335\n",
      "Max gradient at step 713: 0.1915142983198166\n",
      "Max gradient at step 714: 0.2749759554862976\n",
      "Max gradient at step 715: 0.20007623732089996\n",
      "Max gradient at step 716: 0.12439153343439102\n",
      "Max gradient at step 717: 0.1399865448474884\n",
      "Max gradient at step 718: 0.2387622445821762\n",
      "Max gradient at step 719: 0.2261689156293869\n",
      "Max gradient at step 720: 0.1273198425769806\n",
      "Max gradient at step 721: 0.18232442438602448\n",
      "Max gradient at step 722: 0.18540115654468536\n",
      "Max gradient at step 723: 0.20764823257923126\n",
      "Max gradient at step 724: 0.12286148220300674\n",
      "Max gradient at step 725: 0.1883123219013214\n",
      "Max gradient at step 726: 0.08462747931480408\n",
      "Max gradient at step 727: 0.3060300648212433\n",
      "Max gradient at step 728: 0.2757069766521454\n",
      "Max gradient at step 729: 0.17184390127658844\n",
      "Max gradient at step 730: 0.20870639383792877\n",
      "Max gradient at step 731: 0.322199285030365\n",
      "Max gradient at step 732: 0.31786754727363586\n",
      "Max gradient at step 733: 0.18403494358062744\n",
      "Max gradient at step 734: 0.11122161895036697\n",
      "Max gradient at step 735: 0.13472948968410492\n",
      "Max gradient at step 736: 0.12325883656740189\n",
      "Max gradient at step 737: 0.1690637469291687\n",
      "Max gradient at step 738: 0.1329023241996765\n",
      "Max gradient at step 739: 0.06517612189054489\n",
      "Max gradient at step 740: 0.13414189219474792\n",
      "Max gradient at step 741: 0.1369348168373108\n",
      "Max gradient at step 742: 0.11756350100040436\n",
      "Max gradient at step 743: 0.305553674697876\n",
      "Max gradient at step 744: 0.13346879184246063\n",
      "Max gradient at step 745: 0.23631419241428375\n",
      "Max gradient at step 746: 0.13826881349086761\n",
      "Max gradient at step 747: 0.24963417649269104\n",
      "Max gradient at step 748: 0.16624833643436432\n",
      "Max gradient at step 749: 0.17226210236549377\n",
      "Max gradient at step 750: 0.13044878840446472\n",
      "Max gradient at step 751: 0.13173292577266693\n",
      "Max gradient at step 752: 0.2917768061161041\n",
      "Max gradient at step 753: 0.15484920144081116\n",
      "Max gradient at step 754: 0.2070036679506302\n",
      "Max gradient at step 755: 0.13463358581066132\n",
      "Max gradient at step 756: 0.19837938249111176\n",
      "Max gradient at step 757: 0.1251397579908371\n",
      "Max gradient at step 758: 0.2015184760093689\n",
      "Max gradient at step 759: 0.2842922806739807\n",
      "Max gradient at step 760: 0.16306696832180023\n",
      "Max gradient at step 761: 0.11467571556568146\n",
      "Max gradient at step 762: 0.20550863444805145\n",
      "Max gradient at step 763: 0.139113649725914\n",
      "Max gradient at step 764: 0.11578229814767838\n",
      "Max gradient at step 765: 0.14442503452301025\n",
      "Max gradient at step 766: 0.1340900957584381\n",
      "Max gradient at step 767: 0.25990065932273865\n",
      "Max gradient at step 768: 0.1314745843410492\n",
      "Max gradient at step 769: 0.25228172540664673\n",
      "Max gradient at step 770: 0.17964714765548706\n",
      "Max gradient at step 771: 0.09231705963611603\n",
      "Max gradient at step 772: 0.11321137100458145\n",
      "Max gradient at step 773: 0.20850852131843567\n",
      "Max gradient at step 774: 0.2986459732055664\n",
      "Max gradient at step 775: 0.22782455384731293\n",
      "Max gradient at step 776: 0.22326025366783142\n",
      "Max gradient at step 777: 0.1223730742931366\n",
      "Max gradient at step 778: 0.15623323619365692\n",
      "Max gradient at step 779: 0.11605747044086456\n",
      "Max gradient at step 780: 0.2106090635061264\n",
      "Max gradient at step 781: 0.17202235758304596\n",
      "Max gradient at step 782: 0.13075663149356842\n",
      "Max gradient at step 783: 0.11926654726266861\n",
      "Max gradient at step 784: 0.15897832810878754\n",
      "Max gradient at step 785: 0.15878702700138092\n",
      "Max gradient at step 786: 0.16658692061901093\n",
      "Max gradient at step 787: 0.12877528369426727\n",
      "Max gradient at step 788: 0.10651245713233948\n",
      "Max gradient at step 789: 0.10065890103578568\n",
      "Max gradient at step 790: 0.12813514471054077\n",
      "Max gradient at step 791: 0.19997990131378174\n",
      "Max gradient at step 792: 0.1860380470752716\n",
      "Max gradient at step 793: 0.2925788462162018\n",
      "Max gradient at step 794: 0.11605723947286606\n",
      "Max gradient at step 795: 0.22485312819480896\n",
      "Max gradient at step 796: 0.12571091949939728\n",
      "Max gradient at step 797: 0.14513586461544037\n",
      "Max gradient at step 798: 0.2736569941043854\n",
      "Max gradient at step 799: 0.1802167296409607\n",
      "Max gradient at step 800: 0.1607690155506134\n",
      "Training loss (for one batch) at step 800: 0.1123\n",
      "Max gradient at step 801: 0.13747860491275787\n",
      "Max gradient at step 802: 0.0744996964931488\n",
      "Max gradient at step 803: 0.15110227465629578\n",
      "Max gradient at step 804: 0.166640505194664\n",
      "Max gradient at step 805: 0.24774302542209625\n",
      "Max gradient at step 806: 0.12252376228570938\n",
      "Max gradient at step 807: 0.25165802240371704\n",
      "Max gradient at step 808: 0.15885715186595917\n",
      "Max gradient at step 809: 0.237186998128891\n",
      "Max gradient at step 810: 0.16536177694797516\n",
      "Max gradient at step 811: 0.08259596675634384\n",
      "Max gradient at step 812: 0.2565196454524994\n",
      "Max gradient at step 813: 0.15951956808567047\n",
      "Max gradient at step 814: 0.2937110364437103\n",
      "Max gradient at step 815: 0.24206198751926422\n",
      "Max gradient at step 816: 0.1510285586118698\n",
      "Max gradient at step 817: 0.10160190612077713\n",
      "Max gradient at step 818: 0.1986464262008667\n",
      "Max gradient at step 819: 0.0861314907670021\n",
      "Max gradient at step 820: 0.18710049986839294\n",
      "Max gradient at step 821: 0.06138063594698906\n",
      "Max gradient at step 822: 0.09961945563554764\n",
      "Max gradient at step 823: 0.24057990312576294\n",
      "Max gradient at step 824: 0.11874806135892868\n",
      "Max gradient at step 825: 0.11502595990896225\n",
      "Max gradient at step 826: 0.19088014960289001\n",
      "Max gradient at step 827: 0.15163534879684448\n",
      "Max gradient at step 828: 0.21525774896144867\n",
      "Max gradient at step 829: 0.13158708810806274\n",
      "Max gradient at step 830: 0.19568559527397156\n",
      "Max gradient at step 831: 0.23058608174324036\n",
      "Max gradient at step 832: 0.15866924822330475\n",
      "Max gradient at step 833: 0.16450263559818268\n",
      "Max gradient at step 834: 0.04965303838253021\n",
      "Max gradient at step 835: 0.13571469485759735\n",
      "Max gradient at step 836: 0.1190108209848404\n",
      "Max gradient at step 837: 0.21214084327220917\n",
      "Max gradient at step 838: 0.11742240190505981\n",
      "Max gradient at step 839: 0.18241485953330994\n",
      "Max gradient at step 840: 0.1445845514535904\n",
      "Max gradient at step 841: 0.13624845445156097\n",
      "Max gradient at step 842: 0.17138110101222992\n",
      "Max gradient at step 843: 0.08285211026668549\n",
      "Max gradient at step 844: 0.1392764449119568\n",
      "Max gradient at step 845: 0.12779377400875092\n",
      "Max gradient at step 846: 0.10943233966827393\n",
      "Max gradient at step 847: 0.1600465625524521\n",
      "Max gradient at step 848: 0.1375533640384674\n",
      "Max gradient at step 849: 0.11880415678024292\n",
      "Max gradient at step 850: 0.09867129474878311\n",
      "Max gradient at step 851: 0.11641457676887512\n",
      "Max gradient at step 852: 0.1386723518371582\n",
      "Max gradient at step 853: 0.2391442209482193\n",
      "Max gradient at step 854: 0.18083049356937408\n",
      "Max gradient at step 855: 0.14520609378814697\n",
      "Max gradient at step 856: 0.1619945466518402\n",
      "Max gradient at step 857: 0.21230393648147583\n",
      "Max gradient at step 858: 0.09359496086835861\n",
      "Max gradient at step 859: 0.12542928755283356\n",
      "Max gradient at step 860: 0.426393985748291\n",
      "Max gradient at step 861: 0.21882571280002594\n",
      "Max gradient at step 862: 0.11280091106891632\n",
      "Max gradient at step 863: 0.13118991255760193\n",
      "Max gradient at step 864: 0.15975241363048553\n",
      "Max gradient at step 865: 0.1783057153224945\n",
      "Max gradient at step 866: 0.20482183992862701\n",
      "Max gradient at step 867: 0.2605064809322357\n",
      "Max gradient at step 868: 0.3023745119571686\n",
      "Max gradient at step 869: 0.08358775824308395\n",
      "Max gradient at step 870: 0.3223254382610321\n",
      "Max gradient at step 871: 0.2333056628704071\n",
      "Max gradient at step 872: 0.06939619779586792\n",
      "Max gradient at step 873: 0.2838207185268402\n",
      "Max gradient at step 874: 0.2505303621292114\n",
      "Max gradient at step 875: 0.1668154001235962\n",
      "Max gradient at step 876: 0.2968326807022095\n",
      "Max gradient at step 877: 0.233422189950943\n",
      "Max gradient at step 878: 0.25938352942466736\n",
      "Max gradient at step 879: 0.16738010942935944\n",
      "Max gradient at step 880: 0.13790735602378845\n",
      "Max gradient at step 881: 0.1754280924797058\n",
      "Max gradient at step 882: 0.3464292883872986\n",
      "Max gradient at step 883: 0.23657400906085968\n",
      "Max gradient at step 884: 0.1398748904466629\n",
      "Max gradient at step 885: 0.12862788140773773\n",
      "Max gradient at step 886: 0.20793764293193817\n",
      "Max gradient at step 887: 0.08900273591279984\n",
      "Max gradient at step 888: 0.1429305523633957\n",
      "Max gradient at step 889: 0.2739768624305725\n",
      "Max gradient at step 890: 0.24097998440265656\n",
      "Max gradient at step 891: 0.12388576567173004\n",
      "Max gradient at step 892: 0.17521829903125763\n",
      "Max gradient at step 893: 0.1334885060787201\n",
      "Max gradient at step 894: 0.11623777449131012\n",
      "Max gradient at step 895: 0.07999580353498459\n",
      "Max gradient at step 896: 0.13540811836719513\n",
      "Max gradient at step 897: 0.13292713463306427\n",
      "Max gradient at step 898: 0.20609918236732483\n",
      "Max gradient at step 899: 0.09328661859035492\n",
      "Max gradient at step 900: 0.12461031228303909\n",
      "Training loss (for one batch) at step 900: 0.1829\n",
      "Max gradient at step 901: 0.16063708066940308\n",
      "Max gradient at step 902: 0.14528027176856995\n",
      "Max gradient at step 903: 0.11418741941452026\n",
      "Max gradient at step 904: 0.15390978753566742\n",
      "Max gradient at step 905: 0.25623786449432373\n",
      "Max gradient at step 906: 0.3139256238937378\n",
      "Max gradient at step 907: 0.13016818463802338\n",
      "Max gradient at step 908: 0.2137313187122345\n",
      "Max gradient at step 909: 0.22050444781780243\n",
      "Max gradient at step 910: 0.1153479591012001\n",
      "Max gradient at step 911: 0.08134711533784866\n",
      "Max gradient at step 912: 0.08627983182668686\n",
      "Max gradient at step 913: 0.11933828890323639\n",
      "Max gradient at step 914: 0.1188870221376419\n",
      "Max gradient at step 915: 0.18438179790973663\n",
      "Max gradient at step 916: 0.12162307649850845\n",
      "Max gradient at step 917: 0.20502151548862457\n",
      "Max gradient at step 918: 0.18049649894237518\n",
      "Max gradient at step 919: 0.21039170026779175\n",
      "Max gradient at step 920: 0.30578139424324036\n",
      "Max gradient at step 921: 0.14751824736595154\n",
      "Max gradient at step 922: 0.11037353426218033\n",
      "Max gradient at step 923: 0.23111280798912048\n",
      "Max gradient at step 924: 0.09619102627038956\n",
      "Max gradient at step 925: 0.13982169330120087\n",
      "Max gradient at step 926: 0.1626807302236557\n",
      "Max gradient at step 927: 0.33072930574417114\n",
      "Max gradient at step 928: 0.1432117372751236\n",
      "Max gradient at step 929: 0.15006232261657715\n",
      "Max gradient at step 930: 0.3220303952693939\n",
      "Max gradient at step 931: 0.17571040987968445\n",
      "Max gradient at step 932: 0.12995868921279907\n",
      "Max gradient at step 933: 0.20597216486930847\n",
      "Max gradient at step 934: 0.08196710795164108\n",
      "Max gradient at step 935: 0.16747473180294037\n",
      "Max gradient at step 936: 0.2190883308649063\n",
      "Max gradient at step 937: 0.19482876360416412\n",
      "Max gradient at step 938: 0.20202092826366425\n",
      "Max gradient at step 939: 0.21656762063503265\n",
      "Max gradient at step 940: 0.13751238584518433\n",
      "Max gradient at step 941: 0.11736045777797699\n",
      "Max gradient at step 942: 0.07116765528917313\n",
      "Max gradient at step 943: 0.2197948843240738\n",
      "Max gradient at step 944: 0.1012616977095604\n",
      "Max gradient at step 945: 0.16033533215522766\n",
      "Max gradient at step 946: 0.14050734043121338\n",
      "Max gradient at step 947: 0.1969430148601532\n",
      "Max gradient at step 948: 0.1882513165473938\n",
      "Max gradient at step 949: 0.1434766948223114\n",
      "Max gradient at step 950: 0.09336492419242859\n",
      "Max gradient at step 951: 0.177555114030838\n",
      "Max gradient at step 952: 0.19661006331443787\n",
      "Max gradient at step 953: 0.1219402402639389\n",
      "Max gradient at step 954: 0.14128489792346954\n",
      "Max gradient at step 955: 0.1732272207736969\n",
      "Max gradient at step 956: 0.17707771062850952\n",
      "Max gradient at step 957: 0.1758338212966919\n",
      "Max gradient at step 958: 0.09448732435703278\n",
      "Max gradient at step 959: 0.17288875579833984\n",
      "Max gradient at step 960: 0.13134746253490448\n",
      "Max gradient at step 961: 0.5307971239089966\n",
      "Max gradient at step 962: 0.18988646566867828\n",
      "Max gradient at step 963: 0.4325235188007355\n",
      "Max gradient at step 964: 0.18967851996421814\n",
      "Max gradient at step 965: 0.16638760268688202\n",
      "Max gradient at step 966: 0.2568730413913727\n",
      "Max gradient at step 967: 0.19861818850040436\n",
      "Max gradient at step 968: 0.22214752435684204\n",
      "Max gradient at step 969: 0.22070807218551636\n",
      "Max gradient at step 970: 0.15555448830127716\n",
      "Max gradient at step 971: 0.24412322044372559\n",
      "Max gradient at step 972: 0.2667413353919983\n",
      "Max gradient at step 973: 0.21932680904865265\n",
      "Max gradient at step 974: 0.18929782509803772\n",
      "Max gradient at step 975: 0.2596118748188019\n",
      "Max gradient at step 976: 0.09498245269060135\n",
      "Max gradient at step 977: 0.29731905460357666\n",
      "Max gradient at step 978: 0.23527519404888153\n",
      "Max gradient at step 979: 0.132536843419075\n",
      "Max gradient at step 980: 0.24606865644454956\n",
      "Max gradient at step 981: 0.17262758314609528\n",
      "Max gradient at step 982: 0.1690976321697235\n",
      "Max gradient at step 983: 0.2599651515483856\n",
      "Max gradient at step 984: 0.18233796954154968\n",
      "Max gradient at step 985: 0.23837997019290924\n",
      "Max gradient at step 986: 0.2838706076145172\n",
      "Max gradient at step 987: 0.30639123916625977\n",
      "Max gradient at step 988: 0.1830027550458908\n",
      "Max gradient at step 989: 0.15304391086101532\n",
      "Max gradient at step 990: 0.11491518467664719\n",
      "Max gradient at step 991: 0.192616268992424\n",
      "Max gradient at step 992: 0.08586695045232773\n",
      "Max gradient at step 993: 0.1389072835445404\n",
      "Max gradient at step 994: 0.17237581312656403\n",
      "Max gradient at step 995: 0.23104198276996613\n",
      "Max gradient at step 996: 0.13873876631259918\n",
      "Max gradient at step 997: 0.32403478026390076\n",
      "Max gradient at step 998: 0.37262558937072754\n",
      "Max gradient at step 999: 0.25233280658721924\n",
      "Max gradient at step 1000: 0.18099746108055115\n",
      "Training loss (for one batch) at step 1000: 0.4232\n",
      "Max gradient at step 1001: 0.16190281510353088\n",
      "Max gradient at step 1002: 0.1594294011592865\n",
      "Max gradient at step 1003: 0.2578008472919464\n",
      "Max gradient at step 1004: 0.13790582120418549\n",
      "Max gradient at step 1005: 0.1750621199607849\n",
      "Max gradient at step 1006: 0.16592885553836823\n",
      "Max gradient at step 1007: 0.2125215381383896\n",
      "Max gradient at step 1008: 0.1576240211725235\n",
      "Max gradient at step 1009: 0.07405255734920502\n",
      "Max gradient at step 1010: 0.13927021622657776\n",
      "Max gradient at step 1011: 0.23300550878047943\n",
      "Max gradient at step 1012: 0.24504132568836212\n",
      "Max gradient at step 1013: 0.20207814872264862\n",
      "Max gradient at step 1014: 0.11553990095853806\n",
      "Max gradient at step 1015: 0.2864394187927246\n",
      "Max gradient at step 1016: 0.18254856765270233\n",
      "Max gradient at step 1017: 0.2158111333847046\n",
      "Max gradient at step 1018: 0.2212173491716385\n",
      "Max gradient at step 1019: 0.18377333879470825\n",
      "Max gradient at step 1020: 0.13385532796382904\n",
      "Max gradient at step 1021: 0.28511738777160645\n",
      "Max gradient at step 1022: 0.19683539867401123\n",
      "Max gradient at step 1023: 0.10205787420272827\n",
      "Max gradient at step 1024: 0.10285929590463638\n",
      "Max gradient at step 1025: 0.16470296680927277\n",
      "Max gradient at step 1026: 0.13282659649848938\n",
      "Max gradient at step 1027: 0.09953875094652176\n",
      "Max gradient at step 1028: 0.17596857249736786\n",
      "Max gradient at step 1029: 0.14835095405578613\n",
      "Max gradient at step 1030: 0.18621070683002472\n",
      "Max gradient at step 1031: 0.12941370904445648\n",
      "Max gradient at step 1032: 0.06925253570079803\n",
      "Max gradient at step 1033: 0.1240592673420906\n",
      "Max gradient at step 1034: 0.21485881507396698\n",
      "Max gradient at step 1035: 0.15004011988639832\n",
      "Max gradient at step 1036: 0.13622522354125977\n",
      "Max gradient at step 1037: 0.2037179172039032\n",
      "Max gradient at step 1038: 0.23196165263652802\n",
      "Max gradient at step 1039: 0.10808257013559341\n",
      "Max gradient at step 1040: 0.22272175550460815\n",
      "Max gradient at step 1041: 0.027834517881274223\n",
      "Max gradient at step 1042: 0.21677537262439728\n",
      "Max gradient at step 1043: 0.18448933959007263\n",
      "Max gradient at step 1044: 0.15067735314369202\n",
      "Max gradient at step 1045: 0.2675476670265198\n",
      "Max gradient at step 1046: 0.18908759951591492\n",
      "Max gradient at step 1047: 0.08861876279115677\n",
      "Max gradient at step 1048: 0.11695675551891327\n",
      "Max gradient at step 1049: 0.07313008606433868\n",
      "Max gradient at step 1050: 0.20204871892929077\n",
      "Max gradient at step 1051: 0.16636964678764343\n",
      "Max gradient at step 1052: 0.12812362611293793\n",
      "Max gradient at step 1053: 0.14949069917201996\n",
      "Max gradient at step 1054: 0.1952725648880005\n",
      "Max gradient at step 1055: 0.2633323073387146\n",
      "Max gradient at step 1056: 0.21744629740715027\n",
      "Max gradient at step 1057: 0.07924335449934006\n",
      "Max gradient at step 1058: 0.10933205485343933\n",
      "Max gradient at step 1059: 0.21671205759048462\n",
      "Max gradient at step 1060: 0.32738742232322693\n",
      "Max gradient at step 1061: 0.19360074400901794\n",
      "Max gradient at step 1062: 0.07296692579984665\n",
      "Max gradient at step 1063: 0.13410845398902893\n",
      "Max gradient at step 1064: 0.07317796349525452\n",
      "Max gradient at step 1065: 0.1297302395105362\n",
      "Max gradient at step 1066: 0.2704751789569855\n",
      "Max gradient at step 1067: 0.21565812826156616\n",
      "Max gradient at step 1068: 0.1864907592535019\n",
      "Max gradient at step 1069: 0.15575547516345978\n",
      "Max gradient at step 1070: 0.12923891842365265\n",
      "Max gradient at step 1071: 0.14175499975681305\n",
      "Max gradient at step 1072: 0.16105511784553528\n",
      "Max gradient at step 1073: 0.22161339223384857\n",
      "Max gradient at step 1074: 0.25815093517303467\n",
      "Max gradient at step 1075: 0.1069101095199585\n",
      "Max gradient at step 1076: 0.08505020290613174\n",
      "Max gradient at step 1077: 0.10712570697069168\n",
      "Max gradient at step 1078: 0.08671390265226364\n",
      "Max gradient at step 1079: 0.18497121334075928\n",
      "Max gradient at step 1080: 0.19283850491046906\n",
      "Max gradient at step 1081: 0.1869056075811386\n",
      "Max gradient at step 1082: 0.24491159617900848\n",
      "Max gradient at step 1083: 0.1809670776128769\n",
      "Max gradient at step 1084: 0.21331602334976196\n",
      "Max gradient at step 1085: 0.12771832942962646\n",
      "Max gradient at step 1086: 0.1839897483587265\n",
      "Max gradient at step 1087: 0.1508675515651703\n",
      "Max gradient at step 1088: 0.11831216514110565\n",
      "Max gradient at step 1089: 0.12930235266685486\n",
      "Max gradient at step 1090: 0.23158809542655945\n",
      "Max gradient at step 1091: 0.15502870082855225\n",
      "Max gradient at step 1092: 0.10775910317897797\n",
      "Max gradient at step 1093: 0.11304310709238052\n",
      "Max gradient at step 1094: 0.12071067839860916\n",
      "Max gradient at step 1095: 0.13267108798027039\n",
      "Max gradient at step 1096: 0.19241110980510712\n",
      "Max gradient at step 1097: 0.09602963924407959\n",
      "Max gradient at step 1098: 0.21383123099803925\n",
      "Max gradient at step 1099: 0.11113495379686356\n",
      "Max gradient at step 1100: 0.2056855708360672\n",
      "Training loss (for one batch) at step 1100: 0.3482\n",
      "Max gradient at step 1101: 0.08672205358743668\n",
      "Max gradient at step 1102: 0.13432656228542328\n",
      "Max gradient at step 1103: 0.12074777483940125\n",
      "Max gradient at step 1104: 0.19219188392162323\n",
      "Max gradient at step 1105: 0.16014987230300903\n",
      "Max gradient at step 1106: 0.15443730354309082\n",
      "Max gradient at step 1107: 0.15880325436592102\n",
      "Max gradient at step 1108: 0.12437354028224945\n",
      "Max gradient at step 1109: 0.11903037875890732\n",
      "Max gradient at step 1110: 0.1115603893995285\n",
      "Max gradient at step 1111: 0.1448100060224533\n",
      "Max gradient at step 1112: 0.1984390765428543\n",
      "Max gradient at step 1113: 0.13178403675556183\n",
      "Max gradient at step 1114: 0.3223501145839691\n",
      "Max gradient at step 1115: 0.30993446707725525\n",
      "Max gradient at step 1116: 0.21468448638916016\n",
      "Max gradient at step 1117: 0.1298011988401413\n",
      "Max gradient at step 1118: 0.13076519966125488\n",
      "Max gradient at step 1119: 0.24329710006713867\n",
      "Max gradient at step 1120: 0.15497063100337982\n",
      "Max gradient at step 1121: 0.09740656614303589\n",
      "Max gradient at step 1122: 0.10959447175264359\n",
      "Max gradient at step 1123: 0.13723373413085938\n",
      "Max gradient at step 1124: 0.11878123134374619\n",
      "Max gradient at step 1125: 0.06656701862812042\n",
      "Max gradient at step 1126: 0.0708623081445694\n",
      "Max gradient at step 1127: 0.18088555335998535\n",
      "Max gradient at step 1128: 0.13877956569194794\n",
      "Max gradient at step 1129: 0.09968285262584686\n",
      "Max gradient at step 1130: 0.11322250217199326\n",
      "Max gradient at step 1131: 0.09461072087287903\n",
      "Max gradient at step 1132: 0.1928102672100067\n",
      "Max gradient at step 1133: 0.1090826764702797\n",
      "Max gradient at step 1134: 0.0912119448184967\n",
      "Max gradient at step 1135: 0.21673595905303955\n",
      "Max gradient at step 1136: 0.21674171090126038\n",
      "Max gradient at step 1137: 0.34790974855422974\n",
      "Max gradient at step 1138: 0.26391175389289856\n",
      "Max gradient at step 1139: 0.225434347987175\n",
      "Max gradient at step 1140: 0.18220403790473938\n",
      "Max gradient at step 1141: 0.1836744099855423\n",
      "Max gradient at step 1142: 0.19775164127349854\n",
      "Max gradient at step 1143: 0.21553869545459747\n",
      "Max gradient at step 1144: 0.24211111664772034\n",
      "Max gradient at step 1145: 0.18978029489517212\n",
      "Max gradient at step 1146: 0.15869490802288055\n",
      "Max gradient at step 1147: 0.2956991195678711\n",
      "Max gradient at step 1148: 0.16390681266784668\n",
      "Max gradient at step 1149: 0.15182091295719147\n",
      "Max gradient at step 1150: 0.21496841311454773\n",
      "Max gradient at step 1151: 0.1842176616191864\n",
      "Max gradient at step 1152: 0.21500788629055023\n",
      "Max gradient at step 1153: 0.22609734535217285\n",
      "Max gradient at step 1154: 0.19336749613285065\n",
      "Max gradient at step 1155: 0.24884600937366486\n",
      "Max gradient at step 1156: 0.20987264811992645\n",
      "Max gradient at step 1157: 0.15238608419895172\n",
      "Max gradient at step 1158: 0.07046662271022797\n",
      "Max gradient at step 1159: 0.15658725798130035\n",
      "Max gradient at step 1160: 0.18124781548976898\n",
      "Max gradient at step 1161: 0.13614490628242493\n",
      "Max gradient at step 1162: 0.11035753786563873\n",
      "Max gradient at step 1163: 0.1955789029598236\n",
      "Max gradient at step 1164: 0.3119887113571167\n",
      "Max gradient at step 1165: 0.1986631155014038\n",
      "Max gradient at step 1166: 0.10884103924036026\n",
      "Max gradient at step 1167: 0.22686491906642914\n",
      "Max gradient at step 1168: 0.1862674057483673\n",
      "Max gradient at step 1169: 0.11772804707288742\n",
      "Max gradient at step 1170: 0.1532830446958542\n",
      "Max gradient at step 1171: 0.1614241749048233\n",
      "Max gradient at step 1172: 0.2042904645204544\n",
      "Max gradient at step 1173: 0.11869823187589645\n",
      "Max gradient at step 1174: 0.3266858160495758\n",
      "Max gradient at step 1175: 0.1669125258922577\n",
      "Max gradient at step 1176: 0.1417432427406311\n",
      "Max gradient at step 1177: 0.13347868621349335\n",
      "Max gradient at step 1178: 0.060319699347019196\n",
      "Max gradient at step 1179: 0.2926347851753235\n",
      "Max gradient at step 1180: 0.17238692939281464\n",
      "Max gradient at step 1181: 0.16379258036613464\n",
      "Max gradient at step 1182: 0.21258506178855896\n",
      "Max gradient at step 1183: 0.3414316773414612\n",
      "Max gradient at step 1184: 0.12218396365642548\n",
      "Max gradient at step 1185: 0.14719738066196442\n",
      "Max gradient at step 1186: 0.17044925689697266\n",
      "Max gradient at step 1187: 0.23201867938041687\n",
      "Max gradient at step 1188: 0.23069699108600616\n",
      "Max gradient at step 1189: 0.19680969417095184\n",
      "Max gradient at step 1190: 0.08703995496034622\n",
      "Max gradient at step 1191: 0.23232130706310272\n",
      "Max gradient at step 1192: 0.045139066874980927\n",
      "Max gradient at step 1193: 0.21669450402259827\n",
      "Max gradient at step 1194: 0.2599329650402069\n",
      "Max gradient at step 1195: 0.13002531230449677\n",
      "Max gradient at step 1196: 0.21949374675750732\n",
      "Max gradient at step 1197: 0.1514313817024231\n",
      "Max gradient at step 1198: 0.19368840754032135\n",
      "Max gradient at step 1199: 0.1487954556941986\n",
      "Max gradient at step 1200: 0.15560685098171234\n",
      "Training loss (for one batch) at step 1200: 0.1833\n",
      "Max gradient at step 1201: 0.17454202473163605\n",
      "Max gradient at step 1202: 0.14637580513954163\n",
      "Max gradient at step 1203: 0.19686304032802582\n",
      "Max gradient at step 1204: 0.24152475595474243\n",
      "Max gradient at step 1205: 0.14218157529830933\n",
      "Max gradient at step 1206: 0.10651566833257675\n",
      "Max gradient at step 1207: 0.16118274629116058\n",
      "Max gradient at step 1208: 0.12253034114837646\n",
      "Max gradient at step 1209: 0.1377217024564743\n",
      "Max gradient at step 1210: 0.111913301050663\n",
      "Max gradient at step 1211: 0.09916093945503235\n",
      "Max gradient at step 1212: 0.20975898206233978\n",
      "Max gradient at step 1213: 0.13735799491405487\n",
      "Max gradient at step 1214: 0.1553916484117508\n",
      "Max gradient at step 1215: 0.14172396063804626\n",
      "Max gradient at step 1216: 0.1218675822019577\n",
      "Max gradient at step 1217: 0.31114816665649414\n",
      "Max gradient at step 1218: 0.2549874484539032\n",
      "Max gradient at step 1219: 0.12048079073429108\n",
      "Max gradient at step 1220: 0.11749384552240372\n",
      "Max gradient at step 1221: 0.12084797769784927\n",
      "Max gradient at step 1222: 0.26622822880744934\n",
      "Max gradient at step 1223: 0.3142895996570587\n",
      "Max gradient at step 1224: 0.0916089341044426\n",
      "Max gradient at step 1225: 0.1411445140838623\n",
      "Max gradient at step 1226: 0.09929321706295013\n",
      "Max gradient at step 1227: 0.19804655015468597\n",
      "Max gradient at step 1228: 0.2285284847021103\n",
      "Max gradient at step 1229: 0.09328174591064453\n",
      "Max gradient at step 1230: 0.1661524921655655\n",
      "Max gradient at step 1231: 0.16282233595848083\n",
      "Max gradient at step 1232: 0.19046801328659058\n",
      "Max gradient at step 1233: 0.1324498951435089\n",
      "Max gradient at step 1234: 0.24741403758525848\n",
      "Max gradient at step 1235: 0.22520263493061066\n",
      "Max gradient at step 1236: 0.17375493049621582\n",
      "Max gradient at step 1237: 0.09482069313526154\n",
      "Max gradient at step 1238: 0.27339109778404236\n",
      "Max gradient at step 1239: 0.1754457801580429\n",
      "Max gradient at step 1240: 0.24025492370128632\n",
      "Max gradient at step 1241: 0.10351749509572983\n",
      "Max gradient at step 1242: 0.1719612330198288\n",
      "Max gradient at step 1243: 0.19771355390548706\n",
      "Max gradient at step 1244: 0.13142606616020203\n",
      "Max gradient at step 1245: 0.08458331972360611\n",
      "Max gradient at step 1246: 0.2508159875869751\n",
      "Max gradient at step 1247: 0.15650051832199097\n",
      "Max gradient at step 1248: 0.18138492107391357\n",
      "Max gradient at step 1249: 0.12049204111099243\n",
      "Max gradient at step 1250: 0.26967158913612366\n",
      "Max gradient at step 1251: 0.07158360630273819\n",
      "Max gradient at step 1252: 0.1727994680404663\n",
      "Max gradient at step 1253: 0.176883265376091\n",
      "Max gradient at step 1254: 0.16462837159633636\n",
      "Max gradient at step 1255: 0.12973648309707642\n",
      "Max gradient at step 1256: 0.23499706387519836\n",
      "Max gradient at step 1257: 0.09617927670478821\n",
      "Max gradient at step 1258: 0.12325859814882278\n",
      "Max gradient at step 1259: 0.15735416114330292\n",
      "Max gradient at step 1260: 0.11668069660663605\n",
      "Max gradient at step 1261: 0.18880289793014526\n",
      "Max gradient at step 1262: 0.13255801796913147\n",
      "Max gradient at step 1263: 0.1919746994972229\n",
      "Max gradient at step 1264: 0.1391441971063614\n",
      "Max gradient at step 1265: 0.21593110263347626\n",
      "Max gradient at step 1266: 0.18695475161075592\n",
      "Max gradient at step 1267: 0.1379644125699997\n",
      "Max gradient at step 1268: 0.09818882495164871\n",
      "Max gradient at step 1269: 0.14137856662273407\n",
      "Max gradient at step 1270: 0.11823011189699173\n",
      "Max gradient at step 1271: 0.1227513775229454\n",
      "Max gradient at step 1272: 0.11743044853210449\n",
      "Max gradient at step 1273: 0.12461090832948685\n",
      "Max gradient at step 1274: 0.12474052608013153\n",
      "Max gradient at step 1275: 0.12050650268793106\n",
      "Max gradient at step 1276: 0.2024046629667282\n",
      "Max gradient at step 1277: 0.267364501953125\n",
      "Max gradient at step 1278: 0.11372344940900803\n",
      "Max gradient at step 1279: 0.19792097806930542\n",
      "Max gradient at step 1280: 0.17958541214466095\n",
      "Max gradient at step 1281: 0.13926354050636292\n",
      "Max gradient at step 1282: 0.12073139101266861\n",
      "Max gradient at step 1283: 0.13268740475177765\n",
      "Max gradient at step 1284: 0.054896239191293716\n",
      "Max gradient at step 1285: 0.356251984834671\n",
      "Max gradient at step 1286: 0.1745772659778595\n",
      "Max gradient at step 1287: 0.24962960183620453\n",
      "Max gradient at step 1288: 0.1638364940881729\n",
      "Max gradient at step 1289: 0.17026542127132416\n",
      "Max gradient at step 1290: 0.2701091766357422\n",
      "Max gradient at step 1291: 0.3275250196456909\n",
      "Max gradient at step 1292: 0.1679491102695465\n",
      "Max gradient at step 1293: 0.1628716140985489\n",
      "Max gradient at step 1294: 0.3337321877479553\n",
      "Max gradient at step 1295: 0.1692260503768921\n",
      "Max gradient at step 1296: 0.23681873083114624\n",
      "Max gradient at step 1297: 0.14989639818668365\n",
      "Max gradient at step 1298: 0.2447725087404251\n",
      "Max gradient at step 1299: 0.15232747793197632\n",
      "Max gradient at step 1300: 0.3618563115596771\n",
      "Training loss (for one batch) at step 1300: 0.3659\n",
      "Max gradient at step 1301: 0.13299992680549622\n",
      "Max gradient at step 1302: 0.16102854907512665\n",
      "Max gradient at step 1303: 0.29325833916664124\n",
      "Max gradient at step 1304: 0.10642670094966888\n",
      "Max gradient at step 1305: 0.11590510606765747\n",
      "Max gradient at step 1306: 0.22843097150325775\n",
      "Max gradient at step 1307: 0.15616106986999512\n",
      "Max gradient at step 1308: 0.24311967194080353\n",
      "Max gradient at step 1309: 0.19946236908435822\n",
      "Max gradient at step 1310: 0.27433350682258606\n",
      "Max gradient at step 1311: 0.12019523233175278\n",
      "Max gradient at step 1312: 0.27874335646629333\n",
      "Max gradient at step 1313: 0.19752004742622375\n",
      "Max gradient at step 1314: 0.1595468521118164\n",
      "Max gradient at step 1315: 0.13936559855937958\n",
      "Max gradient at step 1316: 0.10620802640914917\n",
      "Max gradient at step 1317: 0.09913848340511322\n",
      "Max gradient at step 1318: 0.2104019671678543\n",
      "Max gradient at step 1319: 0.18348319828510284\n",
      "Max gradient at step 1320: 0.12489432841539383\n",
      "Max gradient at step 1321: 0.2015768587589264\n",
      "Max gradient at step 1322: 0.22805382311344147\n",
      "Max gradient at step 1323: 0.4651842415332794\n",
      "Max gradient at step 1324: 0.14126674830913544\n",
      "Max gradient at step 1325: 0.2280002236366272\n",
      "Max gradient at step 1326: 0.15211822092533112\n",
      "Max gradient at step 1327: 0.18067370355129242\n",
      "Max gradient at step 1328: 0.22527505457401276\n",
      "Max gradient at step 1329: 0.29533851146698\n",
      "Max gradient at step 1330: 0.13589182496070862\n",
      "Max gradient at step 1331: 0.23695746064186096\n",
      "Max gradient at step 1332: 0.1861703097820282\n",
      "Max gradient at step 1333: 0.10598339885473251\n",
      "Max gradient at step 1334: 0.28445538878440857\n",
      "Max gradient at step 1335: 0.174009308218956\n",
      "Max gradient at step 1336: 0.23151957988739014\n",
      "Max gradient at step 1337: 0.21975870430469513\n",
      "Max gradient at step 1338: 0.19956907629966736\n",
      "Max gradient at step 1339: 0.155427485704422\n",
      "Max gradient at step 1340: 0.23537912964820862\n",
      "Max gradient at step 1341: 0.19334293901920319\n",
      "Max gradient at step 1342: 0.1713264137506485\n",
      "Max gradient at step 1343: 0.1878328174352646\n",
      "Max gradient at step 1344: 0.2913258671760559\n",
      "Max gradient at step 1345: 0.23122833669185638\n",
      "Max gradient at step 1346: 0.2088024765253067\n",
      "Max gradient at step 1347: 0.23712854087352753\n",
      "Max gradient at step 1348: 0.18788528442382812\n",
      "Max gradient at step 1349: 0.2526209354400635\n",
      "Max gradient at step 1350: 0.23204050958156586\n",
      "Max gradient at step 1351: 0.17115524411201477\n",
      "Max gradient at step 1352: 0.1577518880367279\n",
      "Max gradient at step 1353: 0.2236873060464859\n",
      "Max gradient at step 1354: 0.13601058721542358\n",
      "Max gradient at step 1355: 0.20569290220737457\n",
      "Max gradient at step 1356: 0.1381087452173233\n",
      "Max gradient at step 1357: 0.2946135401725769\n",
      "Max gradient at step 1358: 0.13615861535072327\n",
      "Max gradient at step 1359: 0.10332980006933212\n",
      "Max gradient at step 1360: 0.12533926963806152\n",
      "Max gradient at step 1361: 0.16670995950698853\n",
      "Max gradient at step 1362: 0.10860524326562881\n",
      "Max gradient at step 1363: 0.1442948877811432\n",
      "Max gradient at step 1364: 0.2200310379266739\n",
      "Max gradient at step 1365: 0.17382735013961792\n",
      "Max gradient at step 1366: 0.23407940566539764\n",
      "Max gradient at step 1367: 0.09033547341823578\n",
      "Max gradient at step 1368: 0.0675617903470993\n",
      "Max gradient at step 1369: 0.07161016762256622\n",
      "Max gradient at step 1370: 0.1200496256351471\n",
      "Max gradient at step 1371: 0.2087358832359314\n",
      "Max gradient at step 1372: 0.13775767385959625\n",
      "Max gradient at step 1373: 0.10953781008720398\n",
      "Max gradient at step 1374: 0.12609872221946716\n",
      "Max gradient at step 1375: 0.10976950824260712\n",
      "Max gradient at step 1376: 0.27930378913879395\n",
      "Max gradient at step 1377: 0.24386334419250488\n",
      "Max gradient at step 1378: 0.18051062524318695\n",
      "Max gradient at step 1379: 0.2410922646522522\n",
      "Max gradient at step 1380: 0.12479434907436371\n",
      "Max gradient at step 1381: 0.07485860586166382\n",
      "Max gradient at step 1382: 0.10950413346290588\n",
      "Max gradient at step 1383: 0.13109511137008667\n",
      "Max gradient at step 1384: 0.11259723454713821\n",
      "Max gradient at step 1385: 0.11524748802185059\n",
      "Max gradient at step 1386: 0.11516166478395462\n",
      "Max gradient at step 1387: 0.08447270840406418\n",
      "Max gradient at step 1388: 0.19138062000274658\n",
      "Max gradient at step 1389: 0.20550107955932617\n",
      "Max gradient at step 1390: 0.2860400974750519\n",
      "Max gradient at step 1391: 0.14586736261844635\n",
      "Max gradient at step 1392: 0.14691835641860962\n",
      "Max gradient at step 1393: 0.06676268577575684\n",
      "Max gradient at step 1394: 0.16912610828876495\n",
      "Max gradient at step 1395: 0.19095543026924133\n",
      "Max gradient at step 1396: 0.11852359026670456\n",
      "Max gradient at step 1397: 0.05433538556098938\n",
      "Max gradient at step 1398: 0.2153606116771698\n",
      "Max gradient at step 1399: 0.22799977660179138\n",
      "Max gradient at step 1400: 0.25420188903808594\n",
      "Training loss (for one batch) at step 1400: 0.3550\n",
      "Max gradient at step 1401: 0.12568962574005127\n",
      "Max gradient at step 1402: 0.17764990031719208\n",
      "Max gradient at step 1403: 0.24721944332122803\n",
      "Max gradient at step 1404: 0.15046389400959015\n",
      "Max gradient at step 1405: 0.2192278951406479\n",
      "Max gradient at step 1406: 0.20166559517383575\n",
      "Max gradient at step 1407: 0.193034365773201\n",
      "Max gradient at step 1408: 0.251478374004364\n",
      "Max gradient at step 1409: 0.22481465339660645\n",
      "Max gradient at step 1410: 0.15038332343101501\n",
      "Max gradient at step 1411: 0.25704261660575867\n",
      "Max gradient at step 1412: 0.17372235655784607\n",
      "Max gradient at step 1413: 0.1291685849428177\n",
      "Max gradient at step 1414: 0.08911669999361038\n",
      "Max gradient at step 1415: 0.1870582550764084\n",
      "Max gradient at step 1416: 0.15870393812656403\n",
      "Max gradient at step 1417: 0.22398816049098969\n",
      "Max gradient at step 1418: 0.3510120213031769\n",
      "Max gradient at step 1419: 0.13926555216312408\n",
      "Max gradient at step 1420: 0.16674448549747467\n",
      "Max gradient at step 1421: 0.16503381729125977\n",
      "Max gradient at step 1422: 0.1953490823507309\n",
      "Max gradient at step 1423: 0.22048328816890717\n",
      "Max gradient at step 1424: 0.2460273951292038\n",
      "Max gradient at step 1425: 0.16269949078559875\n",
      "Max gradient at step 1426: 0.14430509507656097\n",
      "Max gradient at step 1427: 0.23799929022789001\n",
      "Max gradient at step 1428: 0.13211460411548615\n",
      "Max gradient at step 1429: 0.20603489875793457\n",
      "Max gradient at step 1430: 0.1582133173942566\n",
      "Max gradient at step 1431: 0.15957431495189667\n",
      "Max gradient at step 1432: 0.12786230444908142\n",
      "Max gradient at step 1433: 0.1005212590098381\n",
      "Max gradient at step 1434: 0.18158504366874695\n",
      "Max gradient at step 1435: 0.1253863424062729\n",
      "Max gradient at step 1436: 0.20456074178218842\n",
      "Max gradient at step 1437: 0.2926669120788574\n",
      "Max gradient at step 1438: 0.21482422947883606\n",
      "Max gradient at step 1439: 0.27442577481269836\n",
      "Max gradient at step 1440: 0.40416646003723145\n",
      "Max gradient at step 1441: 0.14323507249355316\n",
      "Max gradient at step 1442: 0.18928849697113037\n",
      "Max gradient at step 1443: 0.10575471818447113\n",
      "Max gradient at step 1444: 0.16658107936382294\n",
      "Max gradient at step 1445: 0.1385241150856018\n",
      "Max gradient at step 1446: 0.14989085495471954\n",
      "Max gradient at step 1447: 0.25051867961883545\n",
      "Max gradient at step 1448: 0.1752876192331314\n",
      "Max gradient at step 1449: 0.17552658915519714\n",
      "Max gradient at step 1450: 0.1690129041671753\n",
      "Max gradient at step 1451: 0.2321854531764984\n",
      "Max gradient at step 1452: 0.1135648861527443\n",
      "Max gradient at step 1453: 0.13932004570960999\n",
      "Max gradient at step 1454: 0.1171770766377449\n",
      "Max gradient at step 1455: 0.14871957898139954\n",
      "Max gradient at step 1456: 0.2833269536495209\n",
      "Max gradient at step 1457: 0.17799249291419983\n",
      "Max gradient at step 1458: 0.17481745779514313\n",
      "Max gradient at step 1459: 0.06331244111061096\n",
      "Max gradient at step 1460: 0.32727718353271484\n",
      "Max gradient at step 1461: 0.08120982348918915\n",
      "Max gradient at step 1462: 0.2422589510679245\n",
      "Max gradient at step 1463: 0.05693061649799347\n",
      "Max gradient at step 1464: 0.1122477576136589\n",
      "Max gradient at step 1465: 0.3803166449069977\n",
      "Max gradient at step 1466: 0.16046030819416046\n",
      "Max gradient at step 1467: 0.30074915289878845\n",
      "Max gradient at step 1468: 0.14064078032970428\n",
      "Max gradient at step 1469: 0.2035781592130661\n",
      "Max gradient at step 1470: 0.17689400911331177\n",
      "Max gradient at step 1471: 0.14566077291965485\n",
      "Max gradient at step 1472: 0.16470709443092346\n",
      "Max gradient at step 1473: 0.21055454015731812\n",
      "Max gradient at step 1474: 0.22197337448596954\n",
      "Max gradient at step 1475: 0.21959924697875977\n",
      "Max gradient at step 1476: 0.23719708621501923\n",
      "Max gradient at step 1477: 0.05581500381231308\n",
      "Max gradient at step 1478: 0.2574562132358551\n",
      "Max gradient at step 1479: 0.24148572981357574\n",
      "Max gradient at step 1480: 0.24816426634788513\n",
      "Max gradient at step 1481: 0.06220554560422897\n",
      "Max gradient at step 1482: 0.30077534914016724\n",
      "Max gradient at step 1483: 0.08879116177558899\n",
      "Max gradient at step 1484: 0.11979369819164276\n",
      "Max gradient at step 1485: 0.16033639013767242\n",
      "Max gradient at step 1486: 0.1653890162706375\n",
      "Max gradient at step 1487: 0.1702510118484497\n",
      "Max gradient at step 1488: 0.2838795483112335\n",
      "Max gradient at step 1489: 0.06176183745265007\n",
      "Max gradient at step 1490: 0.17162460088729858\n",
      "Max gradient at step 1491: 0.12097805738449097\n",
      "Max gradient at step 1492: 0.1791815608739853\n",
      "Max gradient at step 1493: 0.15539349615573883\n",
      "Max gradient at step 1494: 0.19062694907188416\n",
      "Max gradient at step 1495: 0.113985575735569\n",
      "Max gradient at step 1496: 0.21307045221328735\n",
      "Max gradient at step 1497: 0.20128747820854187\n",
      "Max gradient at step 1498: 0.19119079411029816\n",
      "Max gradient at step 1499: 0.20156219601631165\n",
      "Max gradient at step 1500: 0.21494506299495697\n",
      "Training loss (for one batch) at step 1500: 0.2608\n",
      "Max gradient at step 1501: 0.12840494513511658\n",
      "Max gradient at step 1502: 0.09087134152650833\n",
      "Max gradient at step 1503: 0.14356783032417297\n",
      "Max gradient at step 1504: 0.08676736056804657\n",
      "Max gradient at step 1505: 0.1312892884016037\n",
      "Max gradient at step 1506: 0.15256182849407196\n",
      "Max gradient at step 1507: 0.09530515968799591\n",
      "Max gradient at step 1508: 0.12899717688560486\n",
      "Max gradient at step 1509: 0.10821042209863663\n",
      "Max gradient at step 1510: 0.24551399052143097\n",
      "Max gradient at step 1511: 0.1248941421508789\n",
      "Max gradient at step 1512: 0.21719448268413544\n",
      "Max gradient at step 1513: 0.16379180550575256\n",
      "Max gradient at step 1514: 0.19347387552261353\n",
      "Max gradient at step 1515: 0.23287852108478546\n",
      "Max gradient at step 1516: 0.13669809699058533\n",
      "Max gradient at step 1517: 0.2566608786582947\n",
      "Max gradient at step 1518: 0.2010008692741394\n",
      "Max gradient at step 1519: 0.16862614452838898\n",
      "Max gradient at step 1520: 0.18064813315868378\n",
      "Max gradient at step 1521: 0.18795238435268402\n",
      "Max gradient at step 1522: 0.11860734224319458\n",
      "Max gradient at step 1523: 0.12341830134391785\n",
      "Max gradient at step 1524: 0.2621517777442932\n",
      "Max gradient at step 1525: 0.22495310008525848\n",
      "Max gradient at step 1526: 0.24002327024936676\n",
      "Max gradient at step 1527: 0.28912970423698425\n",
      "Max gradient at step 1528: 0.309266597032547\n",
      "Max gradient at step 1529: 0.14443184435367584\n",
      "Max gradient at step 1530: 0.1302880048751831\n",
      "Max gradient at step 1531: 0.13971064984798431\n",
      "Max gradient at step 1532: 0.13889361917972565\n",
      "Max gradient at step 1533: 0.10753604769706726\n",
      "Max gradient at step 1534: 0.18056224286556244\n",
      "Max gradient at step 1535: 0.07149885594844818\n",
      "Max gradient at step 1536: 0.04951043426990509\n",
      "Max gradient at step 1537: 0.0559689886868\n",
      "Max gradient at step 1538: 0.13154980540275574\n",
      "Max gradient at step 1539: 0.13884598016738892\n",
      "Max gradient at step 1540: 0.2220405787229538\n",
      "Max gradient at step 1541: 0.1254323273897171\n",
      "Max gradient at step 1542: 0.28980907797813416\n",
      "Max gradient at step 1543: 0.17291632294654846\n",
      "Max gradient at step 1544: 0.24145901203155518\n",
      "Max gradient at step 1545: 0.2209906429052353\n",
      "Max gradient at step 1546: 0.20964385569095612\n",
      "Max gradient at step 1547: 0.15400372445583344\n",
      "Max gradient at step 1548: 0.1755259484052658\n",
      "Max gradient at step 1549: 0.1577182412147522\n",
      "Max gradient at step 1550: 0.1996755301952362\n",
      "Max gradient at step 1551: 0.2280208021402359\n",
      "Max gradient at step 1552: 0.17939093708992004\n",
      "Max gradient at step 1553: 0.27003976702690125\n",
      "Max gradient at step 1554: 0.17788903415203094\n",
      "Max gradient at step 1555: 0.4497355818748474\n",
      "Max gradient at step 1556: 0.12128614634275436\n",
      "Max gradient at step 1557: 0.20393986999988556\n",
      "Max gradient at step 1558: 0.19630394876003265\n",
      "Max gradient at step 1559: 0.14941959083080292\n",
      "Max gradient at step 1560: 0.15216706693172455\n",
      "Max gradient at step 1561: 0.17348583042621613\n",
      "Max gradient at step 1562: 0.16325007379055023\n",
      "Max gradient at step 1563: 0.15464374423027039\n",
      "Max gradient at step 1564: 0.22690770030021667\n",
      "Max gradient at step 1565: 0.13777604699134827\n",
      "Max gradient at step 1566: 0.05500893294811249\n",
      "Max gradient at step 1567: 0.08645012229681015\n",
      "Max gradient at step 1568: 0.1232350617647171\n",
      "Max gradient at step 1569: 0.19060520827770233\n",
      "Max gradient at step 1570: 0.239618718624115\n",
      "Max gradient at step 1571: 0.18786288797855377\n",
      "Max gradient at step 1572: 0.39044150710105896\n",
      "Max gradient at step 1573: 0.14460617303848267\n",
      "Max gradient at step 1574: 0.3662170469760895\n",
      "Max gradient at step 1575: 0.15004296600818634\n",
      "Max gradient at step 1576: 0.1704806685447693\n",
      "Max gradient at step 1577: 0.14260859787464142\n",
      "Max gradient at step 1578: 0.205440491437912\n",
      "Max gradient at step 1579: 0.07820657640695572\n",
      "Max gradient at step 1580: 0.21765942871570587\n",
      "Max gradient at step 1581: 0.1456347405910492\n",
      "Max gradient at step 1582: 0.10518871247768402\n",
      "Max gradient at step 1583: 0.08244989067316055\n",
      "Max gradient at step 1584: 0.18409906327724457\n",
      "Max gradient at step 1585: 0.22125649452209473\n",
      "Max gradient at step 1586: 0.17380401492118835\n",
      "Max gradient at step 1587: 0.21527458727359772\n",
      "Max gradient at step 1588: 0.16534143686294556\n",
      "Max gradient at step 1589: 0.08566893637180328\n",
      "Max gradient at step 1590: 0.19937211275100708\n",
      "Max gradient at step 1591: 0.22069665789604187\n",
      "Max gradient at step 1592: 0.2563827335834503\n",
      "Max gradient at step 1593: 0.17940838634967804\n",
      "Max gradient at step 1594: 0.25586938858032227\n",
      "Max gradient at step 1595: 0.11625628918409348\n",
      "Max gradient at step 1596: 0.08663278073072433\n",
      "Max gradient at step 1597: 0.12988559901714325\n",
      "Max gradient at step 1598: 0.2109338343143463\n",
      "Max gradient at step 1599: 0.19444997608661652\n",
      "Max gradient at step 1600: 0.2289884388446808\n",
      "Training loss (for one batch) at step 1600: 0.2389\n",
      "Max gradient at step 1601: 0.14852845668792725\n",
      "Max gradient at step 1602: 0.3183944523334503\n",
      "Max gradient at step 1603: 0.2314380705356598\n",
      "Max gradient at step 1604: 0.21970412135124207\n",
      "Max gradient at step 1605: 0.19653312861919403\n",
      "Max gradient at step 1606: 0.3491116166114807\n",
      "Max gradient at step 1607: 0.15910547971725464\n",
      "Max gradient at step 1608: 0.2012309730052948\n",
      "Max gradient at step 1609: 0.15450872480869293\n",
      "Max gradient at step 1610: 0.2513258159160614\n",
      "Max gradient at step 1611: 0.10632215440273285\n",
      "Max gradient at step 1612: 0.12710003554821014\n",
      "Max gradient at step 1613: 0.12649089097976685\n",
      "Max gradient at step 1614: 0.207615464925766\n",
      "Max gradient at step 1615: 0.09024286270141602\n",
      "Max gradient at step 1616: 0.1846122145652771\n",
      "Max gradient at step 1617: 0.19442495703697205\n",
      "Max gradient at step 1618: 0.0970357283949852\n",
      "Max gradient at step 1619: 0.1978216916322708\n",
      "Max gradient at step 1620: 0.2092565894126892\n",
      "Max gradient at step 1621: 0.16019652783870697\n",
      "Max gradient at step 1622: 0.13706018030643463\n",
      "Max gradient at step 1623: 0.20213483273983002\n",
      "Max gradient at step 1624: 0.17453712224960327\n",
      "Max gradient at step 1625: 0.22051161527633667\n",
      "Max gradient at step 1626: 0.10265503078699112\n",
      "Max gradient at step 1627: 0.30476605892181396\n",
      "Max gradient at step 1628: 0.17198072373867035\n",
      "Max gradient at step 1629: 0.21059417724609375\n",
      "Max gradient at step 1630: 0.22033804655075073\n",
      "Max gradient at step 1631: 0.28397706151008606\n",
      "Max gradient at step 1632: 0.17788371443748474\n",
      "Max gradient at step 1633: 0.1422344297170639\n",
      "Max gradient at step 1634: 0.17286570370197296\n",
      "Max gradient at step 1635: 0.08000677824020386\n",
      "Max gradient at step 1636: 0.23279689252376556\n",
      "Max gradient at step 1637: 0.20106256008148193\n",
      "Max gradient at step 1638: 0.12546856701374054\n",
      "Max gradient at step 1639: 0.15394726395606995\n",
      "Max gradient at step 1640: 0.1899702399969101\n",
      "Max gradient at step 1641: 0.11836156994104385\n",
      "Max gradient at step 1642: 0.06521182507276535\n",
      "Max gradient at step 1643: 0.1444564014673233\n",
      "Max gradient at step 1644: 0.16932260990142822\n",
      "Max gradient at step 1645: 0.17259952425956726\n",
      "Max gradient at step 1646: 0.16077719628810883\n",
      "Max gradient at step 1647: 0.09848402440547943\n",
      "Max gradient at step 1648: 0.14271292090415955\n",
      "Max gradient at step 1649: 0.17553992569446564\n",
      "Max gradient at step 1650: 0.11995717883110046\n",
      "Max gradient at step 1651: 0.07510101795196533\n",
      "Max gradient at step 1652: 0.16601334512233734\n",
      "Max gradient at step 1653: 0.10186111927032471\n",
      "Max gradient at step 1654: 0.1750391572713852\n",
      "Max gradient at step 1655: 0.17909111082553864\n",
      "Max gradient at step 1656: 0.19307579100131989\n",
      "Max gradient at step 1657: 0.1487560272216797\n",
      "Max gradient at step 1658: 0.4048595130443573\n",
      "Max gradient at step 1659: 0.09141143411397934\n",
      "Max gradient at step 1660: 0.1564060002565384\n",
      "Max gradient at step 1661: 0.11741269379854202\n",
      "Max gradient at step 1662: 0.25056734681129456\n",
      "Max gradient at step 1663: 0.19170889258384705\n",
      "Max gradient at step 1664: 0.10968206077814102\n",
      "Max gradient at step 1665: 0.2512281835079193\n",
      "Max gradient at step 1666: 0.20873238146305084\n",
      "Max gradient at step 1667: 0.14420093595981598\n",
      "Max gradient at step 1668: 0.15365980565547943\n",
      "Max gradient at step 1669: 0.0855504646897316\n",
      "Max gradient at step 1670: 0.1595347821712494\n",
      "Max gradient at step 1671: 0.17903287708759308\n",
      "Max gradient at step 1672: 0.19201377034187317\n",
      "Max gradient at step 1673: 0.16483546793460846\n",
      "Max gradient at step 1674: 0.18511082231998444\n",
      "Max gradient at step 1675: 0.1521204710006714\n",
      "Max gradient at step 1676: 0.1969633847475052\n",
      "Max gradient at step 1677: 0.14029759168624878\n",
      "Max gradient at step 1678: 0.11797039210796356\n",
      "Max gradient at step 1679: 0.20749253034591675\n",
      "Max gradient at step 1680: 0.2164003998041153\n",
      "Max gradient at step 1681: 0.13654287159442902\n",
      "Max gradient at step 1682: 0.21242763102054596\n",
      "Max gradient at step 1683: 0.12874889373779297\n",
      "Max gradient at step 1684: 0.2689058184623718\n",
      "Max gradient at step 1685: 0.19191831350326538\n",
      "Max gradient at step 1686: 0.24972079694271088\n",
      "Max gradient at step 1687: 0.15687726438045502\n",
      "Max gradient at step 1688: 0.13466119766235352\n",
      "Max gradient at step 1689: 0.2335553914308548\n",
      "Max gradient at step 1690: 0.2043692022562027\n",
      "Max gradient at step 1691: 0.10540719330310822\n",
      "Max gradient at step 1692: 0.20853686332702637\n",
      "Max gradient at step 1693: 0.25050047039985657\n",
      "Max gradient at step 1694: 0.10075371712446213\n",
      "Max gradient at step 1695: 0.1532294899225235\n",
      "Max gradient at step 1696: 0.1640031486749649\n",
      "Max gradient at step 1697: 0.15385034680366516\n",
      "Max gradient at step 1698: 0.21275919675827026\n",
      "Max gradient at step 1699: 0.18494899570941925\n",
      "Max gradient at step 1700: 0.16159257292747498\n",
      "Training loss (for one batch) at step 1700: 0.2876\n",
      "Max gradient at step 1701: 0.10170285403728485\n",
      "Max gradient at step 1702: 0.1656949371099472\n",
      "Max gradient at step 1703: 0.15715903043746948\n",
      "Max gradient at step 1704: 0.19858971238136292\n",
      "Max gradient at step 1705: 0.04795370250940323\n",
      "Max gradient at step 1706: 0.2721768915653229\n",
      "Max gradient at step 1707: 0.1291634738445282\n",
      "Max gradient at step 1708: 0.12484778463840485\n",
      "Max gradient at step 1709: 0.2056894302368164\n",
      "Max gradient at step 1710: 0.2754817605018616\n",
      "Max gradient at step 1711: 0.1083918884396553\n",
      "Max gradient at step 1712: 0.25529947876930237\n",
      "Max gradient at step 1713: 0.07813698053359985\n",
      "Max gradient at step 1714: 0.21041280031204224\n",
      "Max gradient at step 1715: 0.2018756866455078\n",
      "Max gradient at step 1716: 0.1619512438774109\n",
      "Max gradient at step 1717: 0.19777239859104156\n",
      "Max gradient at step 1718: 0.14670473337173462\n",
      "Max gradient at step 1719: 0.20763160288333893\n",
      "Max gradient at step 1720: 0.12390273809432983\n",
      "Max gradient at step 1721: 0.11511894315481186\n",
      "Max gradient at step 1722: 0.16433702409267426\n",
      "Max gradient at step 1723: 0.20201842486858368\n",
      "Max gradient at step 1724: 0.20957334339618683\n",
      "Max gradient at step 1725: 0.19073379039764404\n",
      "Max gradient at step 1726: 0.07929471135139465\n",
      "Max gradient at step 1727: 0.11232703924179077\n",
      "Max gradient at step 1728: 0.09611359983682632\n",
      "Max gradient at step 1729: 0.13690339028835297\n",
      "Max gradient at step 1730: 0.19630278646945953\n",
      "Max gradient at step 1731: 0.18508103489875793\n",
      "Max gradient at step 1732: 0.18146207928657532\n",
      "Max gradient at step 1733: 0.2011173814535141\n",
      "Max gradient at step 1734: 0.14796023070812225\n",
      "Max gradient at step 1735: 0.19325785338878632\n",
      "Max gradient at step 1736: 0.15536238253116608\n",
      "Max gradient at step 1737: 0.10149175673723221\n",
      "Max gradient at step 1738: 0.11017909646034241\n",
      "Max gradient at step 1739: 0.11580708622932434\n",
      "Max gradient at step 1740: 0.24279667437076569\n",
      "Max gradient at step 1741: 0.28572964668273926\n",
      "Max gradient at step 1742: 0.18739955127239227\n",
      "Max gradient at step 1743: 0.11046359688043594\n",
      "Max gradient at step 1744: 0.2824701964855194\n",
      "Max gradient at step 1745: 0.1286509931087494\n",
      "Max gradient at step 1746: 0.11427626013755798\n",
      "Max gradient at step 1747: 0.19195298850536346\n",
      "Max gradient at step 1748: 0.08056103438138962\n",
      "Max gradient at step 1749: 0.1172620877623558\n",
      "Max gradient at step 1750: 0.34846392273902893\n",
      "Max gradient at step 1751: 0.3059525191783905\n",
      "Max gradient at step 1752: 0.1615600436925888\n",
      "Max gradient at step 1753: 0.21438826620578766\n",
      "Max gradient at step 1754: 0.14698129892349243\n",
      "Max gradient at step 1755: 0.19718903303146362\n",
      "Max gradient at step 1756: 0.14135003089904785\n",
      "Max gradient at step 1757: 0.1467529535293579\n",
      "Max gradient at step 1758: 0.14321796596050262\n",
      "Max gradient at step 1759: 0.1361909806728363\n",
      "Max gradient at step 1760: 0.1717909723520279\n",
      "Max gradient at step 1761: 0.13370463252067566\n",
      "Max gradient at step 1762: 0.30933117866516113\n",
      "Max gradient at step 1763: 0.17861486971378326\n",
      "Max gradient at step 1764: 0.14133204519748688\n",
      "Max gradient at step 1765: 0.07658152282238007\n",
      "Max gradient at step 1766: 0.26092350482940674\n",
      "Max gradient at step 1767: 0.14073407649993896\n",
      "Max gradient at step 1768: 0.20410095155239105\n",
      "Max gradient at step 1769: 0.12724512815475464\n",
      "Max gradient at step 1770: 0.1436842381954193\n",
      "Max gradient at step 1771: 0.16585832834243774\n",
      "Max gradient at step 1772: 0.18059512972831726\n",
      "Max gradient at step 1773: 0.1384734809398651\n",
      "Max gradient at step 1774: 0.12946946918964386\n",
      "Max gradient at step 1775: 0.1359586864709854\n",
      "Max gradient at step 1776: 0.18760333955287933\n",
      "Max gradient at step 1777: 0.1273636817932129\n",
      "Max gradient at step 1778: 0.12062109261751175\n",
      "Max gradient at step 1779: 0.20503747463226318\n",
      "Max gradient at step 1780: 0.10852091014385223\n",
      "Max gradient at step 1781: 0.08781418204307556\n",
      "Max gradient at step 1782: 0.1107512041926384\n",
      "Max gradient at step 1783: 0.15130087733268738\n",
      "Max gradient at step 1784: 0.12415473163127899\n",
      "Max gradient at step 1785: 0.14514942467212677\n",
      "Max gradient at step 1786: 0.18486742675304413\n",
      "Max gradient at step 1787: 0.2427276223897934\n",
      "Max gradient at step 1788: 0.1426348239183426\n",
      "Max gradient at step 1789: 0.10388082265853882\n",
      "Max gradient at step 1790: 0.25355640053749084\n",
      "Max gradient at step 1791: 0.11386580765247345\n",
      "Max gradient at step 1792: 0.11468012630939484\n",
      "Max gradient at step 1793: 0.21570104360580444\n",
      "Max gradient at step 1794: 0.32808566093444824\n",
      "Max gradient at step 1795: 0.12408734112977982\n",
      "Max gradient at step 1796: 0.193599134683609\n",
      "Max gradient at step 1797: 0.16575708985328674\n",
      "Max gradient at step 1798: 0.12224818766117096\n",
      "Max gradient at step 1799: 0.11324319988489151\n",
      "Max gradient at step 1800: 0.1638287603855133\n",
      "Training loss (for one batch) at step 1800: 0.1768\n",
      "Max gradient at step 1801: 0.1576625555753708\n",
      "Max gradient at step 1802: 0.07265397161245346\n",
      "Max gradient at step 1803: 0.12759681046009064\n",
      "Max gradient at step 1804: 0.14332856237888336\n",
      "Max gradient at step 1805: 0.10695241391658783\n",
      "Max gradient at step 1806: 0.1253034770488739\n",
      "Max gradient at step 1807: 0.15387603640556335\n",
      "Max gradient at step 1808: 0.12706024944782257\n",
      "Max gradient at step 1809: 0.1000601127743721\n",
      "Max gradient at step 1810: 0.18633458018302917\n",
      "Max gradient at step 1811: 0.11006578803062439\n",
      "Max gradient at step 1812: 0.28604990243911743\n",
      "Max gradient at step 1813: 0.21249935030937195\n",
      "Max gradient at step 1814: 0.24577821791172028\n",
      "Max gradient at step 1815: 0.08971131592988968\n",
      "Max gradient at step 1816: 0.1936315894126892\n",
      "Max gradient at step 1817: 0.06527360528707504\n",
      "Max gradient at step 1818: 0.07901293784379959\n",
      "Max gradient at step 1819: 0.13333886861801147\n",
      "Max gradient at step 1820: 0.06178368255496025\n",
      "Max gradient at step 1821: 0.13898257911205292\n",
      "Max gradient at step 1822: 0.10998792201280594\n",
      "Max gradient at step 1823: 0.11908382177352905\n",
      "Max gradient at step 1824: 0.22087815403938293\n",
      "Max gradient at step 1825: 0.24913500249385834\n",
      "Max gradient at step 1826: 0.20250512659549713\n",
      "Max gradient at step 1827: 0.11162970960140228\n",
      "Max gradient at step 1828: 0.05697887763381004\n",
      "Max gradient at step 1829: 0.11606478691101074\n",
      "Max gradient at step 1830: 0.12664887309074402\n",
      "Max gradient at step 1831: 0.18893906474113464\n",
      "Max gradient at step 1832: 0.14880117774009705\n",
      "Max gradient at step 1833: 0.12452886253595352\n",
      "Max gradient at step 1834: 0.13876956701278687\n",
      "Max gradient at step 1835: 0.16176491975784302\n",
      "Max gradient at step 1836: 0.1044512465596199\n",
      "Max gradient at step 1837: 0.22518949210643768\n",
      "Max gradient at step 1838: 0.05425303056836128\n",
      "Max gradient at step 1839: 0.2422173172235489\n",
      "Max gradient at step 1840: 0.09559285640716553\n",
      "Max gradient at step 1841: 0.05687448009848595\n",
      "Max gradient at step 1842: 0.16897690296173096\n",
      "Max gradient at step 1843: 0.05038950592279434\n",
      "Max gradient at step 1844: 0.10183452814817429\n",
      "Max gradient at step 1845: 0.1240064799785614\n",
      "Max gradient at step 1846: 0.14684654772281647\n",
      "Max gradient at step 1847: 0.1935204565525055\n",
      "Max gradient at step 1848: 0.14775222539901733\n",
      "Max gradient at step 1849: 0.13094158470630646\n",
      "Max gradient at step 1850: 0.22630849480628967\n",
      "Max gradient at step 1851: 0.10869315266609192\n",
      "Max gradient at step 1852: 0.10994632542133331\n",
      "Max gradient at step 1853: 0.08174443244934082\n",
      "Max gradient at step 1854: 0.11251373589038849\n",
      "Max gradient at step 1855: 0.13080857694149017\n",
      "Max gradient at step 1856: 0.12139163911342621\n",
      "Max gradient at step 1857: 0.23355765640735626\n",
      "Max gradient at step 1858: 0.07978923618793488\n",
      "Max gradient at step 1859: 0.10718904435634613\n",
      "Max gradient at step 1860: 0.19720889627933502\n",
      "Max gradient at step 1861: 0.2111537754535675\n",
      "Max gradient at step 1862: 0.03834473341703415\n",
      "Max gradient at step 1863: 0.3696598410606384\n",
      "Max gradient at step 1864: 0.1741252839565277\n",
      "Max gradient at step 1865: 0.17452794313430786\n",
      "Max gradient at step 1866: 0.0659489557147026\n",
      "Max gradient at step 1867: 0.11762595921754837\n",
      "Max gradient at step 1868: 0.11568069458007812\n",
      "Max gradient at step 1869: 0.19061322510242462\n",
      "Max gradient at step 1870: 0.10028930008411407\n",
      "Max gradient at step 1871: 0.07864947617053986\n",
      "Max gradient at step 1872: 0.12248161435127258\n",
      "Max gradient at step 1873: 0.09743309766054153\n",
      "Max gradient at step 1874: 0.21678389608860016\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)  # Example batch size\n",
    "\n",
    "# Define the model architecture.\n",
    "model_test_2 = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "epochs = 4\n",
    "accumulators = train_with_gradient_accumulation(model_test_2\n",
    "                                                , train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = prune_model_based_on_gradients(model_test_2, accumulators, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1d4461a73a0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_weights_mask = [tf.not_equal(w, 0.0) for w in model_acc.get_weights()]\n",
    "retrain_model(model_acc, train_dataset, pruned_weights_mask, epochs=2, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9319999814033508\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "conv2d_1, Conv2D, 53\n",
      "dense_1, Dense, 2885\n",
      "Total non-zero weights in the model: 2938\n",
      "Non-zero weights in the model: None\n"
     ]
    }
   ],
   "source": [
    "model_acc.compile(optimizer='sgd',  # Use the same optimizer as used in training\n",
    "                loss='sparse_categorical_crossentropy',  # Use the appropriate loss function\n",
    "                metrics=['accuracy']) \n",
    "_, baseline_model_acc_accuracy = model_acc.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_acc_accuracy)\n",
    "print('Non-zero weights in the model:', non_zero_weights_summary(model_acc))\n",
    "pruned_keras_file = r'C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\grad.h5'\n",
    "keras.models.save_model(model_acc, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Network for the MCU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "396/396 [==============================] - 3s 6ms/step - loss: 0.0628 - accuracy: 0.9915\n",
      "Epoch 2/4\n",
      "396/396 [==============================] - 2s 6ms/step - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 3/4\n",
      "396/396 [==============================] - 2s 6ms/step - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 4/4\n",
      "396/396 [==============================] - 2s 5ms/step - loss: 0.0025 - accuracy: 0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpp2g7xfck\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpp2g7xfck\\assets\n",
      "C:\\Users\\Sever\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Filter the dataset to keep only digits '0' and '1'\n",
    "train_filter = np.where((train_labels == 0) | (train_labels == 1))\n",
    "test_filter = np.where((test_labels == 0) | (test_labels == 1))\n",
    "\n",
    "train_images, train_labels = train_images[train_filter], train_labels[train_filter]\n",
    "test_images, test_labels = test_images[test_filter], test_labels[test_filter]\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1\n",
    "# Cast to float32 to ensure data type consistency\n",
    "train_images = (train_images / 255.0).astype(np.float32)\n",
    "test_images = (test_images / 255.0).astype(np.float32)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.expand_dims(x, -1), y))  # Reshape to include the channel dimension\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# Compile the model with a binary classification loss function\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=4)\n",
    "\n",
    "# Representative dataset to enable quantization\n",
    "def representative_dataset():\n",
    "    for data in train_dataset.take(100):  # 100 batches of the dataset\n",
    "        yield [data[0].numpy()]\n",
    "\n",
    "# Convert the Keras model to a TensorFlow Lite model with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a file\n",
    "tflite_model_name = 'mnist_binary_classifier'\n",
    "with open(tflite_model_name + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 13, 13, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_20 (Flatten)        (None, 1352)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 1353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.9991\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "score = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'test'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_20_input:0\n",
      "shape: [ 1 28 28  1]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [1 1]\n",
      "type: <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'tflite_interpreter' is correctly loaded and initialized\n",
    "\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "# Allocate tensors once outside the loop\n",
    "tflite_interpreter.allocate_tensors()\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(test_images)):\n",
    "    # Get the test image and preprocess it\n",
    "    test_image = test_images[i]\n",
    "    test_image = np.expand_dims(test_image, axis=-1)  # Make sure it's [28, 28, 1]\n",
    "    test_image = test_image.astype(np.float32)  # Ensure the type matches the expected float32\n",
    "\n",
    "    # Normalize and quantize the image\n",
    "    input_scale, input_zero_point = input_details[0]['quantization']\n",
    "    # test_image = test_image #/ 255.0  # Normalize the image from [0, 255] to [0, 1]\n",
    "    # test_image = test_image / input_scale + input_zero_point\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[0]['dtype'])  # Reshape to [1, 28, 28, 1]\n",
    "\n",
    "    # Set the tensor to the input of the model\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "    # Run the model on the input data\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    # Retrieve the output of the model\n",
    "    output_data = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions.append(output_data[0])\n",
    "\n",
    "# Handle predictions as needed (e.g., post-processing, thresholding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of quantized to int8 model is 0.0%\n",
      "Compared to float32 accuracy of 99.90543723106384%\n",
      "We have a change of -99.90543723106384%\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 100\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))\n",
    "print(\"Compared to float32 accuracy of {}%\".format(score[1]*100))\n",
    "print(\"We have a change of {}%\".format((accuracy_score-score[1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000219A68EE280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000219A68EE280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicted label: 1 (1 for '1' and 0 for '0')\n",
      "Actual label: 1\n"
     ]
    }
   ],
   "source": [
    "# Select an image from the test set\n",
    "index = 0  # Change this index to test different images\n",
    "image_to_test = test_images[index]\n",
    "\n",
    "# Preprocess the image\n",
    "image_to_test = image_to_test / 255.0  # Normalize the image\n",
    "image_to_test = np.expand_dims(image_to_test, axis=-1)  # Add channel dimension\n",
    "image_to_test = np.expand_dims(image_to_test, axis=0)  # Add batch dimension (model expects a batch)\n",
    "\n",
    "# Perform inference\n",
    "predictions = model.predict(image_to_test)\n",
    "predicted_label = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Print the prediction result\n",
    "print(f\"Predicted label: {predicted_label[0][0]} (1 for '1' and 0 for '0')\")\n",
    "print(f\"Actual label: {test_labels[index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (784,)\n",
      "Data Type: float64\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "# Select a random image from the training set\n",
    "random_idx = np.random.randint(0, len(train_images))\n",
    "random_image = train_images[random_idx]\n",
    "\n",
    "# Since the image needs to be a flat array of floats for TFLite Micro,\n",
    "# we flatten the 28x28 image into a 1D array of 784 elements\n",
    "flat_image = random_image.flatten()\n",
    "\n",
    "# Optionally, print the image shape and data type to confirm\n",
    "print(\"Shape:\", flat_image.shape)\n",
    "print(\"Data Type:\", flat_image.dtype)\n",
    "\n",
    "# Convert image data to float32 if not already\n",
    "flat_image = flat_image.astype('float32')\n",
    "\n",
    "\n",
    "def save_image_to_c_array(flat_image, filename=\"mnist_image.h\"):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"static const float mnist_image[784] = {\\n\")\n",
    "        for i, pixel in enumerate(flat_image):\n",
    "            file.write(f\"{pixel:.6f}\")\n",
    "            if i < len(flat_image) - 1:\n",
    "                file.write(\", \")\n",
    "            if (i + 1) % 10 == 0:\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n};\\n\")\n",
    "\n",
    "# Call the function to save the image\n",
    "save_image_to_c_array(flat_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'test'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_mag, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "int_image = np.round(flat_image * 255)\n",
    "\n",
    "# Now convert from [0, 255] to [-128, 127]\n",
    "int_image = (int_image - 128).astype(np.int8)\n",
    "\n",
    "# Function to save the array to a C header file\n",
    "def save_image_to_c_array(int_image, filename=\"mnist_image.h\"):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"signed char mnist_image[784] = {\\n\")\n",
    "        for i in range(0, len(int_image), 12):  # Process in chunks of 12 for formatting\n",
    "            line = \", \".join(f\"{int(x)}\" for x in int_image[i:i+12])\n",
    "            if i + 12 < len(int_image):\n",
    "                file.write(\"    \" + line + \",\\n\")\n",
    "            else:\n",
    "                file.write(\"    \" + line + \"\\n\")\n",
    "        file.write(\"};\\n\")\n",
    "\n",
    "save_image_to_c_array(int_image)  # Call the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply techniques to standard networks VGGNet and AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_8[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " predictions (Dense)            (None, 1000)         1281000     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,538,984\n",
      "Trainable params: 3,504,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "model_mobile = MobileNetV2(weights='imagenet', include_top=True)\n",
    "\n",
    "model_mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "train_model(model_mobile, train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8747K9OE72P"
   },
   "source": [
    "## Fine-tune pre-trained model with pruning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F19k7ExXF_h2"
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsZROpNYMWQ0"
   },
   "source": [
    "You will apply pruning to the whole model and see this in the model summary.\n",
    "\n",
    "In this example, you start the model with 50% sparsity (50% zeros in weights)\n",
    "and end with 80% sparsity.\n",
    "\n",
    "In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md), you can see how to prune some layers for model accuracy improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:47.538806Z",
     "iopub.status.busy": "2024-03-09T12:16:47.538113Z",
     "iopub.status.idle": "2024-03-09T12:16:48.436692Z",
     "shell.execute_reply": "2024-03-09T12:16:48.435955Z"
    },
    "id": "oq6blGjgFDCW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
      " _3 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 26, 26, 12)       230       \n",
      " 3 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 13, 13, 12)       1         \n",
      " ling2d_3 (PruneLowMagnitude                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 2028)             1         \n",
      " _3 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_3  (None, 10)               40572     \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,805\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 20,395\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Train a network as usual\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)  # Example batch size\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDr2ijwpGCI-"
   },
   "source": [
    "### Train and evaluate the model against baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUBEn94hXYB1"
   },
   "source": [
    "Fine tune with pruning for two epochs.\n",
    "\n",
    "`tfmot.sparsity.keras.UpdatePruningStep` is required during training, and `tfmot.sparsity.keras.PruningSummaries` provides logs for tracking progress and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:48.447611Z",
     "iopub.status.busy": "2024-03-09T12:16:48.447021Z",
     "iopub.status.idle": "2024-03-09T12:16:56.505215Z",
     "shell.execute_reply": "2024-03-09T12:16:56.504470Z"
    },
    "id": "_PHDGJryE31X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "422/422 [==============================] - 8s 14ms/step - loss: 0.5358 - accuracy: 0.8612 - val_loss: 0.2836 - val_accuracy: 0.9263\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 6s 13ms/step - loss: 0.3003 - accuracy: 0.9147 - val_loss: 0.2179 - val_accuracy: 0.9362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219a0c4cfa0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "  \n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-byC2lYlMkfN"
   },
   "source": [
    "For this example, there is minimal loss in test accuracy after pruning, compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:56.509040Z",
     "iopub.status.busy": "2024-03-09T12:16:56.508400Z",
     "iopub.status.idle": "2024-03-09T12:16:57.138578Z",
     "shell.execute_reply": "2024-03-09T12:16:57.137789Z"
    },
    "id": "6bMFTKSSHyyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned test accuracy: 0.9276000261306763\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "# print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQFiZsqqc0vS"
   },
   "source": [
    "The logs show the progression of sparsity on a per-layer basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "LdLm4hVYc1wx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1UFCxxSIIf5"
   },
   "source": [
    "For non-Colab users, you can see [the results of a previous run](https://tensorboard.dev/experiment/sRQnrycaTMWQOaswXzClYA/#scalars&_smoothingWeight=0) of this code block on [TensorBoard.dev](https://tensorboard.dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IepmUPSITn6"
   },
   "source": [
    "## Create 3x smaller models from pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FgNP4rbOLH8"
   },
   "source": [
    "Both `tfmot.sparsity.keras.strip_pruning` and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression\n",
    "benefits of pruning.\n",
    "\n",
    "*   `strip_pruning` is necessary since it removes every tf.Variable that pruning only needs during training, which would otherwise add to model size during inference\n",
    "*   Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning. However, pruning makes most of the weights zeros, which is\n",
    "added redundancy that algorithms can utilize to further compress the model.\n",
    "\n",
    "First, create a compressible model for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:57.142877Z",
     "iopub.status.busy": "2024-03-09T12:16:57.142284Z",
     "iopub.status.idle": "2024-03-09T12:16:57.180273Z",
     "shell.execute_reply": "2024-03-09T12:16:57.179584Z"
    },
    "id": "w7fztWsAOHTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "pruned_keras_file = r'C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\default.h5'\n",
    "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4_bixlrtTbF"
   },
   "source": [
    "Then, create a compressible model for TFLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:57.183743Z",
     "iopub.status.busy": "2024-03-09T12:16:57.183225Z",
     "iopub.status.idle": "2024-03-09T12:16:58.047214Z",
     "shell.execute_reply": "2024-03-09T12:16:58.046323Z"
    },
    "id": "uIKxSSHmrJSa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpmvxstglm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpmvxstglm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\MFCC.tflite\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file = r'C:\\Users\\Sever\\ML_on_MCU\\Pruning\\models\\MFCC.tflite'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4lv-lRKuMY4"
   },
   "source": [
    "Define a helper function to actually compress the models via gzip and measure the zipped size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'MFCC'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_mag, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:58.051167Z",
     "iopub.status.busy": "2024-03-09T12:16:58.050467Z",
     "iopub.status.idle": "2024-03-09T12:16:58.055122Z",
     "shell.execute_reply": "2024-03-09T12:16:58.054382Z"
    },
    "id": "-E7DXEgUrCDZ"
   },
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caxUoJ_BunqU"
   },
   "source": [
    "Compare and see that the models are 3x smaller from pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:58.058544Z",
     "iopub.status.busy": "2024-03-09T12:16:58.058013Z",
     "iopub.status.idle": "2024-03-09T12:16:58.079078Z",
     "shell.execute_reply": "2024-03-09T12:16:58.078320Z"
    },
    "id": "HzSXC_nxuqJX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gzipped baseline Keras model: 78239.00 bytes\n",
      "Size of gzipped pruned Keras model: 25908.00 bytes\n",
      "Size of gzipped pruned TFlite model: 24848.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8D7WnFF5DZR"
   },
   "source": [
    "## Create a 10x smaller model from combining pruning and quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1c2IecBRCdQ"
   },
   "source": [
    "You can apply post-training quantization to the pruned model for additional benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:58.082718Z",
     "iopub.status.busy": "2024-03-09T12:16:58.082172Z",
     "iopub.status.idle": "2024-03-09T12:16:59.057139Z",
     "shell.execute_reply": "2024-03-09T12:16:59.056365Z"
    },
    "id": "jy_Lgfh8VkyX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmpfs/tmp/tmp35zwmyql/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmpfs/tmp/tmp35zwmyql/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1709986618.942100   10506 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1709986618.942130   10506 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized and pruned TFLite model to: /tmpfs/tmp/tmp3v6lm0h4.tflite\n",
      "Size of gzipped baseline Keras model: 78239.00 bytes\n",
      "Size of gzipped pruned and quantized TFlite model: 8064.00 bytes\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEYsyYVqNgeY"
   },
   "source": [
    "## See persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saadXD4JQsBK"
   },
   "source": [
    "Define a helper function to evaluate the TF Lite model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:59.061170Z",
     "iopub.status.busy": "2024-03-09T12:16:59.060475Z",
     "iopub.status.idle": "2024-03-09T12:16:59.066974Z",
     "shell.execute_reply": "2024-03-09T12:16:59.066288Z"
    },
    "id": "b8yBouuGNqls"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on ever y image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuEFS4CIQvUw"
   },
   "source": [
    "You evaluate the pruned and quantized model and see that the accuracy from TensorFlow persists to the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T12:16:59.070337Z",
     "iopub.status.busy": "2024-03-09T12:16:59.069797Z",
     "iopub.status.idle": "2024-03-09T12:17:00.139241Z",
     "shell.execute_reply": "2024-03-09T12:17:00.138199Z"
    },
    "id": "VqQTyqz4NsWd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#13 is a dynamic-sized tensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Pruned and quantized TFLite test_accuracy: 0.9691\n",
      "Pruned TF test accuracy: 0.9686999917030334\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O5xuci-SonI"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2I7xmyMW5QY"
   },
   "source": [
    "In this tutorial, you saw how to create sparse models with the TensorFlow Model Optimization Toolkit API for both TensorFlow and TFLite. You \n",
    "then combined pruning with post-training quantization for additional benefits.\n",
    "\n",
    "You created a 10x smaller model for MNIST, with minimal accuracy difference.\n",
    "\n",
    "We encourage you to try this new capability, which can be particularly important for deployment in resource-constrained environments.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pruning_with_keras.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
