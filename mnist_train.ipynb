{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4239 - accuracy: 0.8845 - val_loss: 0.2592 - val_accuracy: 0.9266\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2447 - accuracy: 0.9315 - val_loss: 0.2143 - val_accuracy: 0.9384\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2100 - accuracy: 0.9402 - val_loss: 0.1973 - val_accuracy: 0.9407\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1886 - accuracy: 0.9454 - val_loss: 0.1849 - val_accuracy: 0.9448\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1702 - accuracy: 0.9506 - val_loss: 0.1703 - val_accuracy: 0.9502\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1552 - accuracy: 0.9554 - val_loss: 0.1595 - val_accuracy: 0.9546\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1440 - accuracy: 0.9582 - val_loss: 0.1607 - val_accuracy: 0.9521\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1349 - accuracy: 0.9603 - val_loss: 0.1518 - val_accuracy: 0.9570\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1276 - accuracy: 0.9627 - val_loss: 0.1435 - val_accuracy: 0.9578\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1212 - accuracy: 0.9647 - val_loss: 0.1419 - val_accuracy: 0.9580\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9580\n",
      "Test accuracy: 0.9580000042915344\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "x_train = x_train.reshape((-1, 784))  # Flatten the images\n",
    "x_test = x_test.reshape((-1, 784))    # Flatten the images\n",
    "\n",
    "# Create a simple linear neural network model for image classification\n",
    "model = models.Sequential([\n",
    "    layers.Dense(20, input_shape=(784,), activation='relu', use_bias=False),  # Linear layer without activation\n",
    "    layers.Dense(10, activation='softmax', use_bias=False)  # Output layer with 10 classes for digits 0-9\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat GPT model\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)) / 255.0\n",
    "x_test = x_test.reshape((-1, 28, 28, 1)) / 255.0\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split training data to training and validation\n",
    "x_train, x_val = x_train[:50000], x_train[50000:]\n",
    "y_train, y_val = y_train[:50000], y_train[50000:]\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 25s 14ms/step - loss: 0.5156 - accuracy: 0.8378 - val_loss: 0.1369 - val_accuracy: 0.9613\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.1660 - accuracy: 0.9520\n",
      "Test accuracy: 0.9520000219345093\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_vit_classifier():\n",
    "    num_patches = (28 // 4) * (28 // 4)  # 7x7 patches, 49 total\n",
    "    patch_dim = 4 * 4 * 1  # Each patch is 4x4 pixels, 1 grayscale channel\n",
    "    num_heads = 2  # Reduced from 4 to 2\n",
    "    transformer_layers = 1  # Reduced from 4 to 2\n",
    "    projection_dim = 32  # Reduced from 64 to 32\n",
    "\n",
    "    inputs = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "    # Break image into patches and flatten\n",
    "    patches = layers.Conv2D(filters=projection_dim, kernel_size=4, strides=4, padding='valid')(inputs)\n",
    "    patches = layers.Reshape((num_patches, projection_dim))(patches)  # Flatten patches\n",
    "\n",
    "    # Transformer Encoder\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(patches)\n",
    "        # Multi-head attention\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, patches])\n",
    "        # Layer normalization 2\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = layers.Dense(units=projection_dim, activation=tf.nn.gelu)(x3)  # Simplified to match projection_dim\n",
    "        x3 = layers.Dropout(0.1)(x3)\n",
    "        # Skip connection 2\n",
    "        patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Classification head\n",
    "    sequence_representation = layers.LayerNormalization(epsilon=1e-6)(patches)\n",
    "    sequence_representation = layers.Flatten()(sequence_representation)\n",
    "    sequence_representation = layers.Dropout(0.5)(sequence_representation)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(sequence_representation)\n",
    "\n",
    "    # Build model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)) / 255.0\n",
    "x_test = x_test.reshape((-1, 28, 28, 1)) / 255.0\n",
    "\n",
    "# Create and compile the ViT model\n",
    "model = create_vit_classifier()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, epochs=1, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmplzaf46a9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmplzaf46a9\\assets\n",
      "C:\\Users\\Sever\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = 'transformer_mnist'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "            # Ensure the input is cast to float32, as expected by the TensorFlow model\n",
    "            yield [tf.cast(input_value, dtype=tf.float32)]\n",
    "\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 30s 18ms/step - loss: 0.2384 - accuracy: 0.9259 - val_loss: 0.0637 - val_accuracy: 0.9818\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 31s 18ms/step - loss: 0.0852 - accuracy: 0.9746 - val_loss: 0.0591 - val_accuracy: 0.9853\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 31s 18ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 0.0605 - val_accuracy: 0.9840\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 32s 19ms/step - loss: 0.0480 - accuracy: 0.9848 - val_loss: 0.0483 - val_accuracy: 0.9853\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 34s 20ms/step - loss: 0.0388 - accuracy: 0.9874 - val_loss: 0.0571 - val_accuracy: 0.9873\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 33s 20ms/step - loss: 0.0332 - accuracy: 0.9892 - val_loss: 0.0524 - val_accuracy: 0.9893\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 32s 19ms/step - loss: 0.0312 - accuracy: 0.9899 - val_loss: 0.0639 - val_accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 33s 20ms/step - loss: 0.0280 - accuracy: 0.9907 - val_loss: 0.0597 - val_accuracy: 0.9827\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 34s 20ms/step - loss: 0.0232 - accuracy: 0.9925 - val_loss: 0.0741 - val_accuracy: 0.9845\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 34s 20ms/step - loss: 0.0202 - accuracy: 0.9931 - val_loss: 0.0509 - val_accuracy: 0.9895\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0581 - accuracy: 0.9859\n",
      "Test accuracy: 0.9858999848365784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp40o_lgt6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp40o_lgt6\\assets\n"
     ]
    }
   ],
   "source": [
    "def create_approx_vit():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(28, 28, 1)))\n",
    "\n",
    "    # Patch extraction and embedding using depthwise separable convolutions\n",
    "    model.add(layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(16, (1, 1), activation='relu'))  # Pointwise convolution\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Additional lightweight attention mechanism\n",
    "    model.add(layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(32, (1, 1), activation='relu'))  # Simulate channel-wise interactions\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Flatten and classify\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_approx_vit()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmporid8ad2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmporid8ad2\\assets\n",
      "C:\\Users\\Sever\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "411560"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = 'transformer_approx_mnist'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "            # Ensure the input is cast to float32, as expected by the TensorFlow model\n",
    "            yield [tf.cast(input_value, dtype=tf.float32)]\n",
    "\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal trained version for the MCU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 11s 6ms/step - loss: 0.3535 - accuracy: 0.8886 - val_loss: 0.0829 - val_accuracy: 0.9762\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1323 - accuracy: 0.9606 - val_loss: 0.0584 - val_accuracy: 0.9823\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1037 - accuracy: 0.9690 - val_loss: 0.0549 - val_accuracy: 0.9840\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0863 - accuracy: 0.9746 - val_loss: 0.0459 - val_accuracy: 0.9865\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0769 - accuracy: 0.9765 - val_loss: 0.0454 - val_accuracy: 0.9870\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0671 - accuracy: 0.9791 - val_loss: 0.0475 - val_accuracy: 0.9852\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0606 - accuracy: 0.9806 - val_loss: 0.0448 - val_accuracy: 0.9873\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0576 - accuracy: 0.9821 - val_loss: 0.0399 - val_accuracy: 0.9878\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0508 - accuracy: 0.9842 - val_loss: 0.0448 - val_accuracy: 0.9887\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0495 - accuracy: 0.9842 - val_loss: 0.0447 - val_accuracy: 0.9877\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9884\n",
      "Test accuracy: 0.9883999824523926\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)) / 255.0\n",
    "x_test = x_test.reshape((-1, 28, 28, 1)) / 255.0\n",
    "\n",
    "# Create a more compact CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),  # Reduced number of neurons\n",
    "    layers.Dropout(0.2),  # Helps prevent overfitting, does not affect model size\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with augmented data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.2352 - accuracy: 0.9277 - val_loss: 0.0417 - val_accuracy: 0.9860\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1716 - accuracy: 0.9467 - val_loss: 0.0357 - val_accuracy: 0.9886\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1516 - accuracy: 0.9533 - val_loss: 0.0358 - val_accuracy: 0.9876\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1344 - accuracy: 0.9589 - val_loss: 0.0336 - val_accuracy: 0.9886\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1308 - accuracy: 0.9602 - val_loss: 0.0376 - val_accuracy: 0.9878\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1236 - accuracy: 0.9627 - val_loss: 0.0552 - val_accuracy: 0.9819\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1196 - accuracy: 0.9639 - val_loss: 0.0344 - val_accuracy: 0.9879\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1140 - accuracy: 0.9654 - val_loss: 0.0335 - val_accuracy: 0.9878\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1115 - accuracy: 0.9665 - val_loss: 0.0346 - val_accuracy: 0.9894\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1086 - accuracy: 0.9676 - val_loss: 0.0307 - val_accuracy: 0.9898\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0307 - accuracy: 0.9898\n",
      "Test accuracy: 0.989799976348877\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Adjust the learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with a potentially lower learning rate\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit the model with data augmentation\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "          steps_per_epoch=len(x_train) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpfnfatqdb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpfnfatqdb\\assets\n",
      "C:\\Users\\Sever\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19184"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = 'classification'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "            # Ensure the input is cast to float32, as expected by the TensorFlow model\n",
    "            yield [tf.cast(input_value, dtype=tf.float32)]\n",
    "\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write TFLite model to a C source (or header) file\n",
    "with open('modified_data' + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, 'modified_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodified_data.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[1;32m---> 41\u001b[0m total_macs \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_macs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpreter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal MACs in the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_macs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 23\u001b[0m, in \u001b[0;36mcalculate_macs\u001b[1;34m(interpreter)\u001b[0m\n\u001b[0;32m     21\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# Output shape of the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# MACs for convolution\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m current_macs \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mprod(weight_shape) \u001b[38;5;241m*\u001b[39m output_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43moutput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     24\u001b[0m macs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_macs\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MACs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_macs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_model(tflite_model_path):\n",
    "    # Load TFLite model and allocate tensors\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def calculate_macs(interpreter):\n",
    "    macs = 0\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Iterate over all operations\n",
    "    for detail in interpreter.get_tensor_details():\n",
    "        # Check for convolutional layers\n",
    "        if 'Conv2D' in detail['name']:\n",
    "            # Assuming the weights are of the shape [filter_height, filter_width, in_channels, out_channels]\n",
    "            weight_shape = interpreter.get_tensor(detail['index']).shape\n",
    "            output_shape = interpreter.get_tensor(output_details[0]['index']).shape  # Output shape of the model\n",
    "            # MACs for convolution\n",
    "            current_macs = (np.prod(weight_shape) * output_shape[1] * output_shape[2])\n",
    "            macs += current_macs\n",
    "            print(f\"Layer: {detail['name']}, MACs: {current_macs}\")\n",
    "\n",
    "        # Check for dense layers\n",
    "        elif 'MatMul' in detail['name']:  # 'MatMul' is used in TFLite for dense layers\n",
    "            # weight shape is [input_channels, output_channels]\n",
    "            weight_shape = interpreter.get_tensor(detail['index']).shape\n",
    "            # MACs for dense layers\n",
    "            current_macs = np.prod(weight_shape)\n",
    "            macs += current_macs\n",
    "            print(f\"Layer: {detail['name']}, MACs: {current_macs}\")\n",
    "\n",
    "    return macs\n",
    "\n",
    "# Example usage\n",
    "model_path = 'modified_data.tflite'\n",
    "interpreter = load_model(model_path)\n",
    "total_macs = calculate_macs(interpreter)\n",
    "print(f\"Total MACs in the model: {total_macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 4\n",
      "output: [[-128 -127 -128 -128   87 -128 -128 -126 -127  -91]]\n",
      "signed char webcam_image[] = {\n",
      "   -65, -69, -70, -76, -80, -85, -85, -93, -90, -95, -96, -95, -89, -95, -99, -93, -87, -80, -71, -66, -63, -68, -65, -61, -64, -68, -78, -89,\n",
      "   -67, -46, -41, -80, -82, -68, -89, -86, -39, -97, -92, -78, -97, -94, -84, -94, -66, -35, 73, 75, 75, 74, 73, 74, 76, 35, -70, -85,\n",
      "   -71, -75, -80, -79, -84, -85, -91, -91, -94, -89, -96, -96, -94, -96, -94, -83, 47, 77, 75, 76, 49, 68, 76, 76, 74, 76, -70, -89,\n",
      "   -77, -79, -83, -84, -85, -90, -92, -96, -98, -99, -100, -98, -100, -99, -97, -8, 75, 75, -60, -79, -83, -70, 77, 73, 59, 77, -80, -86,\n",
      "   -80, -81, -84, -87, -85, -87, -91, -94, -99, -100, -100, -100, -99, -101, -86, 75, 76, -44, -83, -87, -87, -72, 76, 64, 69, 76, -87, -91,\n",
      "   -80, -82, -85, -86, -84, -86, -87, -94, -96, -98, -100, -98, -98, -98, 29, 76, 63, -78, -85, -89, -87, -61, 76, 63, 77, 77, -81, -91,\n",
      "   -82, -83, -83, -85, -85, -85, -87, -94, -96, -101, -100, -96, -98, -92, 72, 76, -65, -89, -88, -89, -89, -64, 76, 71, 77, 70, -87, -96,\n",
      "   -81, -85, -85, -89, -87, -85, -89, -90, -97, -99, -102, -98, -99, -68, 72, 72, -88, -93, -90, -91, -94, -68, 76, 77, 77, -47, -91, -98,\n",
      "   -82, -84, -87, -88, -87, -90, -93, -96, -98, -98, -100, -100, -97, 38, 74, 39, -91, -94, -90, -91, -89, -68, 75, 77, 75, -83, -96, -98,\n",
      "   -85, -85, -89, -89, -89, -92, -96, -96, -96, -100, -102, -102, -93, 67, 72, -53, -91, -94, -90, -89, -85, -66, 75, 77, -46, -84, -95, -99,\n",
      "   -84, -87, -88, -92, -89, -94, -95, -96, -96, -102, -102, -98, -89, 67, 74, -75, -96, -94, -90, -89, -77, 53, 75, 77, -68, -89, -96, -100,\n",
      "   -86, -87, -87, -91, -90, -94, -96, -96, -98, -102, -102, -102, -88, 68, 71, -78, -94, -98, -90, -83, -30, 74, 76, 77, -41, -90, -102, -102,\n",
      "   -88, -96, -88, -91, -94, -96, -97, -100, -102, -104, -105, -100, -86, 63, 71, -13, -88, -91, -83, -43, 71, 73, 17, 76, -6, -91, -102, -103,\n",
      "   -88, -91, -94, -96, -96, -95, -99, -102, -103, -105, -109, -103, -90, -16, 70, 68, -36, -62, 71, 72, 72, -46, -18, 74, 25, -93, -101, -107,\n",
      "   -91, -94, -92, -97, -96, -100, -97, -101, -102, -107, -107, -102, -93, -82, 55, 70, 70, 68, 70, 70, -74, -86, -47, 74, 45, -93, -103, -106,\n",
      "   -94, -96, -100, -98, -100, -100, -102, -102, -105, -107, -110, -105, -100, -92, -84, -63, 50, 43, -80, -81, -94, -94, -60, 74, 59, -91, -105, -105,\n",
      "   -94, -96, -98, -98, -100, -101, -101, -104, -107, -109, -113, -108, -104, -99, -98, -95, -92, -89, -92, -98, -102, -100, -76, 73, 62, -96, -104, -107,\n",
      "   -92, -98, -101, -103, -99, -101, -103, -101, -107, -110, -110, -111, -108, -104, -104, -102, -100, -100, -102, -106, -108, -103, -73, 73, 66, -93, -107, -109,\n",
      "   -83, -98, -104, -104, -103, -103, -102, -104, -105, -109, -114, -113, -112, -111, -111, -109, -108, -109, -109, -111, -111, -104, -77, 72, 67, -94, -107, -109,\n",
      "   -98, -100, -100, -104, -107, -107, -104, -104, -110, -111, -113, -114, -114, -109, -113, -114, -114, -114, -112, -113, -111, -102, -79, 71, 68, -88, -98, -96,\n",
      "   -102, -102, -104, -104, -104, -109, -108, -107, -109, -111, -113, -112, -113, -115, -114, -115, -114, -113, -101, -105, -85, -91, -64, 71, 71, -95, -107, -111,\n",
      "   -104, -103, -107, -104, -108, -108, -111, -111, -108, -113, -114, -113, -112, -98, -85, -92, -80, -106, -113, -114, -114, -106, -84, 68, 68, -81, -96, -99,\n",
      "   -103, -107, -110, -109, -109, -104, -100, -82, -80, -89, -108, -115, -115, -117, -115, -117, -118, -115, -111, -95, -79, -86, -75, 68, 68, -93, -109, -110,\n",
      "   -80, -85, -82, -88, -101, -108, -114, -114, -114, -117, -114, -114, -110, -89, -97, -95, -107, -117, -116, -115, -115, -113, -97, 67, 68, -76, -105, -108,\n",
      "   -108, -110, -107, -112, -109, -111, -88, -92, -101, -92, -109, -115, -117, -117, -117, -117, -116, -112, -103, -87, -97, -106, -92, 67, 66, -86, -110, -106,\n",
      "   -85, -96, -90, -95, -111, -112, -115, -116, -118, -115, -118, -116, -96, -98, -102, -100, -116, -118, -116, -118, -118, -112, -89, 65, 65, -84, -108, -115,\n",
      "   -109, -113, -114, -115, -113, -113, -92, -90, -111, -101, -117, -119, -118, -117, -119, -118, -118, -119, -119, -117, -118, -111, -103, 62, 63, -95, -111, -115,\n",
      "   -93, -86, -80, -91, -114, -118, -116, -117, -118, -119, -119, -119, -119, -118, -119, -120, -118, -118, -118, -117, -118, -114, -109, 55, 61, -88, -113, -114,\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "#VERSION FOR CNN INFERENCE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def capture_and_preprocess_image_cnn():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume largest contour is the number\n",
    "    if contours:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # Crop and resize the image around the number\n",
    "        cropped = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(cropped, (28, 28))\n",
    "\n",
    "    else:\n",
    "        resized = cv2.resize(gray, (28, 28))  # Default resizing if no contours\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_resized = resized / 255.0\n",
    "\n",
    "    # Show the processed image\n",
    "    plt.imshow(normalized_resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Reshape the 28x28 image to 28x28x1\n",
    "    return normalized_resized.reshape(28, 28, 1)\n",
    "\n",
    "\n",
    "# Prepare the image from webcam\n",
    "input_data = capture_and_preprocess_image_cnn()\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "\n",
    "# Quantize the webcam image\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0]['dtype'])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], [quantized_input_data])\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output details and extract output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char webcam_image[] = {\")\n",
    "for row in quantized_input_data.reshape(28, 28):\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version with Fine-Tuning on self-collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def capture_and_preprocess_image_cnn():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume largest contour is the number\n",
    "    if contours:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # Crop and resize the image around the number\n",
    "        cropped = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(cropped, (28, 28))\n",
    "    else:\n",
    "        resized = cv2.resize(gray, (28, 28))  # Default resizing if no contours\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_resized = resized / 255.0\n",
    "\n",
    "    # Show the processed image\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(normalized_resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Reshape the 28x28 image to 28x28x1\n",
    "    return normalized_resized.reshape(28, 28, 1)\n",
    "\n",
    "def append_save_data(images, labels, directory=\"dataset\"):\n",
    "    \"\"\" Append new images and labels to existing `.npy` files. \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)  # Ensure directory exists\n",
    "\n",
    "    images_path = os.path.join(directory, \"images.npy\")\n",
    "    labels_path = os.path.join(directory, \"labels.npy\")\n",
    "\n",
    "    # Load existing data if it exists\n",
    "    if os.path.exists(images_path) and os.path.exists(labels_path):\n",
    "        existing_images = np.load(images_path)\n",
    "        existing_labels = np.load(labels_path)\n",
    "        images = np.concatenate((existing_images, images))\n",
    "        labels = np.concatenate((existing_labels, labels))\n",
    "\n",
    "    # Save updated arrays\n",
    "    np.save(images_path, images)\n",
    "    np.save(labels_path, labels)\n",
    "    print(\"Data successfully appended and saved to disk.\")\n",
    "\n",
    "def collect_data_for_tuning(num_iterations=10, delay=5, save_interval=10):\n",
    "    collected_images = []\n",
    "    collected_labels = []\n",
    "    digit = 7  # Iterate over each digit class\n",
    "    for i in range(num_iterations):\n",
    "        try:\n",
    "            processed_image = capture_and_preprocess_image_cnn()\n",
    "            # For demo purposes, use the expected digit as the label\n",
    "            label = digit\n",
    "            collected_images.append(processed_image)\n",
    "            collected_labels.append(label)\n",
    "            print(f\"Data collected for {label}: {i + 1}/{num_iterations} of digit {digit}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to capture image on iteration {i + 1} for digit {digit}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            append_save_data(np.array(collected_images), np.array(collected_labels), \"seven\")\n",
    "            collected_images, collected_labels = [], []  # Reset the lists after saving\n",
    "\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    # Save any remaining data not yet saved\n",
    "    if collected_images:\n",
    "        append_save_data(np.array(collected_images), np.array(collected_labels))\n",
    "\n",
    "def load_data(directory=\"dataset\"):\n",
    "    images = np.load(os.path.join(directory, \"images.npy\"))\n",
    "    labels = np.load(os.path.join(directory, \"labels.npy\"))\n",
    "    print(\"Data loaded from disk.\")\n",
    "    return images, labels\n",
    "# Example of how to use the function\n",
    "data_with_labels_full = collect_data_for_tuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n",
      "Data loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "new_images_0, new_labels_0 = load_data(\"zero\")\n",
    "new_images_1, new_labels_1 = load_data(\"one\")\n",
    "new_images_2, new_labels_2 = load_data(\"two\")\n",
    "new_images_3, new_labels_3 = load_data(\"three\")\n",
    "new_images_4, new_labels_4 = load_data(\"four\")\n",
    "new_images_5, new_labels_5 = load_data(\"five\")\n",
    "new_images_6, new_labels_6 = load_data(\"six\")\n",
    "new_images_7, new_labels_7 = load_data(\"seven\")\n",
    "new_images_8, new_labels_8 = load_data(\"eight\")\n",
    "new_images_9, new_labels_9 = load_data(\"nine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels, num_images=10):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKMAAAB7CAYAAACo2ENAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABN3klEQVR4nO2debRfVXn+3wwMIQOZyEgmCIPIoNU/oDShImtVq0agmgpUcai20lpLaRVZuJartLaWVlwqVrRliVC1RQqKq0KxhUJFQJApEIaQOTcDCSEESG4u9+b3h+vsPvvJOc89597v+ZqV3/P5a5+7z/fs4X33u/c5d7/vHrF37969YYwxxhhjjDHGGGNMFxj5q66AMcYYY4wxxhhjjPn/B3+MMsYYY4wxxhhjjDFdwx+jjDHGGGOMMcYYY0zX8McoY4wxxhhjjDHGGNM1/DHKGGOMMcYYY4wxxnQNf4wyxhhjjDHGGGOMMV3DH6OMMcYYY4wxxhhjTNfwxyhjjDHGGGOMMcYY0zVGD/WHAwMD0dPTE+PHj48RI0Z0sk6mAXv37o2dO3fGrFmzYuTI4X9btFz3HzopW8t1/8Fj9sDEcj0wsVwPTCzXAxevnQ5MPGYPTCzXA5Mmch3yx6ienp6YM2fOUH9uOsy6deviyCOPHPZzLNf9j07I1nLd//CYPTCxXA9MLNcDE8v1wMVrpwMTj9kDE8v1wKSOXIf8MWr8+PEREXH66afH6NG/fExvb2/KHxgYyO7Ha/5Cxvci/f39KX3IIYdkea+99lpKT5gwIcvbu3dvSh900EFZHl7zV9ODDz648jlc74kTJ6Z00QcFfX19pfWMiNi9e3dKv/rqq1kePod/h/Uu6tLX1xc//OEPkzyGS/GcpUuXpvI2b96c8lHGXCeUVURef+5nzGO5Kl0ZNWpUZR723ZgxYyrrGRFx6KGHlqYjtMxRP1jmqm7YXnx+xL59U9Db2xtXX311R2RbPGPRokWp3lhHrHtRdlma72WZo+xUn3N5ygZgfzX5nXoOo/57gu1nmau6oU3auXNnSr/22mtx1113tTpmX3755ZT/yiuvZPfPnDkzpTdu3JjlYXumTJmS5T3//PMpjbYvQuvL3LlzU/rFF1+sfCaXt23btuwaZcQT2/r16yufM3bs2JR+4YUXsjyUGdsBtEuHHXZYloc2fPv27RHxS1t8++23d1yuv/mbv7mP3kXk7YrI+5btD7Zlz549WR7qKY8RvD788MOzPNRpnjfR/vI8xs9BfeG5AGFbM1Rwblb2q9C3PXv2xPe+972OyxVtMeq2smmsB6izLFe0v2rOYbuFfcL9o+ydWkvxc1QejzUE28ttwudwP+F4KNZcfX198W//9m8dl+uSJUtS32Od0CZH5HbqpZdeyvLUnIN5RxxxRJaHNoD7Fe0F2q+IfNyxXBl8Dtt6rA/b+jIbVpan9J9tSZU+dlK2xTPWrl27z3tGRMTFF1+cXeMcg7aG68s2GmWixiuvbVV/YRmDrTtxbKm1rcpTNoHLw3vZ7letMXt7e+NrX/tax8fsmjVrSmXLlM0PBdi/3/jGN7K8DRs2VD5TrcNRJupdhH/H8zGOWbav2Je8psC8SZMmZXm4jhzMZgzGSy+9FHPmzOm4XM8888ykr7t27aq8H2XHNmbcuHEpzTYN4b5TY0/VBfWK36F4jke43gi2ISK3NWxPsHxuE+oA69GOHTv2Sff19cVPfvKTWnId8seoosKjR49Owkaj0amPUUjdySxi6B+j+N66Hyb4d/hc/h32E/8O26Hqxs/s1FbE4jkHHXRQah/WiRc4qr5168eyG+rHKOwfJfOIXHbqA6Qy8k0+RmFe3Y9RdfPrUDZesY7cFvVCgm3huqHs+Jl4rcpr8swmH6PUvWrRpF5ylK6gzpXZrjbHrBoLSoexPTwu8F5+JvatevFU9k7lReT91aRuqnyUWRMboT66dlquo0eP3qeMsnKxD5Rt5HGgPloofairY+rDGJehPkbxYkv1Mz6T78PrOh+j6pTXhDJbXPdjlJrXWHZD/RiF8mLZNfkYpdYrKo/1rCpvqB+jmuhRE9AOF+3DcpvYJbVGVHl117as95g32Isl1pt1FcevstFM3Y9RTfQxorNrpwkTJpR+sFB2kVHvE3X/YanmXqbJxyhlzzvxMUqtiet+jKqq+1AZTLZM3Y9R6h9bTBsfo7g8/ODAHxHwmj8+4EcM/qCA/TXcj1EFba6d+MMwov5RPdg6vipPjT31HOyDJu87anw3eRdTtkatiet+D6liyB+jCnp7e5OxQGOqGs8dikJTefw18QMf+EBKf+xjH6ssj1EfOxRsFNULOxoDLmPZsmUpfdlll2V5+MWSf4eGoVA8NcCGw44dO+SkyvUoQy0OsX9uvfXWLE8prpqkVJ7SB0bpat2XhksuuSS7Vh9UUIYoc/6vYyfYs2dPqgtOdPx1nHfTIGqHAtb/q1/9apZ3wgknpHSTj0iI+lDE5TM333xzSt9www2V9/F/H9DIsg1CGfF/H7Zs2ZLSqO9qzAyHsWPHprrif2e5vJ6enpTm/sO2424jptgNVAYvTPC/gspO4n9WOC8ily3u1ozIZcS7TXF8sf6gzN7znvdkeUuWLElp1ldsxwUXXBAR7cl15MiRqe1YD95poRa1ODerj1i8a0lx9tlnp/TixYuzPLX4Vn2pFkYMjsXVq1dneY888khK8+4/bD/vviuTYVtzbFUZ3HfYB7fcckvlM9g24UuG+uisaDL/sc5hGereJv/gwmc22en6s5/9LKWLeaktuR566KHJHqEtYrnibiiec3A+5v6ZMWNGSnOfT5s2LaXVbvSjjz46yzvjjDNS+tRTT83yeBes+mdUXZr8Q+7yyy9PaZ6TqvShDVt88803pzGFcsX5NCLXX7WToUkdp06dmtKsD6rv8N7BPhio+QOfo/55W9fLhWHbVfWRsY01cUQ+xyq5qPXlJz/5yZRWHi/qwwT3O+4+Vh/z+OMXywj7jdf22Pech++japcs7n4fDPxdsUbGdnaSgYGBpJPqY7zSTVU3lAm+X0TkMnjve9+b5eGY5X+OqHHKcubdoIj65yTOL/wBEut27rnnZnnnn39+6X0ReTvOOeeciGi2m92n6RljjDHGGGOMMcaYruGPUcYYY4wxxhhjjDGmawzbTQ9RQcCU6xNeqzgAvOXrO9/5TkrfeOONWZ7yf1bxTdR2dt5yqwIyY951112X5Z144omlz+fyuQ9x62dRngpaNhz6+/tLt6QqP3e17V3FaeAtjCouBMJ6pOKbqNgGLDu8Vi5buC03IuLkk09OaQ50p7ZA49ZY1FW1xXuo9Pf3p77HcjmgqYr/psYytu2OO+7I8jDg4Ve+8pUsb+XKlSnNwVxRd7iexxxzTHb9iU98IqUXLlyY5eGW09/5nd/J8rAdH//4x7M8dA3gbeKox8q1EZ/f1pg9+OCDk16rmDl1t6GrWDIqAG6TWCX4zMFi9NSN58NjH20xbyf/m7/5m6hCuRycd955KY0BkdsAt5pjndg+qL5EWaLLR0TE17/+9ZRW8Zzqxmga7N7BfluFeiafnLNo0aLKe7EPf//3fz/LKxvDbcm1r68vtb1u7MX3v//92TW6VHEwa3QTZvcmdOPlrf+4zuC2d0qu6BqgtvLzGFQu09iH6NoVEXHaaael9Be/+MWI6I6bHs5lKvi2cnlgO3z99ddXlo3PUWOS+xV15Sc/+UmW99Of/jS7xn5X6ywVK5Xbq+adL3/5yyn9kY98JMtDV2Usuw3Zbt68OY0VtL3qnUGtURkVm+baa6+tzFOxaTpFXZdYvg/XS0888USW94Mf/CCl2RWq6r2vGy7TdW3cpZdeml2jLqoDT1gnMFSAWqvx3MTzOMI6gr9VMZvVO6cKvcN1wfazfV+7dm1KF/aRQw60AY5ZZYvVQVwM2ip+H7zmmmtS+qabbsryMCSEkrlyveN8nsfVuhvv5dAMWG+273XHRhFyo8m7jndGGWOMMcYYY4wxxpiu4Y9RxhhjjDHGGGOMMaZr+GOUMcYYY4wxxhhjjOkaHY0ZhbDvsPIdHOqRrPhMfj76oKqjEgc77lQdjaqOGZ0/f35Ks//wlVdemdJ8BKjy32V/0bJ7OsWhhx6afE7R35l9SLF/WHbjx49PaYxRUTy/CoxJwH2ufPWRwWKYYL3xqOSIfY+NRyZMmJDSJ510UpaHPuTs343+uxyXpZv+8Rh/BmmiR0rv8dk//vGPs7xbb701pbkPUMfUsfTM8uXLs+s/+IM/SGn2jUcdYD/pq6++OqW/+c1vZnlLliyprLeK61UVB2aox2E3AevF8sIx3OToabzmvsW28vHzF154YUqzLVQ2nOuNv+U+RLmccsopWd7FF19c+Tsliz/90z9NaT6+e+vWrfvUsy1b3Nvbm+yCiq+o4rXgvcXR9gUqRp96Zjd0uu2xwjqGdqjQ6bbkirYY+53XBCgfjq1y2223ld4XkY9ttc7heBJ4rxqT/EyOzYnzF8/VvB5AUM/4mYqJEyem9NFHH53llc2xTY6dbsJhhx2W7JFaz9WNy8j1XLp0aUo3if+CqLWsemYTeK5UsVpxXYVzOMOxZarWDW3Itq+vL5Wxffv29Hcedzx+EdRnXiOqWFPqSHX8nVoTD3aEPF7zuMNrfg62vyzObAHGt4uI+OxnP5vSXG/s38985jMp3UYc1aHC8sMxxPHsFBhXjudf1i0Ey2B58fsVXvN7BbYD39ki8nUyz5VqbTBmzJiU5jmriCcU8X/yVPFWOwXqLfcXto3HHraN89DOrF69OstDO/39738/y8MYUmo8q5i8g6Fi9OG1GrPMY489ltJf+MIXsrwNGzakdGEDm9hh74wyxhhjjDHGGGOMMV3DH6OMMcYYY4wxxhhjTNcYtpsebjXHbbm8pW/SpEkpjdv0IvLtc7y9ELcw8rZftbUOtwnylknM4y2nvIURj93k7be4ZW7y5MlZ3ne+852U5q2Xd999d+XvsIwXX3yxsm7FNru2XAgWLFiQ+vsXv/hF+jtvk8U+4K2AuKWat0XWrTfrEfal2rLL21vVEdVcBm5bZH382te+VlnmypUrUxq3j0fk/aS2cWO92j7GVm2/V2UrFwLVr2qbvnIvwLoM5iqrXMkwj7f7f/CDH0xpPtoat9jiEeER+ljjqu22bY3Zvr6+VB+sFx97j0frnnjiiVneww8/nNI8ZtFNjsceurpyv+NR5KxzTVy91BHWmKe2PX/oQx/K8tCdB+1cRD5O2Q7g1vPChrcl14MPPjiNuV/7tV9Lf+cj11/3utel9NNPP53loQsTt0XpcJMt3ENFlV/3d4xy/cft5WyL0U2hkHFbckXK5vYCrL+yjcp9um5fMU3sNNsEFeIAUW7ZXG+c1/l3s2fPrvwd2vfC9rcl1xEjRpT2N//tuOOOS2kerzju1BzLa+mZM2em9Pr167M8tN+8tkT78Pzzz2d506ZNy67xtzxHqGPJjzrqqJTGtVJEvg4+/fTTs7xLLrkkpXktj3XbtGlTZdmdANd0s2bNSmnuLxwHPN+hfUFZRUQ899xzKc3rV7V2Uu86Y8eOTWmWFbvKLliwIKVxjRCRjzVeT2C/Y3gSzvvP//zPLA/fg7A/IyKuuOKKlMa2D9VltFPg2OM1PtaN85T9rbv25blYudZyOARErVOV6ybXTdko/B3PS6ijRfvUO9FwGBgYSHXD+vI4wfoqNz01Z7DNQTt57rnnVpan+vytb31rlsfvH/hNhF0dMUzKk08+meWhLf7c5z6X5b33ve9NaW6TCv8xXBda74wyxhhjjDHGGGOMMV3DH6OMMcYYY4wxxhhjTNfwxyhjjDHGGGOMMcYY0zWGHTOq6lh69pFF/2QVo4B9bdEnUh07zah4UurYTfa9xdgy6phF9qtH31L2+8S8Jj7QZf3WJMZGE1555ZXkL4q+oFwe9jPnoQ6wXyzKgI91xeewHmHfcXn4nCZHqzLoW3/EEUdkeSivD3/4w1kexjdTRzWzjlXFwmkjpkXVceJNYiyoetWtM8sOf8f+50rHVZ4aWywD1Bd1HKny/Ve/U+3rFK+++moqB9vO8R+wLg899FCWp+KBoV8/9/u6detS+uyzz87ysC4cr6jqvrIy1L0Y2w/j9UXkvvMbN27M8latWpXSytZwfA28t4gN0JZc9+zZk9p7zz33pL9zDBCMC8B5qN89PT1Z3pFHHjmkeik7rWTH9/7sZz9L6RtuuCHLw9gXV155Za26DFa3ZcuWpTTHPSiLu9SWXPv7+1PdcFzwGghhO63GK865Kmaiiuek5iqOEaFiijB4Lx4fHlG9pozI23HyySdneagfrA84fsvi6nUS1CEVC+WZZ55JabZn2LdcT4xRxGunNWvWVD5z27ZtKc39ivGl+JlsM/G5rAM4Xrne2F5eV33pS1+qrNtTTz2V0qxjOO/g79pYO82bNy/FZ1q0aFH6+1133ZXdh+OX9XD79u0pzXNK1TMicr3nsYz3qrhQXBfu5xUrVlTWB3WCj7DH5zzxxBNZHto1tjNoHz772c9Wlo1jqO04qoOh1sw4LjBWV0TeR/w7tBcYuy0iH7O4xuG6DCZblYf14XGDcY8wNhnDv1NxvtBmFOnhvK8pRo4cmfQOxwbGOI3I18jveMc7srx//dd/TWmWAca64/E8ffr0lN6yZUuWN2XKlJTmuH9oQ7/73e9meRi/NiKXnZr/ed2NcVw55t2GDRuiije84Q0p/cgjj2R5qAOFXJusnbwzyhhjjDHGGGOMMcZ0DX+MMsYYY4wxxhhjjDFdY9huehH/tx1XbRtU4LY9tWW8rhsMo45qHA647e+6667L8nC7NB8jj/XhLdHYF+iiGJG3n49+7TQvvfRSaRm8DRi3bvLWYjzm9dlnn83ysN28bXXu3LkpzS5GWCfe2qmOCFduJHwvlnH11Vdneffdd19K8/ZGdbQ2tpGPXUU3GTwKtw1Qh9RxwdhfPH6U/iIsV3VMLD5THWHbxIWP64a/5Ty8Zl259dZbU5rbhO3gemPdeNy0wa5du1J9lDsrjlOWLeYpFx3uP3WsMbadj59t4qaM9/KW8Ztuuqnyd9dcc01Kq6PqlauXOmq70Ou2jop/9dVXUxmom2p8MZj3qU99KsvDLdxKrqzDmKfcJ7hf1b1sG9H+DtUlXbnpsz5g+4s+a8sVfu/evenZaONYBthf3BblmqSOq8a8Jm7JuJZhWbH+qVAJaEu2bt1aWSb3BY5Dnm/x3gsvvDDLQ5tU9Gdb43X37t2pvdi3yv1S6RjLR62BFFhGE1dQNUaUXeQ8tNl/8id/kuWhzN///vdneWpOQlD/2hiza9asSbYS3bBYj1QYDqy/mvs4r254ADW/qbwIvZbB8lmuSiYYuoLfZ9AljZ952WWXlT7jV+2mh7BMsH937tyZ5eF7Hc+jqKu8/sc+Y5dflJcahxH52FBrUXa7Va7eWAa/42LfKL0rylP2aDiMHj062StceypXtNtvvz27Rtc87h+cu9gOYBk8j+G7I/8O+5l1jO29chvFcfkP//APWR6Wed5552V5yrX83nvvTWn1LjaUbyzeGWWMMcYYY4wxxhhjuoY/RhljjDHGGGOMMcaYruGPUcYYY4wxxhhjjDGmaww7ZtTAwEDyHUT/Uo4fgD7W6AMckfsX8u9mzpyZ0nzErPIlxzwVO4Z9y9nXtiyGRMGNN96Y0uyviTEL2B8Wn8n+oioGD/YFHvnZBrt37079VHYUZ4HyZV++fHlKK/kwGCeK/VLR71cdBc+/U/Eh2Jf9E5/4ROVz/vqv/zqlWQbot6388bntVT66bcS02LNnT+lRp02OYkdZqthPHNdHxVLAPH4m1m2w8YLjUMW44fZiXVlX//7v/76y3qovqspu66j4tWvXpv5QsSlQT5VtUrAtVDG/0Gaocali5JU9F8E2/eVf/mWW19PTU1rPweqt4h6gnhVxRNqSK4L2j4/rVeVjffG45ggdCwz7lW2/6juUK+sK1xPlzkdbI2quUfbroosukuUjOB6KtrcVW2hgYCD1IfatGpOqn1l2KoYhws9Ux3Jj3/FR1lw+toPHtopLgTE6+He4jvzqV7+a5aF+sI6j/hft7VTsUGbXrl2pn9S8psovi0lXgLqiZKeeqfJU/MiIocd3mTp1akqfdtppWR6ul7dv357loR5ze6tiPbZhi0eOHJnKf+6559LfWZdVnD1sC+u2ivOI7WF9UHEEsX8Gi6Ol3ktU/DnUR45dhM/k+erLX/5yZV3w2HhsU1u2uC5Kx+rGb1O6yX2L+qLWXDwmVRksB4RtOOqvWjsp3VLv1IU8685VTdm9e3dpHFVey+A12zsVz5GfU5WnbD3bCKXjPL6ULca11CmnnJLlfe9736t8hopHrfqpTFeaxO7zzihjjDHGGGOMMcYY0zX8McoYY4wxxhhjjDHGdI1hu+lVuf00OWJUue/gUZe8HVa5ZaljrqvqFbHvdjrcsjZ79uwsD93m3vWud2V5uIWdt6qp7bi49Y23U6KbYrFdsMnR6E3o7+9PfYF1UsfEc56SgZIdorYLM8rFiOuGusruZO94xztSGl32Isrdc8qe2eTIZexDbF8bW5IHBgaSPLFPuO/qupwpefDW27oubU1guaqt5ngUObf3+9//fkorlyzeBq+OIa/aXt6WO9eb3/zmtPX/gQceSH9nOSgXKtyGy+1B+Sm7pWyScsFEF7Sy8nG8XX755ZXPue+++yrLVG5H6shldYxtIee25FrlVq5cMljmav5FufLWayxPyZW3qyvXMtY5vL766qsry1dueqrerFd4DLVySSrscluuIX19falPsY94m7xyBcC+4/vwOcqdmscZXvM8pnScj/dW+qjcc/E5XDd0PWA9eve7353SeHR2RPlx4m2N1y1btqR6K91RbiOoD03cCfFe7nN1ZHjdcc6wfuBaCt3yIiKuvfbalEY3t4j8GHQer8oVtWoua2vtVPQNhhrh/sL6K9dVfp9BeU2YMCHLQ3mpkBc8JlA+g+m7Whdg3dRzuE3Y/jlz5mR56HJ7/vnnZ3lV7kdtjdmXX3459evmzZvT31mHUS6sY7h+UO6kag2ibDHPC8oFmNcyeM15yvUQ89hGoT2pOy75mUW6LTe9vXv3JtumbKp6N1HvFAolc0TptHqniNDvXx/72MdSmtv7rW99K6V37tyZ5WFd1fuOCnFU6EqTucQ7o4wxxhhjjDHGGGNM1/DHKGOMMcYYY4wxxhjTNfwxyhhjjDHGGGOMMcZ0jWHHjMLjTlWcnLrH2LKPIfoOsx/xUI+4RdgHlP1yMU7Jj370oywPfSbR552fy37UeHQx+3wqP9OyfmrLh3rq1KmpL7BtKvYE5ym/4apnROg4ISgPjPMTEXH44Yen9O23357lLV68OLvG+EEqLhXHMKmK38J1/fznP5/l/fCHPyx9foQ+Ir1NsC0qjpvyVVdMmTIlu0ZffG4nxmDg491x3LM+KN989nfGenMZqA//8i//kuWhPzvbIPS/V7FdsH/bivO2fft2eSxrgYrlouKuYf9xP3DcFwTHiSpP+aBH5H39G7/xG5Xl1T1ul1FxB1Xdit+1ZYsxthDWQ9lblmPdmF48ntRYV7YK1wLKhkdEjB8/vvI5Kk5UXXbs2JFdo5yUPhS0FTMK422irrEeKZuG+qDWQErmnKfai+OebU2TeQLbxDLG57Cu3HDDDSnNstu2bVtK81oUbUfRZ02OnW7CxIkTU3kY57PJ0egoyyZHb9eNTciywXsHi9OJdVXr5X/8x3/M8lDOHIsT45aouEPch1UxVjthN5izzjorrd2feeaZ9Pc777wzu0/pvRrnap7E9nBMLXxmk75jvcLnqCPjGdQBXEdF5GvAf/qnf8ry8L0CxwlTFXuzk6xduzbJFmNbPfroo9l9999/f0rzvILMnTs3u37iiSdSmmNnrVu3LqV5rGHfsr1Dfec8lp+ytxijb9KkSVke2lEuA/UX370ich2dMWNGlofvAVXP7hQY5w3L4PJUXFFlp1UMJRyLbCfVuKwba4rrzXPleeedl9IczxHnSjWm2A5hGSqvqLdjRhljjDHGGGOMMcaY/RJ/jDLGGGOMMcYYY4wxXWPYbnq7d+9OWwvruhg12fqvjr1UW+sQdZTmYNt5cZspP2fRokUpzdsicYse/w5dkviYTUTlFX3elmvIQQcdlMrHfmY3AUXduvF92F8sH9z+iUc5R0TMnDmz9L6IiC9+8YvZNbpK/vu//3uWd/bZZ6c0b2/EvlC6gzLm36kj2VFv2pDtwMBAab153ClXAJWHsuNtzMrdBI/5VdvHeRs4g21j1zEcT2984xsrn3Hddddl11hv3sZcVU+m7tHIw2HNmjWlbna8fRjrwn2EecolkstRdhphm4b6ro4c5rpy3tatW1Oax75yOVDHnaM8eVs36mGh8224hhTPL3u2ch1URwI3ceGr63LCqKPIWc5f+tKXKp8zVHAbOm8nR31Q7smFvrXlzoUhDpT7t3ITQFhWSh/Vcd6o602OuVbHgjPqmG2cX4466qjKZ+I8HZGPey4b29S2W+2mTZukDSwo07UClDnnoQ4o1yvOw35W4SHY1inXIXYNmT17dmk6IuLtb397Sm/atCnLwzK577BNnId9ocIOdIKf//znMWbMmIiIeOGFF9Lfm7iOKXd91Fl2My3Kjdh37aTGkgqjwfeinVSumep3vDa86qqrKn/3x3/8xynNc3TVM9sas88++2ya09EVD+UcEdHT01P5DNThFStWZHk45ymXRHZ3w35g+amxzuBv0SU2IpfLiy++mOWh7WH5oU7gfMv1WbVqVWV5hWzbCl0xatSoWt8n6rq0MzgWeW4ucw0vUDajSdgWzJ8wYUJl3pIlS2qXj2OM5x7UOW5T2brBbnrGGGOMMcYYY4wxZr/EH6OMMcYYY4wxxhhjTNfwxyhjjDHGGGOMMcYY0zWGHTMKaRK3CUHfRhXfiX3ZlW9lXT9u/t3UqVOz69tuuy2l2V8d451wXAoVzwrLVDGYVAyIwle/LV/bRx55JLUX6zHU+Bns66383BUq/suaNWtSWh2XGRHx7W9/O6W5n59//vmU5qNyVVwWzGO5YhkcMwVjXWBeG/7xBx98cOMYb+qoUzXuuF/xd+pY4yZjgmWg4iFh32IsAy4T5R+Rxw3gWAoqRhXm4fPbGrOHHXZY6g/U27q+8hE6DhD2PT9TxZJBWLZo03lsT548ObvGeF1c/i233JLSKh4K6wvGo+BYUxi3Q8XJKHS5Lbm+9tprtewu9rvSUx57HPcFwefwHKfiLJQd81vAsWSOOOKIyvLV0cmKT3/60ynN9VaUxXRpS6579uxJ/YttY9mpY69VfEUcayrOn+ofFb+HYfuH9WYdwzyuG9577bXXZnkYf4TjlCi7g/1b9FObMd4KuWCdOI4k9hfbOoxNw/2D9W6i21g+x17E+ULZA64Pj90bb7wxpTluDsajUbJS+s6/U3rdabZt25ZkhjLg8ariNCE8p+C1iqHE60c1ljCvSbxXRtlhtBHTp0/P8hYsWJDSH//4x7O8zZs3pzTPvVVx0dqKGXXIIYck2eK8z+Nr4sSJKa30lOuJOsJyuPvuu1P6fe97X5aH5bO+qLWaeudU70Y89s8666yU/vCHP5zloW6/853vzPLQtnE/oY4W9RqObip2796d2odjSo1LzlNxRRGWj/qdisuk1nFq7H3mM5/J8lDO/B6r4kJWPYOvVXzvJu/06TeNf2GMMcYYY4wxxhhjzBDxxyhjjDHGGGOMMcYY0zWG7aaHx07XPWJUbf0arKwqBjtmGMGtb7wNnbcp4r2nnnpqlodb39Q2wyZuMooy9762XAjGjRuX+ubll19Of2+y/VPl4TVvLUZZjh8/PsvDrd68lRllx3LkrZC4nfijH/1olofb2RcuXJjlLV++vLQNEbkO8PbesWPHprRyN63z9+FQ5fIzmPsborafYp5yM+W2qWOFUY94uylvWUe4fJQB98Hb3va20rpE5H3BYxn1jOuC+olH9ra11XzcuHGpDjhmm2zDxfYpN0v+nXLPxLHHfYtjgV1Hmhxb/uCDD6Y022K1XRnbi/oRkcuJj9ou69O2bHF/f3/SV9RFVR6PS9QHdgnCtrGd5P5C1LHPWDc+cvjMM8/MrstcqMryFNwXOE+ouUe5ORW62dZ4xWOn67ojcp66F9vJc+WcOXOyeiB4L7uWzZw5M6W577iflJug2u6/dOnSqOIP//APU5pde7B8VV7b7pe4dsLxc+SRR2b34VhmvZ8yZUpKcz3x3nHjxlXm8Tpj2rRpKa30aDAXCxz373nPeyrL/4u/+IssT4WuUPqA8yrLvIo2xuzevXtTPz311FPZ36vgcYcy599h33Ef4L3qmXXXlhH79pGaW5ROoA7edNNNlWU8/fTTWR62g59f5Vbe1phdt25djBkzJiIitm/fXlmectM68cQTU3rLli37PL/gmGOOyfI+8pGPpPR3v/vdLA/bftFFF2V5OBbQfTBi31ATaItnzZqV5eG4vOaaa7I8NS9deOGFKY22JSJiw4YNUcUZZ5yR0vfff3/lfZ0Ax2zd7xP87lPX1VbZNH43UGtizFPvUMxb3vKW7BptgXKDbeKyiKgQLbyOrIN3RhljjDHGGGOMMcaYruGPUcYYY4wxxhhjjDGma/hjlDHGGGOMMcYYY4zpGsOOGTVy5MjkO6j8J4fqd6nihNT1H1ZHfXOckh//+MfZtYqFoupWFt+prD7Kd/VXyYgRI1Ld0G+Z43+oI8Pr+pCyzyzGieI+R99ozqs6DjZi3yNnsd54xGxE7gOPvt4Reewf5dfPdcPYQpxXNTbaiHvQ19eX+gb1kP3B6x5nqo5t5fqrI4+VP7fSMfbvxnvZT/vb3/52aV24PspWqdhwyvcbY6+0NcZ37NiR+gPL4FhIqIvqCGKuZ10/c9YJlCfHcsPxPFicCrbVCB7zzuWr4+gRlq2Km4YUfdiWXCdNmpTkguOL64u6z3Xn2DIIymSocf94rGH5aDMjIj7wgQ9U1nuosK5wjK8qVHyItmML4dhTx3RjnbidO3bsSGl1zD2PO5zX2G5hXXjeXrNmTUqr8TgY2O8cb+STn/xkSt97772Vz+B4kjjuecyXybAtuY4YMSLpNMoO45hx+WyHUX+bjA98jpJrE9h2qHibn/rUp1J648aNWZ6aOzGe1tq1a7M8FWsKUfNaJ3j++efTGMO4OzgmInLdY/ui4i4iyi6pdwa1rhqsTzCuII97NYdjfDPWsXPPPTel2SbX7YtucNxxx6V10l133ZX+jvY1Ird5PL7w3lWrVmV52L4nn3wyy8M+++3f/u0sr4hjxc+IyGMEYRymiH3XfMuWLUvpn//851ne5z//+ZS+4IILsjyMn8Xjd+vWrSnN7w+ovzwv/cd//EdKF2sDtZ4cDv39/cl+KnuLqLhrTd7XUVe4fcq+43qJY3bOmzcvu8Z8HnsYz099u2DZ7dy5M6X5fR+fo/pwKGsn74wyxhhjjDHGGGOMMV3DH6OMMcYYY4wxxhhjTNcYtpveUI6d5u1+aruXOnpZbXPHPH4+brXjrd689fzSSy9NaXW8N5eB2+d4m3Pd7amqX9p2Ddm1a1fptmo+6rnu9krcAjyce9VR4wj3+V/91V9l1yjLTZs2ZXno8lP3aPGIfLslb70c7Ljksvt+lVuXsf5qez23S8kO9VkdLc3l4bZw3vKrXBh4q/KMGTNSGo9mjsj1mrfIY3243sqFAH/XDbmie4hyWVFHFyM8tpXbGpahbLE6Dpa3/+MW9Yj8eFquN84TnDd37tyUZrdbLINli3m4JT2i3K20Lbm+9NJLqU+VvVXzL16r8czg+OKxh32g3AKVy0enuOSSS7JrtO+87V655Kot+Z3mkEMOSXJVdkzlYf15vOCY4D5Q40XpGP6O9Yj7C/PZjRP145RTTqks73Of+1x2jUeW83HVaGfYlqB+FnNUW+O1t7c31QX7kvsL7Y1aLwx1jahsrbLlyt0zIuILX/hCSnO9H3zwwZTmeQd1h+uNrnmq3pyHcsby2nDTGzt2bLJlqM/cB1hf1nusP7q3ReRujTwGUSa8xsI8HudoEziP9RHrqtZnrDs333xz5e82bNiQ0qwP2G9N1hptsHz58tRX6FrI7wZ33HFHSvO4XL58eUqrkCVqfcm/w3cRtmk33XRTabqsbrgWvu2227K8888/P6XZvVC9f2ObeAxgO1ju2I5Cl9ty00PUvF+1VmdU6BHOq/tMHjNoC3nthDoWEfHmN7+58jlbtmypzFOhgiZNmpTSrHMIv4uX2V+76RljjDHGGGOMMcaY/RJ/jDLGGGOMMcYYY4wxXcMfo4wxxhhjjDHGGGNM1xi2Y+6oUaOS/6HyL1THj1bdN9gz2QcaQR9WFT/g937v9yqfEZEfg8nloT8n+4YrP2Dlb65iVmC9iz78VccpUfFH0F9dPUPFZWK/VBV/BmG/7NmzZ2fX6E/LPs3KtxhhmSMsV3XMJ8a+wGM924h7gHGF6sYL4r7EtqkjXTFGU0Qe24D7p0y3C7CfWd84BtCCBQtSmmWOZXAMMdTVOXPmZHnoN79ixYos7w1veENKs789xjeZOHFiSrch14hfjtmijdjWJnGuMI9jP2FcLY4rUueY14h94+6hbeaxzvH7jjjiiJRWPvCsP+vXr09p1rsXXnghpXk847hU8TWKeaHNGDRlto7rhPMR36/i06i8urA+4PU///M/V5Y3nDLxOTz2UM9YLk3iALbJ9u3bk5xQn9S8po5u53iOmMe6ouK1YP8M5zh2HNssY4xLcfnll2d5GKeF42ChbWabgzF3ONYG5rFd6zRVcaI4/graaJaPst/4HJYd9rnSc7XOZjs8bdq07PqNb3xjSl944YVZHsYj5PZifVSbWD5qPGD8JPxdG2P8tddeK43xxvWdOnVqSnPcSjw2nTnyyCNTmtekKK/JkydneXwvgv3A8XEV6gh7XtchF1xwQXaNY1TF1uI1Jtqdujo9HCZMmJBszZo1a9LfcX3H5d95551ZHvYR11Otj7Dtasyod5HBYm6dccYZlb/FuRPHb0TepiaxQBG1Fime39aauLe3N/UN6hG3BedfHrNom3mdg2NPxQLjtaWaR9W44Ovrr78+pbkPcQypWMvqHZ5/p2wsztVFvzhmlDHGGGOMMcYYY4zZL/HHKGOMMcYYY4wxxhjTNfwxyhhjjDHGGGOMMcZ0jWHHjNq9e3fyCUX/QPZ9Rdi3EX0k68YeidB+qgj7RE6YMCGlL7rooiwPYwlF5PFGMIbIYGUg7C+KvtPKN5tjVGG92Xe00xx77LGp3hg3i2M4YB3ZnxbzWK5KPxQqXhHCPtR8/cADD1TmYRlNYj8hXDf0vWVdwX5CH+Q2YtBg/BkV0wPrxPXAeBw8JvA5GzZsqKyHKk/FtGF4bOF4veqqqyrLeO6557I8lM+OHTsq8zBGRUTEvffeW1k37F/0xW8rttDChQvTGFy5cmX6O/vHo95y36I9YjlgnhpfajxxXVDfOc4L3/umN72pst74HK43jjf+HdostlEqBkRZTIu25Lpr167U99gnPHcoGah4Lcqmog5z+3AOUnaSYzaqGFFqrKv4ihj7K6L++oP7CfWoaENbch03blySp5ofUOaso/g7Hj9o75rEWlH6oNYuPF5RP9hOY/w3luvSpUtTmmPsYHtVrEdeH6EOqnm7E+zatSv1BcYfOf7447P7sI5qHsV4VxERL774YkpzW9Q6FO/F2JQRuV5xzJR3vvOdlXV75plnsjy0i6puKkYW6zhes45X/a6NMdvX15favm3btvR3bifqLNslHE/cTjXOUdfVO4Mar4O9P+FcyHn47nHjjTdWlvH4449nebNmzUppXldNnz49pbmfcJ2l4v91ir6+vqRLzz77bPr7I488kt2H74f8foZ9pmLt8O+w/1QcWhXrjp/Jcy7GSD3zzDOzPJSLmptVbDL1Psp1w3vLvh10koGBgdQmrAfXCeWFsV4j6r9r89jDNvF4xrHGv8O6qXHIv+VxifLi2I9lMiirN+uj+uaCdq9OXGLGO6OMMcYYY4wxxhhjTNfwxyhjjDHGGGOMMcYY0zWG7evV19dXunWLt3fVdT9SrhLqXrVlnLcejhs3rrK8t73tbdk1bp9u4saB8JZkdcQ99hv3If6uqFdb2xufeeaZ0m2h3BYsn7dx4nZD3l6Iz1HblXm7q3ITQNhlkLf7n3XWWSl9xRVXZHm4pZG3aKI+KDcZlgseZc26glvPsb1tyHby5MmlxxOj20RE3l+8xRNdYHjbPOoAH0GMW5y579TRzhMnTiytVxmoV8cee2xlGdy3Sh/xXt7+jLLkenf7CPljjz021Q9dQh599NHsvoceeiil2e0Qj3/mrfO/9Vu/ldI81tElc/78+Vne//zP/6T0ggULsryFCxem9JNPPpnl4RHZERGnn356SqvjcFl+6FqsjrhVc5bavl7YubZs8ZQpU5JuYVt4zOKY4ragfuPR1RG5uwS3YfPmzaXPj8jHJc8Ls2fPrvwdU9c1j+v2gx/8IKXZLU+57KCuqHVDcV9bx07v3Lmz1P2S7Qi2jduJ7pDcTpx/cf6JyG0xu2Vhn8+dOzfLW716dWV58+bNy66xb3GcR0R84xvfSGlu07Rp00rrEpHb9MceeyzLQ/cK5apbyLOt8bp48eI0L95yyy3p72hbI/KxzPXF9RKOz4iIZcuWVT4Txz33j5pHsXzs/4iIT3/609n1f/3Xf1U+h9cKiLIDyn0NYV2pcilpY+6dMGFCkusTTzyR/s5txnHHLnw4ttlmqvqrY+KxT7iP8TmDhT/AerNNQDvD9uniiy+OKtCNk8vD0AXK7Z/X8m1w0EEHpX5FefL66MEHH0xpXgPh2pDbijKr60oboV29lNueWhugi2lE3g7l5slgfdR9DJZRlN3WHItueti3ah3IfaC+M6iwOtgn/J6EZfA8pPqCxyXW55vf/GaWp9Y5KsQC5rFtU+6mdXW8Cu+MMsYYY4wxxhhjjDFdwx+jjDHGGGOMMcYYY0zX8McoY4wxxhhjjDHGGNM1hh0zavTo0ckfEf0im/jjo88ix+hBX0f2Q0SfyDoxPQowbok6RjYi99FU8WrYr1kdJ66OlkbYzxRjuBT91FbcAwTryzJAP2nuZ2w3+ocPBvYJ+8dje9UR8uzPvWTJkuwafcH5WGVsEx6jHJHrJ8Ye43s5thC2g3Uc5apinXWCvXv3Jrng81euXJndhzLnWCAYo4D1YebMmSmNcYQi8lgUGFMmIo9Ns3bt2iwPY128/vWvr6xLhI4xc+WVV6Y0ywf7XcXBYl9z1EH24Ud9QPm3IdeIvC+wnni8ckTET3/608q6YNwV1u+77747pbmt2J8YUyQij1ezfv36LA/tO8uSjx+fM2dOVIHxydguoBw4BgSWz/qi4piUxSRqyxZv27Yt1RPry32J9eU5B3/HsbjWrVtXWTaO05deeinLQ51mWV199dUpPRx9xz7l/v3Wt76V0hzbQMVywL7geaJMH9qK/dbf35+erY62x7HG6wwVtxLv5bULrmvUGoTnBZQ52vqIfe096gTGqIrIdeL888/P8lDPWHb4nKOOOirLwyPKWVfQtgzl2Okm9Pf3Jxli/THOUERuM9m24ZjEeD0Ruex6enqyPJwrMW5gRC5nXtfgfMFjmfXq0ksvTWkVq43XbkrPymK1FeCY5LpUxRZqY4497LDDkszUMfcItwXrz7qN+qhitai4Qry2xHtZVhxvDOvD8zuuu9g+3X///SnNuoo6zrYLxyuvB1etWlVaXltjdtmyZfus+yL2Xe9h+zj2EoLx4BgVT4plpGKSqnHxox/9KLs+88wzU5p1V8UxU/2t5kWUNZfHOtomfX19qQ3YTo6hhOtC1m/VFtUHajzXfQbbscsuuyy7RnlxjFcczypGL9sh7CcVb1PZNvW3KrwzyhhjjDHGGGOMMcZ0DX+MMsYYY4wxxhhjjDFdY9j75fbs2VN6ZCu7wSC89Uwd3YnbJHmbojoCserI14iIZ599NqUXLVqU5W3cuDG7xm1m3E5sI9dF5eEzeTsntlcds1lskWvLheC1115Lz8Y68RZLdHHj7X64hVBtm+Y2oOyabLdGObNbAm+3VdsUq+oSkesAbjPmPN7CiFt6Wd+nTJlSel8bW5I3bdqU6ontVv2MrltcrzpHo5flPf3001ke6gD3z3PPPZfS7FLE9Z4/f35l3j333JPSmzZtyvLQdYlduVDnWf/xGo/j5jKwL9raar5z584kU5QLu8jOmDEjpbdu3Vr5PLZNONZ5zHKfITxOENyOz/adjybHfLapeIQ0ywjbcfTRR2d52E+oZxH7bplHULeKvmhLriNGjEjlKZfpsvmhAMcUu/ZgfzWRK7r6qKOklQ3lfM7DNvF4RnciLkPZdNRj1jns0yKvLbnOmzcv9T3WidcrOH6V/eFxjm627CaA+sHuKTh3ch72D9tQ7ku8vvnmm7M8lA/agIhc59hlDOdH1gclJ7y3aENbaycE5YoudFiPiH3nQ7Q9PCbwmu0gu0oi2F52p8LQFYO512E/87oK66Pc0Bgsg3Uc6811wboql95OsH379jQe1NHoqLPsfokuqDwmldtX2XxTgLqi3hl4LCtXe577r7rqqpRmfVRrG2WHsTyee3FstC3XiF+6HBf1WbFiRfo7ywjHDfcD3svjEvVFucLxWkmFvsFnsus96yS6N7OrmZJR3fv4ff/UU09NaW4Tug8Xbert7Y3HH3+8Vj2aMGrUqKTnWA+WgXJNw/GlXCXrfo+IyGXH8sC+5N+9613vyq6xTHa9xjaifY/IZYnvn1w3/h6yZs2alFYhdIYS4sA7o4wxxhhjjDHGGGNM1/DHKGOMMcYYY4wxxhjTNfwxyhhjjDHGGGOMMcZ0jWHHjMJ4FnV9ydnXEH0b1VHLKq4N56GvIvuzYt04Zgr7aKKPKNdbHceMftTsa4tH8/Lv0EeY/VqxTW0fO40xo7AeKr6T8pnlvLoxpJq0D/tZHecdoeNS4bXyHy6LlVb1O3wmx2tAHayr70MFY7ypPkB5qaPGWT54zfrL1wiOpSZHsasxyfEl8LccC2D16tWV9VTtxbqpWDzdYOzYscl+rF27Nv0dj16OyONBsL/45s2bU5qPW0afe47rhb/jPOx3dawwjxm222X2rwCPqUa/9ohcnqwvaG85RpSyA920xXv37k3PxnZyPCeMh/Xkk09meVh/FQNGxQNhuWL/8DHhaDO4X9RcoI51Z7uA+si/Q5mrGB0qrl1Rz7bilPT29qbycVxwbBe85nUV5v36r/96lofxaTjWXllsrAIcdzwG1bzA9g5j53A/n3POOSk92BoMwfbzWFbzJepOoasDAwP7xNnoBPfcc09pvDG2fQpsC49JfA73ueo7FacEY+7NmjUry8P4LxF5XCplF1mvENYHZWvxXo5lhPeijNsYszi/Yv+xfcH4eTx+cN3BNnPSpEkpzXF2UE9VTEseA1ge6wrfy+OpijvuuCO7xlh1qnyMYReRz7doqyKq4wi3NccuWrQo1e+DH/xg+vsVV1yR3XffffelNNtipfsq7hDqj4rRx+sT1J8PfehDWR7HD0SdUe806t0c31sj8nHJc8F///d/pzTHUkVbU/btoJMMDAykMlA+3E4VmxqvjzvuuCyvrk3nMYvvgPzegM+cPn16lsd28+/+7u9SmucJlAm+30TktpLXBqgP/K6K/aZiqhZ97ZhRxhhjjDHGGGOMMWa/xB+jjDHGGGOMMcYYY0zXGLYfSX9/f+mWLN4SilvIeHsjbv3iLXK4LY23qCmXHITLwy1zyi0gIt8uylvycJsab0FVx5Gq7W1tbVdsyvPPP1+6hZK3N6ojvHFLNW/fxW2C/Ew8WpqPQ8Ztz6xjKB98RkTECSeckF3jkcu8FRH1RZWhjlxW/cTyx/LwiOc23X4i6h9FqsYI11G5RKHMlWukOrJbuVtG6G2z2O98H255Vq5K3Be4dZld/6rcONuS65133pn0df369ZXloQsA56GMHn744SyvrmutOn6bn4H3su3nuuGW5D//8z+vrDe7cymXaRx7XDflytRNN71du3aluuHcxfV97LHHUlrJgEE7oNyLGRxDM2bMyPKwfGUnm8D2CreXs5tHXRdqZWvU3zrBihUrUvloR9h1DPWK7RYeB4+uuRF6bKmj4vF3PFfhvexSwq41f/ZnfxZV4FHjXDcck1y+cn1G/VBzVvHMtuS6e/fu1CYsVx0Fz32JLtKch23jte3rXve6lH7hhReyPLQBPF5Qr9C9MiI/6p5R7nYM2l7WFZSdGpPKxqK9bkO2r7zySmov1p/7Gcfvpk2bsjysP68XcA3CLnwK7Fd+JsJjh12H0E2QXQ+XLl2a0uhmFZGvtXks4zsA2zXM47GB5Stb1Sluv/329G5x8sknp7+j63tEPk547GH/Kpd/bivm4XtJRD5meQ5HHf+jP/qjLG/x4sXZNcqIXV3V+ze2V4W94Dahax7LrI3wI1WMHj06tQnHiVofsFyxT3hNrN7z0W6rPpg8eXKWN2/evNL7yq6XLVuW0ixXbK8KjcBrYqwr94UKqYBlFP1rNz1jjDHGGGOMMcYYs1/ij1HGGGOMMcYYY4wxpmv4Y5QxxhhjjDHGGGOM6RrDjhnV19dX6gPK/pPoo8n+i+wnW4U68pWfqWIZ4e8G82lEv1j27USfa/a/Rn9R9o/H9rIfsIrtgb+rOvq0U5x99tmpjOuvvz79nf3OMYYT+5CqmFrKZ3fBggUp3cSHWsWTwqPnI3JfWI5LhfF2+Jhj5fOPz+Q4VBijgeuGfYN5bfjHY/wZPKaUZYdxhVjXML4J+xTjWMMjjiNyGUyZMiXLw7byscboC83jjJkzZ07pMyN032KMAo7LwsczV+VxnAC0SeqY605x8MEHpzaqI8WxLix3Fcekrj6yvmC/qHgBbKdZ1nfffXdK/+7v/m6Wh/EoVHsZFWsQf8fPxDYWfd5WPIve3t7UbypOoorZqOKuIdxXqMPcPrxm+67iUHH5qt8wj+eCadOmpTTPBXj8fN3YeBHl+tmNOI5oR7k/sG2cVzf+lbKFHB8Gn8NzHP5usNhfb3nLW1Ia5RHxy5iUBazHaHe4DBUbDturYpYW7etG7BKsE9szrBPHcMI4RDy2jjnmmJTmNuAcy3qLcWNw7ETk8cY4BhKPH7SFXAbOhywfbC+PZVwvqTHJdhifibarDdm++OKLye5j27Zv377PfVWoMYPx37jvyuabAhw/Kj4ty1HFauO5/6mnnkpplgGWz/2OcmW9wuewfcK6qXe5TrFnz55U93vuuae0jhERb3/721P6hhtuyPJUXFjsT24DXvN7A66he3p6sjy11sWYfBERJ510Ukrj+01Erk+sy29605tSGnUgIuLYY49NaYzfyOD7QkQeE+mBBx6IiPbkOmHChNQ+jGPF7x+om7ze37hxY0rzGFqzZk1Kq7hnDOoDywrniUWLFmV5LGe8l+t21FFHlZYXkdsarjfmcRvQbnN5w13/emeUMcYYY4wxxhhjjOka/hhljDHGGGOMMcYYY7rGsN30BgYGSl0IFGrbPIPbxNSR60zdo4ubbOdl96GyowwL1BY9LFP1Bfcnbl1t+zjxnTt3JvcA3DrK20hxOyrnYdt4m6DqH+xn7gN85sqVK7M83JaotsJG5Nuen3766SwPt5GqI8PrHjPN5fEWXnZhaJODDjoo6R9u1+X+UXqJ24VZ//BY6K1bt2Z52F/K3ZNRR5TjccT8XK7bOeeck9L/+7//m+XhVlneZq/cElGPeesvPrPu8dTD4dVXXy11SePy0AWA70c5sCsFyvb444/P8tAFhG3q3LlzU5q36mN/8nHW+LuIiFWrVqU0yjJCHzOsXNSwb3g+wb5gNzD8XTFW2pLr/Pnzk5ywLTwfKddwlLnaeq22y6u58tFHH82uzzvvvJQe7GhnzFdl8HZ23E7O7hT4TOUuxPpQ5lrbllxHjBiR2qtcLHErPruUv/71r09p7gO8nj17dpaH7lzoohWRj222d3hUPR9bf8IJJ0QV73vf+yrzWOY4DpXseEzyvUiZ22hbct21a1eqJ871KlyDWuvxkd2PP/54SnMbcO3Cc6pyd0eXHw5pwHOscl9G2akj7NU6UrlYso0uWxNzulNMnDgx2Vj17oF6yf2j3GOVPqrQIso9UY0XBuvDa1TM4/IxlAWuESLydQGGhYjI5y/Wh9WrV6e0chXvFK+88krqY1zvsdsazkG8RkaXZq4n9gOj9BvdmXk8n3rqqVn9EV5n4TsGl1Hl6hqR23h0D46IeOyxx1Kabf+KFStSGsOuROTvW8V4astNb/369al9WAa/8+HYYNd0XFfxHDN//vyUnjFjRpaHYx/XrhG5PvB7A+ZdfPHFWR73E44Tft9CXWW54nNYr7AvuE3Lly9P6SbhFurgnVHGGGOMMcYYY4wxpmv4Y5QxxhhjjDHGGGOM6Rr+GGWMMcYYY4wxxhhjusawY0aNHDky+Smj3y/7XVYd1RmR+y+yH66K/YS+jex/rXyzVewaBuvKdcPfqvgFyo+bUUfjlvmNt+VDPTAwkOSCsQ7UEefse4r15b5DmXM8KYxnwH2AZXBMGRVrSskZ4yxE5H7ifCwp1o3lqGLMoA5yHsbKQR/xNmT78ssvJ33E+CNK11Q8i7LnF6hjhjkWF8L6gLrDfadiOH3961/P8q6//vqU5rhQaryi3zzXDY+LZT/9Kvm1NWZHjx6dZIV9xkeDY5+xbUS5c1tx7BdH8lbdi+Cxvxx/BOOTHH300VkeH0GMTJ8+PbtGHcXjdiNyWbNOotw5pgv2IfcT6mHbMWhWrlxZOsfyuMS4EWy3UAd4bua4AFWwjcDyeV5AO8D9quDnYJkcd0PFrlGgLFWsuuK+tuTa19eX5Ip9xDE9sJ851s/DDz+c0iqWDPcP6gPHAkH7x3XBuZL15qMf/WhUwXMl6iDHs8JYF6xzOH45rg3KUsU8LIvT1UlGjhyZ2qvmLoTbib/j2FyoK5MnT87yMP4X2weMGcXloa6wzKdOnZpdY5lcBtpetpl1Y8Xy/Iu6q2KI1o1ZO1R6e3uTzuD6hduJMXLY9qGOTps2Lcv7xS9+kdIcJwzjwbB8cExwXdS7lYqjymNLxePCeESsq9hGjnGIz8EYQ4q2xiyCaxnuT4yTxPMo6h/3dVVsM/4d6wuXgfzt3/5tSp911llZHseQqhunmHXrueeeK00zDz30UHaNffjkk09meWWx/dqS68SJE1Pf4zqQ45PieGb7gzJgfUB7y+tOFVMar/m9AW0Ex1ljcB3Msf5UrFSUM+sYrjE4DpWKfdsk/nYZ3hlljDHGGGOMMcYYY7qGP0YZY4wxxhhjjDHGmK4x7H2to0ePTlvAcOsXb39H1ye1ZU1tb2SUKxFup+NnqG3ovIVSudSpI6nxmp+JZfK2TNwuyH3YzePE77333tR23F7LfYlbH3lLH7ZzuFv46qC2wnK9sd/UtllGudvhdkfWK5Qz91PVsd5tyHbevHlJrrj9nbetqjzcis3jFV0u2BVOudJgu1lXVB4/E7e8fuUrX8nycGxx36Jc1dZ6Lm/jxo0pza5qVcfLtzVmt23bVmqvuDw8jhZdYiJyFz7eqo9jiPsI7+U6YN8q1w3eqs+yRrdcrhu6A7CLGrpSct3QbZC3mquj4sv0pS259vf3l7oWKZcM3vqtXDNRrnzENo49dq3FI72VPvA2cNVPLFesN9tpdtFF8F41N3N5ZXVrS65V6weuL97H8xi6Y6i1ysqVK7NrfM6cOXOyPKX3KGd0UYiIOOecc7JrbAfrB7p+8fyCbr08F6O8eP5VbhFl67G25Lpr165UPpbLrjNYR1UXzsM5COcffiaPc3RzZT1Cl30+ip3HHV6z6xXOLWzPUQd43YA2gvVfvQNUybwN2Y4ePTrpHLoqslzxXYfBscX2C5/JLskoS3bvw3tZVvg7Lk+5T7PNVut35WaGa122M6qf1LtVG2zZsiXJtqenJ/2d5wfsa1xXRORrfl7/K9uE93J5yr0OZctrbfVeq/pTzSGsW2XvowVDDVPTaXp6elL56r0fr9neYFs4j5+DKNdWHCdsp9FFl/uK+xltLNdF2UClq1g3rjf2IYf/QJsxlDnWO6OMMcYYY4wxxhhjTNfwxyhjjDHGGGOMMcYY0zX8McoYY4wxxhhjjDHGdI1hx4zC44kxZory8+ZYSOiTyTErlC8n+jayjzP6R7MfMz6HfTC5btgmjheDvtLKf5JjG2A8D643ls/+mt2k6nhi9gFF/1bVl0P1YVaoWGAqbk2EjsOB+sIxEZQvO5bB5avxUFV2G/Lftm1bqjfKS8UPwKN7I3JfeR4TGFeG9QHjhLC/vTqWHnWHxxLHa8AyWAZoE1iPMXYSH59+5JFHpvSqVauiCqVj3Yh7cMQRRySbhPEY+JholNmUKVOyPDwe9rTTTsvynnnmmcrfqSPFUQ/UEeYM9xkeLcx9vXr16spnYvtZ79D+qvhBHA8M7y3SbR47XTwby2WbquIOocy5njguONYUz8cIjnWOg4cy4JhAfFQ8/pbHM9aV5YNlqFiPTeLxqHmh04wZM6bUFrNcUXa8PsGxxmML66/mI7ZpOBdwnBk1zt/61rdm12iDVPw5jo+DcdxUezkmEcqO54myuKRtjdexY8em8rAPuL9Qzqy/de1ikzao323btq00HdEs3qYaM3WPGlfrOjVeh9ovdRk1alSqG64RuCzM47kX4z2x3h9++OEpzbpy/PHHp/SDDz6Y5R177LEpzXH9VLxNFYtMxYpRMQe5DJw316xZk+WhjrO+dzMWWMQv5VTYPVzzcz/gemHevHlZHsaawvhfEblN5fGFcyzH8MT3Si4Pxwy/f86YMSO7xjK5DzEWJ8br4zJ47bRw4cKUxnVjRN4OXiti3R5++OHSOnUKjLeJslRrJ55HUf9U/DwVm5L1G3/HbcfxxPVk24hrKYzXF5HrMdqWiDzuL+sc2gyet7He/J5UZt8dM8oYY4wxxhhjjDHG7Jf4Y5QxxhhjjDHGGGOM6RrDdtMbNWpU6bHTvD2LXW8Q3MavtujyMbZ4HC1v58ZtabzVDd2O8Bll4DY1boM62hK3svJWXawrbyfHba3ch7ytt01effXV1Pe4TZG3aqJbh9ray20pO2q5oO7Rn6o8hvPqHuXJv1NbzRV170U3lTa2rqJccTsmHw2r+lJt+VXb15ULX12XmMGOfeZtxgiOX3Y/wvHKW54xD7c0c/m8bRXtTDfc9Hbt2pXqgzaGZYtbqpXr8aOPPprl4fZqdgc46aSTUlptQ2d3Lqwb22LePqyOR0YZqePg2e0W66PGqDrquu2j4seOHVtqi1l2aJt4fOHvWBdRb3k+wnYr9z7uc5wnuF9Zrqir7BbGcyeiji3HvCZHWeN12+6X48aNS+XNmjUr/Z2PPEf5cB7KmW1hXTcYBvuyyRyHbrQR+VzGazBc57DsUK95zYW25Jhjjsny0I1Yzdttu+n19fWlflPuTap8JR8co/wM7Ge2p1g+u/HgmoddOthm4vhl/cB+ZvuE9ea1G+YpFxoOJ4Btwt+1Idv+/v4kF7SLPO8rN1ece7mORx11VEqzqxD2AbqxRkQ8++yzpWVH5LaV5zA197OuonyUO5KyQcr9U7mCtu1+GfHLfisLVcFrJ+xDzsP3WO4jFeoE82bOnJnlof6w/N797nenNLoIltVNhbRR8z/Ov7zmQ3uv9JXHc5mbb1tyHT16dLJR3O6qOqlQNDz/ovsst3P69Okp/fjjj2d56K6LISYi8vXqPffck+UtXrw4u165cmVK85yBcyfaiIjchrA9wbHP31VwTq/jsmg3PWOMMcYYY4wxxhizX+KPUcYYY4wxxhhjjDGmawzZTa/YflV1sobawsXUjUjPW77qPlPdN9g2MnVyiDotq+7vVD+pk0O4/zu1zbHseXXb0uTUIpVXty2dbnPTe4f6u6bP70Q7B5NrG33QRv90qgw17tTJRqo8NTbK/t5p/a06XayOHamTp04vwy3KyiWoiS3slF0Yanublt1tWzzUdlaV0+S+wfKarAWUfqjnDLW96r5uzrFVp/s0OXmnEzJXecOZt+vWbajrI+6npmOjG3Lt1PxY9buh9qvSMZXXpPxu2OHBbGMn107oitTt+bXuHDoc+zBUm9Bp2am8tsYsyrbK9ZNRYUKUa/hQ50oel+o0ziZ9rXR5qHnKDpW507a5dipzBRzOvIbUXRMPte/YzZdD9Qx1XNbVueHaiCZyHbF3iNJfv359zJkzZyg/NS2wbt267Aj6oWK57n90QraW6/6Hx+yBieV6YGK5HphYrgcuXjsdmHjMHphYrgcmdeQ65I9RAwMD0dPTE+PHj+9KcF5Tzt69e2Pnzp0xa9asRsFGq7Bc9x86KVvLdf/BY/bAxHI9MLFcD0ws1wMXr50OTDxmD0ws1wOTJnId8scoY4wxxhhjjDHGGGOa4gDmxhhjjDHGGGOMMaZr+GOUMcYYY4wxxhhjjOka/hhljDHGGGOMMcYYY7qGP0YZY4wxxhhjjDHGmK7hj1HGGGOMMcYYY4wxpmv4Y5QxxhhjjDHGGGOM6Rr+GGWMMcYYY4wxxhhjuoY/RhljjDHGGGOMMcaYruGPUcYYY4wxxhhjjDGma/hjlDHGGGOMMcYYY4zpGv4YZYwxxhhjjDHGGGO6hj9GGWOMMcYYY4wxxpiu8f8Ay5IU15cbAosAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(new_images_7, new_labels_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1693/1693 [==============================] - 11s 6ms/step - loss: 0.3600 - accuracy: 0.8887 - val_loss: 0.1119 - val_accuracy: 0.9641\n",
      "Epoch 2/10\n",
      "1693/1693 [==============================] - 11s 7ms/step - loss: 0.1376 - accuracy: 0.9580 - val_loss: 0.0808 - val_accuracy: 0.9751\n",
      "Epoch 3/10\n",
      "1693/1693 [==============================] - 12s 7ms/step - loss: 0.1097 - accuracy: 0.9668 - val_loss: 0.0691 - val_accuracy: 0.9769\n",
      "Epoch 4/10\n",
      "1693/1693 [==============================] - 11s 7ms/step - loss: 0.0901 - accuracy: 0.9730 - val_loss: 0.0589 - val_accuracy: 0.9819\n",
      "Epoch 5/10\n",
      "1693/1693 [==============================] - 11s 7ms/step - loss: 0.0789 - accuracy: 0.9749 - val_loss: 0.0550 - val_accuracy: 0.9811\n",
      "Epoch 6/10\n",
      "1693/1693 [==============================] - 12s 7ms/step - loss: 0.0684 - accuracy: 0.9790 - val_loss: 0.0554 - val_accuracy: 0.9829\n",
      "Epoch 7/10\n",
      "1693/1693 [==============================] - 12s 7ms/step - loss: 0.0616 - accuracy: 0.9811 - val_loss: 0.0520 - val_accuracy: 0.9834\n",
      "Epoch 8/10\n",
      "1693/1693 [==============================] - 13s 8ms/step - loss: 0.0576 - accuracy: 0.9821 - val_loss: 0.0441 - val_accuracy: 0.9857\n",
      "Epoch 9/10\n",
      "1693/1693 [==============================] - 12s 7ms/step - loss: 0.0521 - accuracy: 0.9840 - val_loss: 0.0495 - val_accuracy: 0.9854\n",
      "Epoch 10/10\n",
      "1693/1693 [==============================] - 13s 8ms/step - loss: 0.0499 - accuracy: 0.9838 - val_loss: 0.0463 - val_accuracy: 0.9860\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0355 - accuracy: 0.9885\n",
      "Test accuracy: 0.9884999990463257\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)) / 255.0\n",
    "x_test = x_test.reshape((-1, 28, 28, 1)) / 255.0\n",
    "\n",
    "x_train = np.concatenate((x_train, new_images_0, new_images_1, new_images_2, new_images_3, new_images_4, new_images_5, new_images_6, new_images_7, new_images_8, new_images_9))\n",
    "y_train = np.concatenate((y_train, new_labels_0, new_labels_1, new_labels_2, new_labels_3,new_labels_4, new_labels_5, new_labels_6, new_labels_7,  new_labels_8, new_labels_9 ))\n",
    "\n",
    "# Create an array of indices and shuffle it\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use the shuffled indices to reorder the training data and labels\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# Create a more compact CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),  # Reduced number of neurons\n",
    "    layers.Dropout(0.2),  # Helps prevent overfitting, does not affect model size\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1880/1880 [==============================] - 25s 13ms/step - loss: 0.2302 - accuracy: 0.9299 - val_loss: 0.0376 - val_accuracy: 0.9870\n",
      "Epoch 2/10\n",
      "1880/1880 [==============================] - 23s 12ms/step - loss: 0.1635 - accuracy: 0.9501 - val_loss: 0.0375 - val_accuracy: 0.9876\n",
      "Epoch 3/10\n",
      "1880/1880 [==============================] - 23s 12ms/step - loss: 0.1448 - accuracy: 0.9560 - val_loss: 0.0367 - val_accuracy: 0.9883\n",
      "Epoch 4/10\n",
      "1880/1880 [==============================] - 24s 13ms/step - loss: 0.1266 - accuracy: 0.9610 - val_loss: 0.0385 - val_accuracy: 0.9866\n",
      "Epoch 5/10\n",
      "1880/1880 [==============================] - 26s 14ms/step - loss: 0.1227 - accuracy: 0.9632 - val_loss: 0.0370 - val_accuracy: 0.9876\n",
      "Epoch 6/10\n",
      "1880/1880 [==============================] - 25s 13ms/step - loss: 0.1148 - accuracy: 0.9649 - val_loss: 0.0347 - val_accuracy: 0.9884\n",
      "Epoch 7/10\n",
      "1880/1880 [==============================] - 25s 13ms/step - loss: 0.1104 - accuracy: 0.9663 - val_loss: 0.0334 - val_accuracy: 0.9899\n",
      "Epoch 8/10\n",
      "1880/1880 [==============================] - 23s 12ms/step - loss: 0.1056 - accuracy: 0.9676 - val_loss: 0.0317 - val_accuracy: 0.9897\n",
      "Epoch 9/10\n",
      "1880/1880 [==============================] - 20s 11ms/step - loss: 0.1030 - accuracy: 0.9685 - val_loss: 0.0346 - val_accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "1880/1880 [==============================] - 20s 11ms/step - loss: 0.1031 - accuracy: 0.9692 - val_loss: 0.0367 - val_accuracy: 0.9875\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0367 - accuracy: 0.9875\n",
      "Test accuracy: 0.987500011920929\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Adjust the learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with a potentially lower learning rate\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit the model with data augmentation\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "          steps_per_epoch=len(x_train) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp_ur_sbi_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp_ur_sbi_\\assets\n",
      "C:\\Users\\Sever\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19152"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = 'modified_data'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "            # Ensure the input is cast to float32, as expected by the TensorFlow model\n",
    "            yield [tf.cast(input_value, dtype=tf.float32)]\n",
    "\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 9\n",
      "output: [[-128 -128 -128 -128  -49 -128 -128 -128 -127   49]]\n",
      "signed char webcam_image[] = {\n",
      "   -30, -32, -29, -45, -34, -38, -37, -36, -35, -35, -34, -29, -35, -34, -32, -31, -18, -11, -19, -17, -17, -10, -5, -5, -1, 1, 5, 10,\n",
      "   -48, -49, -46, -52, -43, -46, -36, -27, -16, 114, 123, 123, 123, 123, 124, 123, 124, 121, -12, -14, -16, -17, -13, -9, -8, -4, -1, 3,\n",
      "   -51, -56, -55, -56, -49, -43, -18, 123, 123, 123, 48, -28, -30, -26, -19, 124, 124, 124, 121, -10, -19, -19, -19, -10, -12, -10, -8, -3,\n",
      "   -53, -55, -55, -55, -45, 54, 124, 121, 30, -41, -48, -45, -45, -42, -30, 117, 125, 80, 124, 123, -16, -21, -27, -21, -19, -16, -16, -14,\n",
      "   -38, -49, -57, -53, 79, 123, 123, -10, -35, -43, -26, -48, -46, -42, -35, 25, 124, 123, 58, 124, 121, -16, -21, -25, -25, -24, -26, -21,\n",
      "   -52, -56, -53, 23, 123, 121, -18, -36, -45, -45, -43, -45, -42, -36, -30, -17, 123, 122, -2, 123, 124, 2, -23, -25, -28, -29, -28, -28,\n",
      "   -55, -57, -38, 123, 124, 62, -43, -46, -48, -45, -43, -43, -43, -40, -36, -18, 123, 124, -4, 61, 124, 116, -26, -30, -33, -36, -33, -33,\n",
      "   -51, -50, 29, 124, 123, -31, -46, -47, -44, -43, -46, -41, -42, -39, -34, -27, 95, 123, 113, 34, 124, 120, -23, -29, -33, -33, -33, -32,\n",
      "   -58, -46, 101, 123, 119, -38, -52, -52, -46, -46, -49, -46, -44, -36, -36, -26, 9, 124, 123, 114, 123, 70, -36, -41, -43, -43, -42, -42,\n",
      "   -61, -48, 92, 123, 122, -37, -49, -53, -49, -49, -48, -46, -46, -38, -37, -26, -12, 123, 122, 124, 123, -32, -43, -47, -48, -48, -49, -48,\n",
      "   -58, -48, -25, 124, 124, -1, -41, -49, -45, -44, -46, -43, -43, -37, -36, -31, -5, 123, 123, 124, -14, -37, -44, -48, -56, -51, -49, -49,\n",
      "   -60, -57, -41, 2, 123, 123, 99, -36, -39, -43, -42, -39, -36, -29, -20, 16, 124, 124, 123, 77, -39, -38, -44, -39, -47, -48, -53, -58,\n",
      "   -65, -60, -55, -48, -35, 102, 124, 122, 123, 123, 123, 123, 123, 123, 123, 123, 100, 42, 124, 120, -34, -51, -49, -58, -60, -61, -59, -57,\n",
      "   -69, -68, -65, -60, -51, -49, -46, -40, -36, -18, 0, -16, -32, -30, -33, -40, -40, -21, 123, 123, -37, -42, -53, -54, -62, -60, -68, -56,\n",
      "   -71, -71, -72, -67, -63, -65, -61, -53, -54, -56, -56, -49, -53, -52, -53, -55, -50, -32, 123, 123, -33, -47, -59, -55, -57, -59, -60, -49,\n",
      "   -79, -74, -78, -72, -68, -69, -67, -67, -67, -61, -65, -61, -60, -58, -57, -60, -58, -34, 123, 123, -31, -50, -58, -61, -60, -60, -58, -51,\n",
      "   -84, -82, -79, -81, -77, -73, -68, -73, -73, -72, -68, -68, -70, -67, -68, -68, -60, -39, 123, 123, -23, -41, -58, -57, -59, -61, -54, -54,\n",
      "   -80, -84, -85, -80, -84, -80, -74, -78, -73, -73, -73, -74, -72, -74, -77, -72, -65, -46, 117, 123, 24, -50, -62, -65, -70, -70, -69, -66,\n",
      "   -86, -81, -89, -85, -81, -80, -81, -81, -81, -82, -80, -82, -82, -82, -78, -74, -74, -55, 69, 121, 102, -43, -55, -54, -67, -65, -70, -61,\n",
      "   -86, -84, -85, -86, -85, -87, -83, -89, -84, -83, -91, -90, -86, -84, -80, -81, -75, -58, -31, 121, 122, -40, -52, -54, -66, -70, -67, -68,\n",
      "   -83, -88, -92, -92, -90, -90, -89, -93, -92, -94, -91, -93, -91, -93, -88, -86, -78, -72, -41, 122, 122, -28, -47, -62, -69, -70, -68, -67,\n",
      "   -94, -92, -92, -94, -94, -94, -96, -98, -99, -96, -96, -96, -93, -93, -93, -88, -85, -72, -50, 119, 120, 10, -44, -64, -62, -72, -65, -72,\n",
      "   -93, -93, -93, -98, -96, -97, -101, -99, -98, -99, -99, -99, -98, -96, -93, -97, -88, -80, -57, 38, 119, 109, -51, -69, -70, -70, -80, -77,\n",
      "   -98, -98, -98, -96, -99, -105, -102, -102, -102, -105, -103, -98, -100, -99, -97, -95, -93, -86, -70, -41, 115, 115, -44, -58, -72, -80, -87, -69,\n",
      "   -99, -103, -103, -104, -103, -102, -104, -105, -103, -101, -107, -104, -101, -103, -101, -102, -98, -87, -79, -43, 112, 112, -48, -77, -89, -94, -89, -91,\n",
      "   -101, -103, -104, -104, -107, -105, -105, -103, -103, -104, -104, -104, -104, -103, -104, -102, -98, -94, -87, -57, 97, 112, 41, -86, -94, -96, -94, -93,\n",
      "   -100, -106, -105, -105, -104, -107, -105, -107, -107, -105, -105, -106, -104, -104, -104, -103, -102, -97, -90, -70, -33, 109, 109, -87, -96, -98, -99, -97,\n",
      "   -105, -104, -106, -107, -105, -108, -105, -108, -107, -110, -104, -107, -108, -107, -104, -104, -101, -101, -96, -89, -69, -24, -79, -103, -103, -103, -99, -101,\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "#VERSION FOR CNN INFERENCE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def capture_and_preprocess_image_cnn():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume largest contour is the number\n",
    "    if contours:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # Crop and resize the image around the number\n",
    "        cropped = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(cropped, (28, 28))\n",
    "\n",
    "    else:\n",
    "        resized = cv2.resize(gray, (28, 28))  # Default resizing if no contours\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_resized = resized / 255.0\n",
    "\n",
    "    # Show the processed image\n",
    "    plt.imshow(normalized_resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Reshape the 28x28 image to 28x28x1\n",
    "    return normalized_resized.reshape(28, 28, 1)\n",
    "\n",
    "\n",
    "# Prepare the image from webcam\n",
    "input_data = capture_and_preprocess_image_cnn()\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='modified_data.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "\n",
    "# Quantize the webcam image\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0]['dtype'])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], [quantized_input_data])\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output details and extract output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char webcam_image[] = {\")\n",
    "for row in quantized_input_data.reshape(28, 28):\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "  c_str = ''\n",
    "\n",
    "  # Create header guard\n",
    "  c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "  c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "  # Add array length at top of file\n",
    "  c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "  # Declare C variable\n",
    "  c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "  hex_array = []\n",
    "  for i, val in enumerate(hex_data) :\n",
    "\n",
    "    # Construct string from hex\n",
    "    hex_str = format(val, '#04x')\n",
    "\n",
    "    # Add formatting so each line stays within 80 characters\n",
    "    if (i + 1) < len(hex_data):\n",
    "      hex_str += ','\n",
    "    if (i + 1) % 12 == 0:\n",
    "      hex_str += '\\n '\n",
    "    hex_array.append(hex_str)\n",
    "\n",
    "  # Add closing brace\n",
    "  c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "  # Close out header guard\n",
    "  c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "  return c_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2034 - accuracy: 0.9378 - val_loss: 0.0357 - val_accuracy: 0.9881\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1664 - accuracy: 0.9492 - val_loss: 0.0347 - val_accuracy: 0.9889\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1468 - accuracy: 0.9552 - val_loss: 0.0371 - val_accuracy: 0.9884\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1366 - accuracy: 0.9579 - val_loss: 0.0345 - val_accuracy: 0.9894\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1263 - accuracy: 0.9614 - val_loss: 0.0409 - val_accuracy: 0.9863\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1240 - accuracy: 0.9631 - val_loss: 0.0360 - val_accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1177 - accuracy: 0.9643 - val_loss: 0.0361 - val_accuracy: 0.9885\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1126 - accuracy: 0.9659 - val_loss: 0.0466 - val_accuracy: 0.9849\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1082 - accuracy: 0.9671 - val_loss: 0.0387 - val_accuracy: 0.9880\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0387 - accuracy: 0.9880\n",
      "Test accuracy: 0.9879999756813049\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Adjust the learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with a potentially lower learning rate\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit the model with data augmentation\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "          steps_per_epoch=len(x_train) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp9hc75fa5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp9hc75fa5\\assets\n",
      "C:\\Users\\Sever\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19152"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_name = 'classification'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "            # Ensure the input is cast to float32, as expected by the TensorFlow model\n",
    "            yield [tf.cast(input_value, dtype=tf.float32)]\n",
    "\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF Model Accuracy: 0.9869999885559082\n",
      "Quantized TFLite Model Accuracy: 0.9868\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "tflite_model_path = 'classification.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Helper function to run inference on a single image\n",
    "def run_tflite_inference_single_image(image):\n",
    "    # Get the scale and zero point of the input tensor\n",
    "    input_scale, input_zero_point = input_details[0]['quantization']\n",
    "    \n",
    "    # Adjust the image data type and scale it according to quantization parameters\n",
    "    if input_details[0]['dtype'] == np.int8:\n",
    "        image = image / input_scale + input_zero_point\n",
    "        image = np.int8(image)  # Ensure the type is int8\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function to evaluate the TFLite model on the entire test dataset\n",
    "def evaluate_tflite_model(x_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(x_test)):\n",
    "        test_image = x_test[i].reshape(1, 28, 28, 1)  # Reshape the image to [1, 28, 28, 1] if needed\n",
    "        output = run_tflite_inference_single_image(test_image)\n",
    "        predicted_label = np.argmax(output)\n",
    "        if predicted_label == y_test[i]:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / len(x_test)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate the TFLite model accuracy\n",
    "tflite_accuracy = evaluate_tflite_model(x_test, y_test)\n",
    "print(\"Original TF Model Accuracy:\", accuracy)  # Assuming `accuracy` was from your earlier `model.evaluate`\n",
    "print(\"Quantized TFLite Model Accuracy:\", tflite_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def capture_and_preprocess_image(threshold=120):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply thresholding directly after capturing the image\n",
    "    # Pixels below the threshold are set to black (0)\n",
    "    mask = gray < threshold\n",
    "    gray[mask] = 0\n",
    "\n",
    "    # Resize the image to 28x28\n",
    "    resized = cv2.resize(gray, (28, 28))\n",
    "\n",
    "    # Show the image without axes\n",
    "    plt.imshow(resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('webcam_image.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "    # Normalize the image\n",
    "    resized = resized / 255.0\n",
    "\n",
    "    # Flatten the 28x28 image to a 784 vector\n",
    "    return resized.flatten()\n",
    "\n",
    "# Prepare the image from webcam\n",
    "input_data = capture_and_preprocess_image()\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "# Quantize the webcam image\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0][\"dtype\"])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], [quantized_input_data])\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extract and process the output\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
    "dequantized_output = (output_data - output_zero_point) * output_scale\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char webcam_image[] = {\")\n",
    "for row in quantized_input_data.reshape(28, 28):  # Reshape flat array back to 28x28 for printing\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 2 but expected 4 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m quantized_input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(input_data \u001b[38;5;241m/\u001b[39m input_scale \u001b[38;5;241m+\u001b[39m input_zero_point)\u001b[38;5;241m.\u001b[39mastype(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Set the input tensor\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mquantized_input_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[1;34m(self, tensor_index, value)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[0;32m    682\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \n\u001b[0;32m    684\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 2 but expected 4 for input 0."
     ]
    }
   ],
   "source": [
    "#Denoised version\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def capture_and_preprocess_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume largest contour is the number\n",
    "    if contours:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # Crop and resize the image around the number\n",
    "        cropped = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(cropped, (28, 28))\n",
    "\n",
    "    else:\n",
    "        resized = cv2.resize(gray, (28, 28))  # Default resizing if no contours\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_resized = resized / 255.0\n",
    "    # Show the processed image\n",
    "    plt.imshow(normalized_resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Flatten the 28x28 image to a 784 vector\n",
    "    return normalized_resized.flatten()\n",
    "\n",
    "# Prepare the image from webcam\n",
    "input_data = capture_and_preprocess_image()\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "\n",
    "# Quantize the webcam image\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0]['dtype'])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], [quantized_input_data])\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output details and extract output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char webcam_image[] = {\")\n",
    "for row in quantized_input_data.reshape(28, 28):\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def capture_and_preprocess_image_cnn():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        cropped = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(cropped, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        resized = cv2.resize(gray, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    normalized_resized = resized / 255.0\n",
    "    plt.imshow(normalized_resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return normalized_resized.reshape(28, 28, 1)\n",
    "\n",
    "def collect_data_for_tuning(num_iterations=50, delay=8):\n",
    "    collected_data = []\n",
    "    for i in range(num_iterations):\n",
    "        try:\n",
    "            processed_image = capture_and_preprocess_image_cnn()\n",
    "            collected_data.append((processed_image, 9))\n",
    "            print(f\"Data collected: {i + 1}/{num_iterations}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to capture image on iteration {i + 1}: {str(e)}\")\n",
    "            continue\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return collected_data\n",
    "\n",
    "# Example of how to use the function\n",
    "data_with_labels = collect_data_for_tuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 3.1297 - accuracy: 0.5000 - val_loss: 2.3624 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.1641 - accuracy: 0.4444 - val_loss: 2.0321 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5555 - accuracy: 0.5000 - val_loss: 1.6955 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5576 - accuracy: 0.5556 - val_loss: 1.4555 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0266 - accuracy: 0.7222 - val_loss: 1.2694 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18748d943a0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming data_with_labels is a list of (image, label) tuples\n",
    "new_images, new_labels = zip(*data_with_labels)\n",
    "\n",
    "# Convert to NumPy arrays and ensure correct shape and type\n",
    "new_images = np.array(new_images).reshape(-1, 28, 28, 1)\n",
    "new_labels = np.array(new_labels)\n",
    "\n",
    "# For simplicity, take a subset of the original training data\n",
    "subset_x_train = x_train[:len(new_images)]\n",
    "subset_y_train = y_train[:len(new_images)]\n",
    "\n",
    "# Combine the datasets\n",
    "combined_x_train = np.concatenate([subset_x_train, new_images])\n",
    "combined_y_train = np.concatenate([subset_y_train, new_labels])\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(combined_x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "combined_x_train = combined_x_train[indices]\n",
    "combined_y_train = combined_y_train[indices]\n",
    "\n",
    "# Continue training the model\n",
    "model.fit(combined_x_train, combined_y_train, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference with Image caught from the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n",
      "output: [[-128 -106    2 -127  -85 -124 -114 -127 -122  -93]]\n",
      "signed char webcam_image[] = {\n",
      "   92, 92, 92, 93, 92, 30, -32, -31, -32, -29, -30, -25, -19, -18, -8, -6, 1, 11, 18, 17, 24, 38, 67, 91, 109, 123, 124, 124,\n",
      "   90, 92, 92, 82, -35, -44, -49, -45, -45, -37, -36, -29, -29, -22, 90, -18, -13, -11, -8, -7, -2, 4, 9, 15, 24, 35, 54, 77,\n",
      "   94, 94, 92, -30, -47, -52, -55, -54, -51, -52, -50, -47, -46, -42, -35, -28, -23, -19, -18, -15, -14, -11, -6, 2, 7, 18, 28, 38,\n",
      "   95, 96, 66, -40, -47, -48, -51, -49, -51, -55, -51, -51, -50, -48, -49, -40, -37, -30, -30, -26, -25, -19, -16, -11, -7, 0, 8, 13,\n",
      "   98, 96, 37, -42, -50, -55, -56, -54, -57, -57, -58, -58, -58, -55, -55, -49, -46, -39, -41, -42, -37, -34, -27, -25, -20, -14, -7, -12,\n",
      "   99, 98, 17, -42, -49, -55, -56, -57, -61, -64, -65, -64, -63, -63, -63, -57, -54, -49, -50, -46, -45, -42, -33, -29, -30, -24, -21, -23,\n",
      "   99, 98, -6, -39, -51, -54, -40, -43, -63, -47, -64, -66, -64, -63, -62, -58, -57, -55, -54, -53, -47, -43, -40, -46, -40, -31, -45, -45,\n",
      "   103, 99, -24, -42, -53, -55, -55, -57, -59, -64, -66, -67, -64, -64, -64, -63, -64, -63, -63, -61, -59, -57, -55, -54, -54, -54, -54, -51,\n",
      "   94, 91, -27, -44, -54, -57, -61, -59, -59, -66, -66, -67, -69, -67, -66, -66, -63, -58, -63, -63, -61, -57, -59, -57, -54, -54, -56, -55,\n",
      "   93, 89, -32, -49, -55, -55, -62, -60, -64, -64, -67, -70, -76, -71, -74, -73, -73, -74, -74, -73, -72, -73, -68, -70, -67, -71, -71, -68,\n",
      "   104, 98, -37, -53, -57, -62, -64, -66, -63, -65, -67, -72, -75, -75, -73, -74, -74, -77, -74, -74, -72, -72, -69, -70, -72, -66, -74, -71,\n",
      "   106, 66, -48, -61, -61, -65, -67, -69, -74, -75, -75, -79, -87, -85, -80, -68, -65, -57, -58, -57, -57, -53, -68, -66, -69, -72, -73, -75,\n",
      "   108, 38, -49, -59, -64, -64, -68, -72, -74, -77, -79, -80, -81, -75, -58, -58, 98, 96, 96, 95, 83, 94, 93, 96, -51, -63, -74, -78,\n",
      "   107, 52, -52, -61, -66, -67, -72, -72, -77, -77, -79, -81, -73, 41, 99, 36, -59, -73, -73, -75, -76, -74, -71, -64, 98, 50, -70, -85,\n",
      "   107, 9, -51, -67, -67, -69, -72, -75, -79, -79, -85, -75, 99, 104, -62, -77, -79, -81, -79, -81, -87, -82, -81, -70, -41, 95, -64, -80,\n",
      "   107, 8, -55, -73, -73, -75, -78, -81, -82, -89, -85, -74, 79, 56, -80, -87, -87, -83, -86, -87, -87, -89, -84, -69, 18, 96, -69, -80,\n",
      "   108, -5, -58, -75, -76, -80, -87, -89, -89, -89, -91, -77, -74, -85, -89, -90, -88, -87, -86, -86, -85, -80, -74, -58, 97, -66, -80, -87,\n",
      "   107, -30, -65, -77, -81, -88, -94, -93, -93, -91, -91, -89, -91, -95, -86, -93, -68, -75, -75, -75, -75, -70, -9, 95, -59, -80, -85, -87,\n",
      "   89, -51, -66, -72, -63, -82, -93, -94, -96, -95, -98, -93, -97, -93, -90, -82, -57, -55, -44, -44, -39, 92, 78, -64, -75, -86, -88, -88,\n",
      "   61, -69, -69, -84, -87, -93, -95, -96, -97, -98, -98, -93, -78, -79, -81, 91, 92, 92, 93, 92, 19, 92, 93, 48, -69, -76, -79, -89,\n",
      "   -13, -82, -85, -91, -93, -98, -100, -101, -101, -95, -78, -85, -91, -81, -90, -72, -69, -74, -80, -86, -81, -80, -74, -39, 88, -12, -74, -88,\n",
      "   0, -7, -85, -102, -101, -100, -102, -105, -105, -103, -104, -102, -102, -101, -95, -92, -92, -97, -97, -99, -96, -93, -92, -81, -54, 89, -70, -90,\n",
      "   -11, 0, -89, -87, -78, -100, -87, -103, -104, -104, -105, -106, -102, -96, -97, -100, -99, -100, -101, -101, -101, -99, -98, -81, -54, 87, -80, -96,\n",
      "   -17, -100, -100, -81, -90, -99, -107, -109, -108, -108, -109, -108, -105, -105, -102, -102, -103, -105, -106, -102, -102, -97, -92, -75, 87, -38, -93, -99,\n",
      "   -60, -103, -107, -108, -108, -106, -111, -110, -109, -110, -110, -107, -106, -102, -101, -102, -104, -101, -99, -101, -96, -80, 65, 83, -83, -99, -97, -99,\n",
      "   -88, -108, -111, -113, -111, -112, -112, -110, -110, -112, -110, -107, -95, -98, -99, -99, -96, -97, -93, -73, 79, 83, -44, -94, -103, -107, -107, -102,\n",
      "   -98, -106, -113, -110, -113, -112, -112, -112, -105, -102, -99, -102, 87, 85, 80, 83, 82, 80, 82, 54, -91, -101, -105, -110, -113, -112, -106, -101,\n",
      "   -108, -101, -113, -114, -113, -114, -115, -115, -113, -113, -114, -110, -108, -103, -105, -100, -103, -102, -102, -105, -109, -111, -112, -114, -114, -112, -112, -107,\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "#VERSION FOR CNN INFERENCE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def capture_and_preprocess_image_cnn():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IOError(\"Cannot capture image from webcam\")\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume largest contour is the number\n",
    "    if contours:\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # Crop and resize the image around the number\n",
    "        cropped = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(cropped, (28, 28))\n",
    "\n",
    "    else:\n",
    "        resized = cv2.resize(gray, (28, 28))  # Default resizing if no contours\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_resized = resized / 255.0\n",
    "\n",
    "    # Show the processed image\n",
    "    plt.imshow(normalized_resized, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Reshape the 28x28 image to 28x28x1\n",
    "    return normalized_resized.reshape(28, 28, 1)\n",
    "\n",
    "\n",
    "# Prepare the image from webcam\n",
    "input_data = capture_and_preprocess_image_cnn()\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "\n",
    "# Quantize the webcam image\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0]['dtype'])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], [quantized_input_data])\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output details and extract output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char webcam_image[] = {\")\n",
    "for row in quantized_input_data.reshape(28, 28):\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m quantized_input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(input_data \u001b[38;5;241m/\u001b[39m input_scale \u001b[38;5;241m+\u001b[39m input_zero_point)\u001b[38;5;241m.\u001b[39mastype(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Set the input tensor\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_input_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[1;34m(self, tensor_index, value)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[0;32m    682\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \n\u001b[0;32m    684\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 0."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load the image, convert to grayscale, and invert (if your model was trained on white-on-black images like MNIST)\n",
    "    image = Image.open(image_path).convert('L')\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    image = image.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert to numpy array and normalize\n",
    "    image_array = np.array(image)\n",
    "    image_array = image_array / 255.0  # Normalize to range [0, 1]\n",
    "    return image_array\n",
    "\n",
    "# Path to your image\n",
    "image_path = r'\\Users\\Sever\\ML_on_MCU\\images\\three.png'  # Replace with your image path\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_array = load_and_preprocess_image(image_path)\n",
    "\n",
    "# Flatten the image to match the model's expected input shape (784,)\n",
    "image_flattened = image_array.flatten()\n",
    "\n",
    "# Load the TensorFlow Lite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details and prepare the quantized input\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "input_data = np.expand_dims(image_flattened, axis=0)  # Add batch dimension\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0][\"dtype\"])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], quantized_input_data)\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extract and process the output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming output also uses quantization\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
    "dequantized_output = (output_data - output_zero_point) * output_scale\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char mnist_image[] = {\")\n",
    "for row in quantized_input_data[0].reshape(28, 28):  # Reshape flat array back to 28x28 for printing\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n",
    "\n",
    "# Show and save the preprocessed image for reference\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.show()  # Optionally display the image in a window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signed char test_image[] = {\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -90, 126, -19, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -41, 124, -46, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 7, 113, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -83, 116, 22, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -44, 126, -65, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 74, 95, -117, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -96, 126, 88, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -33, 126, 67, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 12, 126, -51, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -71, 109, 77, -120, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -4, 127, 37, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 43, 126, -47, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -104, 104, 87, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -8, 126, 31, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 23, 126, 14, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 100, 126, -62, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -67, 123, 126, -62, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 13, 126, 77, -125, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -118, 87, 126, -7, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -123, 70, 48, -118, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "    -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "};\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "image = x_test[2]  # First image in the test dataset\n",
    "\n",
    "# Normalize the image data\n",
    "normalized_image = image / 255.0\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"classification.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details and quantization parameters\n",
    "input_details = interpreter.get_input_details()\n",
    "scale = input_details[0]['quantization'][0]\n",
    "zero_point = input_details[0]['quantization'][1]\n",
    "\n",
    "# Quantize the normalized image\n",
    "quantized_image = np.round(normalized_image / scale + zero_point).astype(np.int8)\n",
    "\n",
    "# Generate C array with rows\n",
    "print(\"signed char test_image[] = {\")\n",
    "for row in quantized_image:\n",
    "    row_string = ', '.join(map(str, row))\n",
    "    print(f\"    {row_string},\")\n",
    "print(\"};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 2 but expected 4 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m quantized_input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(input_data \u001b[38;5;241m/\u001b[39m input_scale \u001b[38;5;241m+\u001b[39m input_zero_point)\u001b[38;5;241m.\u001b[39mastype(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Set the input tensor\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_input_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m     32\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[1;34m(self, tensor_index, value)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[0;32m    682\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \n\u001b[0;32m    684\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 2 but expected 4 for input 0."
     ]
    }
   ],
   "source": [
    "#FCN Inference\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Select the image index you want to use\n",
    "image_index = 4  # Change this index to select a different image\n",
    "\n",
    "# Normalize and prepare input data\n",
    "x_test_normalized = x_test / 255.0\n",
    "x_test_flattened = x_test_normalized.reshape((-1, 784))\n",
    "\n",
    "# Load the TensorFlow Lite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details and prepare the quantized input\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "# Use the specified test image\n",
    "input_data = x_test_flattened[image_index:image_index+1]  # Select one example and add batch dimension\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0][\"dtype\"])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], quantized_input_data)\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extract and process the output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming output also uses quantization\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
    "dequantized_output = (output_data - output_zero_point) * output_scale\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char mnist_image[] = {\")\n",
    "for row in quantized_input_data[0].reshape(28, 28):  # Reshape flat array back to 28x28 for printing\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n",
    "\n",
    "\n",
    "plt.imshow(x_test[image_index], cmap='gray')\n",
    "plt.title(f'Image Index {image_index}')\n",
    "plt.savefig('mnist_image_index_{:04d}.png'.format(image_index))  # Saves the image\n",
    "plt.show()  # Optionally display the image in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n",
      "output: [[-128  127 -128 -128 -128 -128 -128 -128 -128 -128]]\n",
      "signed char mnist_image[] = {\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -90, 126, -19, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -41, 124, -46, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 7, 113, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -83, 116, 22, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -44, 126, -65, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 74, 95, -117, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -96, 126, 88, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -33, 126, 67, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 12, 126, -51, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -71, 109, 77, -120, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -4, 127, 37, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 43, 126, -47, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -104, 104, 87, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -8, 126, 31, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 23, 126, 14, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 100, 126, -62, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -67, 123, 126, -62, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, 13, 126, 77, -125, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -118, 87, 126, -7, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -123, 70, 48, -118, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "   -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128, -128,\n",
      "};\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Select the image index you want to use\n",
    "image_index = 2  # Change this index to select a different image\n",
    "\n",
    "# Normalize and prepare input data for a CNN\n",
    "x_test_normalized = x_test / 255.0\n",
    "x_test_reshaped = x_test_normalized.reshape((-1, 28, 28, 1))  # Reshape for CNN\n",
    "\n",
    "# Load the TensorFlow Lite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details and prepare the quantized input\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "# Use the specified test image\n",
    "input_data = x_test_reshaped[image_index:image_index+1]  # Select one example and add batch dimension\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0][\"dtype\"])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], quantized_input_data)\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extract and process the output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming output also uses quantization\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
    "dequantized_output = (output_data - output_zero_point) * output_scale\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char mnist_image[] = {\")\n",
    "for row in quantized_input_data[0].reshape(28, 28):  # Reshape flat array back to 28x28 for printing\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n",
    "\n",
    "plt.imshow(x_test[image_index], cmap='gray')\n",
    "plt.title(f'Image Index {image_index}')\n",
    "plt.savefig('mnist_image_index_{:04d}.png'.format(image_index))  # Saves the image\n",
    "plt.show()  # Optionally display the image in a window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# open('classification' + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "  c_str = ''\n",
    "\n",
    "  # Create header guard\n",
    "  c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "  c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "  # Add array length at top of file\n",
    "  c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "  # Declare C variable\n",
    "  c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "  hex_array = []\n",
    "  for i, val in enumerate(hex_data) :\n",
    "\n",
    "    # Construct string from hex\n",
    "    hex_str = format(val, '#04x')\n",
    "\n",
    "    # Add formatting so each line stays within 80 characters\n",
    "    if (i + 1) < len(hex_data):\n",
    "      hex_str += ','\n",
    "    if (i + 1) % 12 == 0:\n",
    "      hex_str += '\\n '\n",
    "    hex_array.append(hex_str)\n",
    "\n",
    "  # Add closing brace\n",
    "  c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "  # Close out header guard\n",
    "  c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "  return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write TFLite model to a C source (or header) file\n",
    "with open('modified_data' + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, 'modified_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 2 but expected 4 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m quantized_input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(input_data \u001b[38;5;241m/\u001b[39m input_scale \u001b[38;5;241m+\u001b[39m input_zero_point)\u001b[38;5;241m.\u001b[39mastype(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Set the input tensor\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_input_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\lite\\python\\interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[1;34m(self, tensor_index, value)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[0;32m    682\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \n\u001b[0;32m    684\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 2 but expected 4 for input 0."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load the image, convert to grayscale, and invert (if your model was trained on white-on-black images like MNIST)\n",
    "    image = Image.open(image_path).convert('L')\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    image = image.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert to numpy array and normalize\n",
    "    image_array = np.array(image)\n",
    "    image_array = image_array / 255.0  # Normalize to range [0, 1]\n",
    "    return image_array\n",
    "\n",
    "# Path to your image\n",
    "image_path = r'\\Users\\Sever\\ML_on_MCU\\images\\three.png'  # Replace with your image path\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_array = load_and_preprocess_image(image_path)\n",
    "\n",
    "# Flatten the image to match the model's expected input shape (784,)\n",
    "image_flattened = image_array.flatten()\n",
    "\n",
    "# Load the TensorFlow Lite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='classification.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details and prepare the quantized input\n",
    "input_details = interpreter.get_input_details()\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "input_data = np.expand_dims(image_flattened, axis=0)  # Add batch dimension\n",
    "quantized_input_data = np.round(input_data / input_scale + input_zero_point).astype(input_details[0][\"dtype\"])\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], quantized_input_data)\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extract and process the output\n",
    "output_details = interpreter.get_output_details()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Assuming output also uses quantization\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
    "dequantized_output = (output_data - output_zero_point) * output_scale\n",
    "predicted_class = np.argmax(output_data)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"output: {output_data}\")\n",
    "\n",
    "# Print the quantized image data as a C array\n",
    "print(\"signed char mnist_image[] = {\")\n",
    "for row in quantized_input_data[0].reshape(28, 28):  # Reshape flat array back to 28x28 for printing\n",
    "    print(\"   \" + ', '.join(map(str, row)) + ',')\n",
    "print(\"};\")\n",
    "\n",
    "# Show and save the preprocessed image for reference\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.show()  # Optionally display the image in a window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def non_zero_weights_summary(model):\n",
    "    \"\"\"\n",
    "    Prints a summary of the model showing only non-zero weights for each layer.\n",
    "\n",
    "    Args:\n",
    "    model (tf.keras.Model): The model to summarize.\n",
    "    \"\"\"\n",
    "    print(\"Layer Name, Layer Type, Non-zero Weights\")\n",
    "    total_non_zero = 0\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'weights') and len(layer.weights) > 0:\n",
    "            non_zero_count = 0\n",
    "            for weight in layer.get_weights():\n",
    "                non_zero_count += np.count_nonzero(weight)\n",
    "            print(f\"{layer.name}, {type(layer).__name__}, {non_zero_count}\")\n",
    "            total_non_zero += non_zero_count\n",
    "    print(f\"Total non-zero weights in the model: {total_non_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def prune_weights_magnitude(original_model, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Prunes the model by setting weights below a certain threshold to zero.\n",
    "\n",
    "    Args:\n",
    "    original_model (tf.keras.Model): The trained Keras model.\n",
    "    threshold (float): Threshold below which weights are set to zero.\n",
    "\n",
    "    Returns:\n",
    "    tf.keras.Model: A new model with pruned weights.\n",
    "    \"\"\"\n",
    "    # Clone the model architecture\n",
    "    new_model = tf.keras.models.clone_model(original_model)\n",
    "    \n",
    "    # Compile the new model with appropriate loss for regression\n",
    "    new_model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Copy and prune the weights\n",
    "    for layer in new_model.layers:\n",
    "        if hasattr(layer, 'weights') and layer.weights:\n",
    "            new_weights = []\n",
    "            for w in layer.get_weights():\n",
    "                # Prune weights below the threshold\n",
    "                pruned_weights = tf.where(tf.abs(w) < threshold, tf.zeros_like(w), w)\n",
    "                new_weights.append(pruned_weights.numpy())  # Convert to numpy array for inspection\n",
    "            layer.set_weights(new_weights)\n",
    "            \n",
    "            # Debugging: Print the number of non-zero weights\n",
    "            non_zero_count = np.count_nonzero(new_weights[0])\n",
    "            total_count = np.size(new_weights[0])\n",
    "            print(f\"Layer {layer.name} has {non_zero_count} non-zero weights out of {total_count}\")\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer dense_49 weight shape (20, 10) is not compatible with provided weight shape (784, 20).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_model, pruned_model\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m pruned_reduced_model, pruned_model \u001b[38;5;241m=\u001b[39m \u001b[43mprune_and_reduce_model_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Compile the pruned and reduced model\u001b[39;00m\n\u001b[0;32m    118\u001b[0m pruned_reduced_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[71], line 71\u001b[0m, in \u001b[0;36mprune_and_reduce_model_2\u001b[1;34m(model, threshold)\u001b[0m\n\u001b[0;32m     64\u001b[0m new_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     65\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInputLayer(input_shape\u001b[38;5;241m=\u001b[39minput_shape),\n\u001b[0;32m     66\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(reduced_first_fcn_weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),  \u001b[38;5;66;03m# Adjusted layer\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(reduced_second_fcn_weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Adjusted layer\u001b[39;00m\n\u001b[0;32m     68\u001b[0m ])\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Set the pruned weights to the new model\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreduced_first_fcn_weights\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m new_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mset_weights([reduced_second_fcn_weights])\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_model, pruned_model\n",
      "File \u001b[1;32mc:\\Users\\Sever\\anaconda3.1\\envs\\mfcc_test\\lib\\site-packages\\keras\\engine\\base_layer.py:1643\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1641\u001b[0m ref_shape \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_shape\u001b[38;5;241m.\u001b[39mis_compatible_with(weight_shape):\n\u001b[1;32m-> 1643\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1644\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1645\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not compatible with provided weight \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1646\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1647\u001b[0m weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((param, weight))\n\u001b[0;32m   1648\u001b[0m weight_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer dense_49 weight shape (20, 10) is not compatible with provided weight shape (784, 20)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def prune_weights_magnitude(original_model, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Prunes the model by setting weights below a certain threshold to zero.\n",
    "\n",
    "    Args:\n",
    "    original_model (tf.keras.Model): The trained Keras model.\n",
    "    threshold (float): Threshold below which weights are set to zero.\n",
    "\n",
    "    Returns:\n",
    "    tf.keras.Model: A new model with pruned weights.\n",
    "    \"\"\"\n",
    "    # Clone the model architecture\n",
    "    new_model = tf.keras.models.clone_model(original_model)\n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Copy and prune the weights\n",
    "    for layer, original_layer in zip(new_model.layers, original_model.layers):\n",
    "        if hasattr(layer, 'get_weights') and layer.get_weights():\n",
    "            new_weights = []\n",
    "            for w in original_layer.get_weights():\n",
    "                # Prune weights below the threshold\n",
    "                pruned_weights = np.where(np.abs(w) < threshold, 0, w)\n",
    "                new_weights.append(pruned_weights)\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def prune_and_reduce_model_2(model, threshold=0.4):\n",
    "    pruned_model = prune_weights_magnitude(model, threshold)\n",
    "\n",
    "    # Extract and transpose the weights of the first FCN layer to shape (20, 5)\n",
    "    first_fcn_weights = pruned_model.layers[0].get_weights()[0].T  # Transpose from (5, 20) to (20, 5)\n",
    "    \n",
    "    # Identify rows in the first FCN that are entirely zeros\n",
    "    non_zero_rows = ~np.all(first_fcn_weights == 0, axis=1)\n",
    "\n",
    "    # Extract weights of the second FCN layer (shape should be (20, 1))\n",
    "    second_fcn_weights = pruned_model.layers[1].get_weights()[0]\n",
    "    \n",
    "    # Identify columns in the second FCN that are entirely zeros\n",
    "    non_zero_cols = ~np.all(second_fcn_weights == 0, axis=1)\n",
    "    \n",
    "    print(non_zero_cols)\n",
    "    # Update non_zero_rows to consider both original non-zero rows and non-zero columns of the second layer\n",
    "    non_zero_rows = non_zero_rows  | non_zero_cols\n",
    "\n",
    "    # Similarly, update non_zero_cols to reflect non-zero rows (as the two layers are interconnected)\n",
    "    non_zero_cols = non_zero_rows.copy()\n",
    "\n",
    "    # Prune the first FCN layer by removing zero rows\n",
    "    reduced_first_fcn_weights = first_fcn_weights[non_zero_rows, :].T  # Transpose back to (5, ?)\n",
    "\n",
    "    # Prune the second FCN layer by removing zero columns\n",
    "    reduced_second_fcn_weights = second_fcn_weights[ non_zero_cols, :]\n",
    "\n",
    "    # Create a new model with the reduced FCN layers\n",
    "    input_shape = (784,)\n",
    "    new_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(reduced_first_fcn_weights.shape[1], use_bias=False),  # Adjusted layer\n",
    "        tf.keras.layers.Dense(reduced_second_fcn_weights.shape[1], activation='softmax', use_bias=False)  # Adjusted layer\n",
    "    ])\n",
    "\n",
    "    # Set the pruned weights to the new model\n",
    "    new_model.layers[1].set_weights([reduced_first_fcn_weights])\n",
    "    new_model.layers[2].set_weights([reduced_second_fcn_weights])\n",
    "\n",
    "    return new_model, pruned_model\n",
    "\n",
    "def prune_and_reduce_model(model, threshold=0.4):\n",
    "    pruned_model = prune_weights_magnitude(model, threshold)\n",
    "\n",
    "    # Extract and transpose the weights of the first FCN layer to shape (20, 5)\n",
    "    first_fcn_weights = pruned_model.layers[0].get_weights()[0].T  # Transpose from (5, 20) to (20, 5)\n",
    "    \n",
    "    # Identify rows in the first FCN that are entirely zeros\n",
    "    non_zero_rows = ~np.all(first_fcn_weights == 0, axis=1)\n",
    "\n",
    "    print(non_zero_rows)\n",
    "    # Prune the first FCN layer by removing zero rows\n",
    "    reduced_first_fcn_weights = first_fcn_weights[non_zero_rows, :].T  # Transpose back to (5, ?)\n",
    "    \n",
    "    # Extract weights of the second FCN layer (shape should be (20, 1))\n",
    "    second_fcn_weights = pruned_model.layers[1].get_weights()[0]\n",
    "    \n",
    "    # Identify columns in the second FCN that are entirely zeros\n",
    "    non_zero_cols = ~np.all(second_fcn_weights == 0, axis=0)\n",
    "    print(non_zero_cols)\n",
    "    # Prune the second FCN layer by removing zero columns\n",
    "    reduced_second_fcn_weights = second_fcn_weights[non_zero_rows, :]\n",
    "\n",
    "    # Create a new model with the reduced FCN layers\n",
    "    input_shape = (784,)\n",
    "    \n",
    "    new_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Dense(reduced_first_fcn_weights.shape[1], input_shape=(784,), use_bias=False),  # Linear layer without activation\n",
    "    layers.Dense(reduced_second_fcn_weights.shape[1], activation='softmax', use_bias=False)  # Output layer with 10 classes for digits 0-9\n",
    "    ])\n",
    "\n",
    "    # Set the pruned weights to the new model\n",
    "    \n",
    "    new_model.layers[0].set_weights([reduced_first_fcn_weights])\n",
    "    new_model.layers[1].set_weights([reduced_second_fcn_weights])\n",
    "\n",
    "    return new_model, pruned_model\n",
    "\n",
    "# Example usage\n",
    "pruned_reduced_model, pruned_model = prune_and_reduce_model_2(model, threshold=0.4)\n",
    "\n",
    "# Compile the pruned and reduced model\n",
    "pruned_reduced_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = pruned_reduced_model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "accuracy = pruned_model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name, Layer Type, Non-zero Weights\n",
      "dense_6, Dense, 15680\n",
      "dense_7, Dense, 200\n",
      "Total non-zero weights in the model: 15880\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(non_zero_weights_summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name, Layer Type, Non-zero Weights\n",
      "dense_44, Dense, 3\n",
      "dense_45, Dense, 0\n",
      "Total non-zero weights in the model: 3\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(non_zero_weights_summary(pruned_reduced_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_pruned_model(pruned_model, x_train, y_train, x_test, y_test, epochs=10):\n",
    "    # First, create masks where weights are non-zero\n",
    "    masks = []\n",
    "    for layer in pruned_model.layers:\n",
    "        weights = layer.get_weights()\n",
    "        if weights:  # Check if the layer has weights\n",
    "            mask = [w != 0 for w in weights]\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(None)\n",
    "    \n",
    "    # Compile the model\n",
    "    pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Custom training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = pruned_model(tf.expand_dims(x, 0), training=True)\n",
    "                loss = tf.keras.losses.mean_squared_error(y, predictions)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = tape.gradient(loss, pruned_model.trainable_weights)\n",
    "            \n",
    "            # Apply mask to gradients\n",
    "            masked_gradients = []\n",
    "            for grad, mask in zip(gradients, masks):\n",
    "                if mask is not None:\n",
    "                    masked_grad = grad * mask[0]  # Mask[0] because mask is also a list\n",
    "                    masked_gradients.append(masked_grad)\n",
    "                else:\n",
    "                    masked_gradients.append(grad)\n",
    "            \n",
    "            # Apply gradients\n",
    "            pruned_model.optimizer.apply_gradients(zip(masked_gradients, pruned_model.trainable_weights))\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = pruned_model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(\"Validation loss: {:.4f}\".format(val_loss))\n",
    "    \n",
    "    return pruned_model\n",
    "\n",
    "# Assuming pruned_model is already created and available\n",
    "retrained_pruned_model = train_pruned_model(pruned_reduced_model, x_train, y_train, x_test, y_test, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp1ym0ejmk\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmp1ym0ejmk\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_reduced_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open('classification_pruned_retrained' + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('classification_pruned_retrained' + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, 'regression_pruned_reduced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.6148546  0.         0.        ]\n",
      " [0.         0.56768966 0.         0.         0.        ]\n",
      " [0.         0.70054936 0.         0.         0.        ]\n",
      " [0.         0.6300295  0.         0.5276827  0.        ]\n",
      " [0.         0.         0.60085905 0.         0.        ]\n",
      " [0.59661    0.         0.         0.         0.        ]\n",
      " [0.         0.6035348  0.         0.         0.        ]\n",
      " [0.6069804  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.6993582 ]\n",
      " [0.         0.7548715  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.52693814]\n",
      " [0.         0.62781113 0.         0.         0.        ]\n",
      " [0.6006947  0.         0.         0.         0.        ]\n",
      " [0.5666188  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.7742989 ]]\n"
     ]
    }
   ],
   "source": [
    "print(pruned_reduced_model.layers[0].get_weights()[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmput0di78j\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmput0di78j\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_reduced_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open('regression_pruned_reduced' + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('regression_pruned_reduced' + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, 'regression_pruned_reduced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step\n",
      "MSE pruned: 0.6350662708282471\n",
      "RMSE pruned: 0.7969104647636414\n",
      "7/7 [==============================] - 0s 978us/step\n",
      "MSE full: 0.4786926805973053\n",
      "RMSE full: 0.691876232624054\n"
     ]
    }
   ],
   "source": [
    "predictions = model_pruned.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse.update_state(y_test, predictions)\n",
    "mse_result = mse.result().numpy()\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "print(f\"MSE pruned: {mse_result}\")\n",
    "print(f\"RMSE pruned: {rmse_result}\")\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse.update_state(y_test, predictions)\n",
    "mse_result = mse.result().numpy()\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "print(f\"MSE full: {mse_result}\")\n",
    "print(f\"RMSE full: {rmse_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpojcfl7mc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpojcfl7mc\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_pruned)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open('regression_pruned' + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('regression_pruned' + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, 'regression_pruned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.        , -0.30304018,  0.        ,  0.44263718,  0.        ,\n",
      "         0.46753874,  0.38222703,  0.41902497,  0.        , -0.4409041 ,\n",
      "         0.        , -0.4657119 ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        ,  0.        , -0.41438824],\n",
      "       [ 0.        ,  0.        , -0.38398546,  0.        ,  0.        ,\n",
      "         0.        , -0.397628  ,  0.4592397 ,  0.        ,  0.        ,\n",
      "         0.44924358, -0.48444194,  0.        ,  0.        , -0.45686248,\n",
      "        -0.3285644 ,  0.        ,  0.4179335 ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.4732389 ,  0.48533067,  0.32945833,  0.        ,\n",
      "         0.        ,  0.36855677,  0.        , -0.40763783,  0.        ,\n",
      "         0.35279378,  0.        ,  0.        ,  0.        ,  0.38006082,\n",
      "         0.        ,  0.        ,  0.        ,  0.        ,  0.3511643 ],\n",
      "       [ 0.        , -0.4239313 ,  0.4427186 ,  0.        ,  0.        ,\n",
      "         0.        ,  0.48787495,  0.        ,  0.47734854,  0.        ,\n",
      "         0.        ,  0.        ,  0.39817593,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ,  0.        , -0.3627692 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.39783767,  0.        ,\n",
      "         0.        ,  0.        ,  0.48912504, -0.4008501 ,  0.        ,\n",
      "        -0.43105102,  0.        ,  0.        , -0.45241132, -0.48480123,\n",
      "         0.        ,  0.        ,  0.32283124,  0.4235219 ,  0.        ]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = model_pruned.layers[0].get_weights()\n",
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.38203338  0.          0.          0.          0.\n",
      "  -0.45583427  0.          0.38515183 -0.32091397  0.4898065   0.\n",
      "   0.         -0.42284682  0.         -0.35481924  0.          0.\n",
      "   0.42638984  0.        ]\n",
      " [ 0.         -0.30329525  0.3388928   0.43698487  0.          0.320128\n",
      "   0.          0.         -0.45020577  0.          0.         -0.35465294\n",
      "   0.          0.3669155  -0.39899704  0.4848421   0.          0.\n",
      "  -0.4699785   0.31200513]\n",
      " [ 0.43570867  0.31032595  0.         -0.39224362  0.4366763   0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.30672258  0.32276478 -0.3920224   0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.38097492 -0.37626386  0.          0.37766185  0.\n",
      "   0.          0.         -0.36145598  0.          0.          0.39423648\n",
      "  -0.35788387  0.         -0.46454108  0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.4583255   0.         -0.37518755  0.         -0.36023903  0.43028036\n",
      "   0.          0.          0.          0.         -0.30445078 -0.30515963\n",
      "   0.          0.          0.36953405  0.          0.          0.\n",
      "  -0.33555904 -0.44934097]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming matrix1 is a list containing a single NumPy array\n",
    "# Extract the array from the list\n",
    "matrix1 = matrix1[0]\n",
    "\n",
    "# Copy the matrix\n",
    "matrix_copy = np.copy(matrix1)\n",
    "\n",
    "# Identify rows that are all zeros\n",
    "rows_to_delete = np.all(matrix_copy == 0, axis=1)\n",
    "\n",
    "# Delete rows that are all zeros\n",
    "matrix_copy = matrix_copy[~rows_to_delete]\n",
    "\n",
    "# matrix_copy now contains the matrix without the rows that have only zero entries\n",
    "print(matrix_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7788 - val_loss: 0.7898\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 0.6676\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 0.5491\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4465 - val_loss: 0.4340\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3280\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2541 - val_loss: 0.2366\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1787 - val_loss: 0.1638\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1210 - val_loss: 0.1087\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0800 - val_loss: 0.0717\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0528 - val_loss: 0.0477\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "dense_16, Dense, 68\n",
      "dense_17, Dense, 15\n",
      "Total non-zero weights in the model: 83\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "dense_16, Dense, 100\n",
      "dense_17, Dense, 20\n",
      "Total non-zero weights in the model: 120\n",
      "7/7 [==============================] - 0s 999us/step\n",
      "MSE pruned: 0.4135053753852844\n",
      "RMSE pruned: 0.6430438160896301\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "MSE full: 0.36301368474960327\n",
      "RMSE full: 0.602506160736084\n"
     ]
    }
   ],
   "source": [
    "# # Freeze the pruned weights\n",
    "# for layer in model_pruned.layers:\n",
    "#     if hasattr(layer, 'kernel'):\n",
    "#         weights = layer.get_weights()\n",
    "#         mask = weights[0] != 0  # Create a mask where weights are not zero\n",
    "#         new_kernel = tf.Variable(weights[0], trainable=True)\n",
    "#         layer.kernel = tf.where(mask, new_kernel, tf.zeros_like(new_kernel))\n",
    "\n",
    "# Compile the model again if needed\n",
    "model_pruned.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Continue training\n",
    "history = model_pruned.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "non_zero_weights_summary(model_pruned)\n",
    "non_zero_weights_summary(model)\n",
    "# Predictions\n",
    "predictions = model_pruned.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse.update_state(y_test, predictions)\n",
    "mse_result = mse.result().numpy()\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "print(f\"MSE pruned: {mse_result}\")\n",
    "print(f\"RMSE pruned: {rmse_result}\")\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse.update_state(y_test, predictions)\n",
    "mse_result = mse.result().numpy()\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "print(f\"MSE full: {mse_result}\")\n",
    "print(f\"RMSE full: {rmse_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name, Layer Type, Non-zero Weights\n",
      "dense_16, Dense, 100\n",
      "dense_17, Dense, 20\n",
      "Total non-zero weights in the model: 120\n",
      "Layer Name, Layer Type, Non-zero Weights\n",
      "dense_16, Dense, 68\n",
      "dense_17, Dense, 15\n",
      "Total non-zero weights in the model: 83\n"
     ]
    }
   ],
   "source": [
    "non_zero_weights_summary(model)\n",
    "non_zero_weights_summary(model_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpjczodjgj\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Sever\\AppData\\Local\\Temp\\tmpjczodjgj\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1732"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_pruned)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open('regression_pruned' + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write TFLite model to a C source (or header) file\n",
    "with open('regression_pruned' + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, 'regression_pruned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "Predicted output: [-0.07744812]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming the model is already defined and trained\n",
    "\n",
    "# Create a single data point\n",
    "# Example: [x, x^2, x^3, sin(x), cos(x)] for x = 0.5\n",
    "x_single = np.array([[0.4, 0.4, 0.4, 0.4, 0.4]])  # Shape (1, 5)\n",
    "\n",
    "# Predict using the neural network\n",
    "y_single_pred = model_pruned.predict(x_single)\n",
    "\n",
    "# Output the prediction\n",
    "print(\"Predicted output:\", y_single_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 733us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAAIhCAYAAABXMMsoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydd3xUZfbGn3tnJpM+6SQhCSEIIYTeBCwgRQQE7F0BC6uuXde2q2BZRVxd177+VkXsuggqrChFbFTpkEBoIYQQ0sikZ8p9f3+cvHNnJjNpJDTP9/OBZGZueW9J8j73nPMcRQghwDAMwzAMwzAMw7Qb6skeAMMwDMMwDMMwzJkGCy2GYRiGYRiGYZh2hoUWwzAMwzAMwzBMO8NCi2EYhmEYhmEYpp1hocUwDMMwDMMwDNPOsNBiGIZhGIZhGIZpZ1hoMQzDMAzDMAzDtDMstBiGYRiGYRiGYdoZFloMwzAMwzAMwzDtDAsthjlBbNu2DTNmzEDXrl0RGBiI0NBQDBw4EHPnzkVZWdnJHl6HM336dKSmpp7sYRw3mzdvxsiRI2GxWKAoCl555ZVm1ykpKYHZbIaiKPj999/bvO8333wT8+bNa/P6rWHevHlQFAW5ubktWs7Xv4ceeuiEjNUfn3zyid/roygKZs+efULHcyKYPXs2FEVBXFwcKisrG32empqKiy+++CSMrHXk5uZCUZRm73e5nKIo+Oyzzxp9Ls9HSUlJq8ewevVqzJ49G+Xl5a1et6NYtWoVFEXBf//731av636uvP8NHjy4A0bbcpo616NGjcKoUaNO+JgYpj0wnuwBMMwfgf/7v//DnXfeifT0dPzlL39Br169YLfb8fvvv+Ptt9/GmjVrsHDhwpM9zA7liSeewL333nuyh3Hc3HzzzaiursZnn32GyMjIFonHDz/8EDabDQDw7rvvtnlS8+abbyImJgbTp09v0/odyfvvv4+ePXt6vJeYmHiSRkN88skn2LFjB+67775Gn61ZswZJSUknflAniOLiYsydOxfPPPPMyR7KCeOvf/0rLr/8cphMpnbZ3urVq/HUU09h+vTpiIiIaJdtngrcfffduO666zzeCw0NPUmjIZo612+++ebJGRTDtAMstBimg1mzZg3uuOMOjBs3DosWLYLZbHZ9Nm7cODz44INYunTpSRxhx1JTU4Pg4GB069btZA+lXdixYwduu+02TJgwocXrvPfee4iLi0OXLl3w6aef4uWXX0ZQUFAHjvLE07t375P+VLw1DBs27GQPoUO56KKL8M9//hN//vOfER8ff7KHg9raWgQGBkJRlA7Z/oQJE/Ddd9/h7bffxt13390h+ziZOJ1OOByOdtlWSkrKaXX/9+rV62QPgWHaDKcOMkwH89xzz0FRFLzzzjseIksSEBCAKVOmuF5rmoa5c+eiZ8+eMJvNiIuLw0033YT8/HyP9UaNGoXevXtjzZo1GDFiBIKCgpCamor3338fALBkyRIMHDgQwcHB6NOnTyMxJ1NqNm/ejMsuuwzh4eGwWCy44YYbUFxc7LHs559/jgsvvBAJCQkICgpCRkYGHn30UVRXV3ssN336dISGhmL79u248MILERYWhjFjxrg+847+fPnllzj77LNhsVgQHByMtLQ03HzzzR7L5OXl4YYbbkBcXBzMZjMyMjLw0ksvQdM01zIyJeYf//gHXn75ZXTt2hWhoaEYPnw41q5d29TlcbFjxw5MnToVkZGRCAwMRP/+/fHBBx+4Ppcpcg6HA2+99ZYr5aY51q1bhx07duDGG2/EbbfdBqvVigULFjRaTtM0vPbaa+jfvz+CgoIQERGBYcOG4ZtvvgFAKV87d+7ETz/95Nq3PJ/+0vxkmtGqVatc7y1btgxTp05FUlISAgMDcdZZZ+FPf/pTm1KrWoq/NL3U1FSP6Jw8jh9//BF33HEHYmJiEB0djcsuuwwFBQWN1v/kk08wfPhwhIaGIjQ0FP3798e7774LgH4+lixZgoMHD3qkSDU1pubuAUA/p59++in++te/IjExEeHh4Rg7dix2797d5HlYtGgRFEXBihUrGn0m76lt27YBAPbv349rrrkGiYmJMJvN6NSpE8aMGYMtW7Y0uQ/Js88+C4fD0aL0SJvNhmeffdb1Oyc2NhYzZsxo9Hugtdfxhx9+wM0334zY2FgEBwejvr4ee/fuxYwZM9C9e3cEBwejc+fOmDx5MrZv396i4/LH6NGjMX78eDzzzDM+Uya9Wb58OcaMGYPw8HAEBwfjnHPO8bgus2fPxl/+8hcAQNeuXV33z6pVq/CXv/wFFosFTqfTtfzdd98NRVHw4osvut4rLS2Fqqp47bXXXO+15vfZ3Llz8eyzz6Jr164wm8348ccffR5LRUUFxo8fj06dOmH9+vUtP2k+8Jem5/37u7W/c9etW4fJkycjOjoagYGB6NatmyvS3NS59jemsrIy3HnnnejcuTMCAgKQlpaGv/71r6ivr/dYTlEU3HXXXfjwww+RkZGB4OBg9OvXD4sXL27zOWKY1sBCi2E6EKfTiZUrV2LQoEFITk5u0Tp33HEHHnnkEYwbNw7ffPMNnnnmGSxduhQjRoxoNBkuLCzEjBkzcOutt+Lrr79Gnz59cPPNN+Ppp5/GY489hocffhgLFixAaGgoLrnkEp+T1UsvvRRnnXUW/vvf/2L27NlYtGgRxo8fD7vd7lpmz549mDhxIt59910sXboU9913H7744gtMnjy50fZsNhumTJmC0aNH4+uvv8ZTTz3l8zjXrFmDq6++Gmlpafjss8+wZMkSPPnkkx5PbYuLizFixAj88MMPeOaZZ/DNN99g7NixeOihh3DXXXc12uYbb7yBZcuW4ZVXXsHHH3+M6upqTJw4EVartclzvnv3bowYMQI7d+7Eq6++iq+++gq9evXC9OnTMXfuXADApEmTsGbNGgDAFVdcgTVr1rheN4Wc+N9888245pprEBwc7HrPnenTp+Pee+/FkCFD8Pnnn+Ozzz7DlClTXOJp4cKFSEtLw4ABA1z7bku66b59+zB8+HC89dZb+OGHH/Dkk09i3bp1OPfccz2ueWuRT9zd/7WVW2+9FSaTCZ988gnmzp2LVatW4YYbbvBY5sknn8T111+PxMREzJs3DwsXLsS0adNw8OBBAJRudM455yA+Pt51vpq6Xi25B9x5/PHHcfDgQfznP//BO++8gz179mDy5Mkek29vLr74YsTFxbkehrgzb948DBw4EH379gUATJw4ERs3bsTcuXOxbNkyvPXWWxgwYECL64W6dOmCO++8E++++y5ycnL8LqdpGqZOnYo5c+bguuuuw5IlSzBnzhwsW7YMo0aNQm1tbYv254ubb74ZJpMJH374If773//CZDKhoKAA0dHRmDNnDpYuXYo33ngDRqMRZ599drNCtTleeOEFlJSUeIgdX3z00Ue48MILER4ejg8++ABffPEFoqKiMH78eJfYuvXWW12Rsa+++sp1/wwcOBBjx45FRUWFh6hZvnw5goKCsGzZMtd7K1asgBACY8eOBdD632evvvoqVq5ciX/84x/47rvvGqXmAkB+fj7OPfdcHDx4EGvWrMHQoUObPU+apjX6WRVCNLueL1ryO/f777/Heeedh7y8PLz88sv47rvv8Le//Q1Hjx4F0PS59kVdXR0uuOACzJ8/Hw888ACWLFmCG264AXPnzsVll13WaPklS5bg9ddfx9NPP40FCxYgKioKl156Kfbv39+mY2aYViEYhukwCgsLBQBxzTXXtGj57OxsAUDceeedHu+vW7dOABCPP/64672RI0cKAOL33393vVdaWioMBoMICgoShw8fdr2/ZcsWAUC8+uqrrvdmzZolAIj777/fY18ff/yxACA++ugjn2PUNE3Y7Xbx008/CQBi69atrs+mTZsmAIj33nuv0XrTpk0TXbp0cb3+xz/+IQCI8vJyv+fj0UcfFQDEunXrPN6/4447hKIoYvfu3UIIIQ4cOCAAiD59+giHw+Fabv369QKA+PTTT/3uQwghrrnmGmE2m0VeXp7H+xMmTBDBwcEeYwQg/vznPze5PUl1dbUIDw8Xw4YNc703bdo0oSiK2Lt3r+u9n3/+WQAQf/3rX5vcXmZmphg5cmSj999//30BQBw4cMDj/R9//FEAED/++KPP7clrefDgQQFAfP31181u09++ff2z2+1CCDpns2bNarRuly5dxLRp0xpty/v+nzt3rgAgjhw5IoQQYv/+/cJgMIjrr7++ybFNmjTJ455zx3tMLb0H5DmdOHGix3JffPGFACDWrFnT5JgeeOABERQU5HFPZWVlCQDitddeE0IIUVJSIgCIV155pclt+UL+XBcXF4uSkhJhsVjE5Zdf7vq8S5cuYtKkSa7Xn376qQAgFixY4LGdDRs2CADizTffdL3X2ut40003NTteh8MhbDab6N69u8fvIvkz/f777ze5vlzuxRdfFEIIcf3114uQkBDXveJ+PoSgn8moqCgxefJkj+04nU7Rr18/MXToUNd7L774os+fgerqahEQECCefvppIYQQ+fn5AoB45JFHRFBQkKirqxNCCHHbbbeJxMRE13qt/X3WrVs3YbPZPJaV99+XX34pNm/eLBITE8V5550nSktLmzxP7tv19W/ZsmVCCPq74ut3jPfv79b8zu3WrZvo1q2bqK2t9Ts2f+fa15jefvttAUB88cUXHsu98MILAoD44YcfXO8BEJ06dRIVFRWu9woLC4WqquL555/3Ox6GaS84osUwpxAyNcTb7GDo0KHIyMholHKUkJCAQYMGuV5HRUUhLi4O/fv39zAiyMjIAADX0353rr/+eo/XV111FYxGo0eayv79+3HdddchPj4eBoMBJpMJI0eOBABkZ2c32ubll1/e7LEOGTLEtb8vvvgChw8fbrTMypUr0atXr0ZPaadPnw4hBFauXOnx/qRJk2AwGFyvZXTA13F772fMmDGNoo7Tp09HTU1NiyJXvvjiiy9QUVHhkQ558803QwjhEdX47rvvAAB//vOf27Sf1lBUVITbb78dycnJMBqNMJlM6NKlCwDf17KlzJ8/Hxs2bPD4ZzS2rQzYPZUWaHwdly1bBqfT2a7nq7X3QHNj9MfNN9+M2tpafP7556733n//fZjNZpdBQVRUFLp164YXX3wRL7/8MjZv3uyRWtZSoqOj8cgjj2DBggVYt26dz2UWL16MiIgITJ482SPC0b9/f8THx3uknbYWX78HHA4HnnvuOfTq1QsBAQEwGo0ICAjAnj17juv+kzz77LOw2+1+I+mrV69GWVkZpk2b5nG8mqbhoosuwoYNGxqlRHsTHByM4cOHY/ny5QDofoyIiMBf/vIX2Gw2/PrrrwAoyiWjWUDrf59NmTLFr7GHjBKdf/75WLZsGaKiopo+MW7ce++9jX5Wzz777Bav705zv3NzcnKwb98+3HLLLQgMDGzTPrxZuXIlQkJCcMUVV3i8L/9uev+dvOCCCxAWFuZ63alTJ8TFxTX7s8ow7QELLYbpQGJiYhAcHIwDBw60aPnS0lIAJKC8SUxMdH0u8fXHNSAgoNH7AQEBACjlwhvvQnmj0Yjo6GjXvqqqqnDeeedh3bp1ePbZZ7Fq1Sps2LABX331FQA0Si0KDg5GeHh4k8cJAOeffz4WLVoEh8OBm266CUlJSejduzc+/fRT1zKlpaV+z4X83J3o6GiP17Imrrn0p9bup6W8++67CAwMxEUXXYTy8nKUl5ejb9++SE1Nxbx581xpZsXFxTAYDB1uWqBpGi688EJ89dVXePjhh7FixQqsX7/eVVNxPGliGRkZGDx4sMe/ttLcdZS1Q+3pGnii7rXMzEwMGTLEJbSdTic++ugjTJ061fVzK+u4xo8fj7lz52LgwIGIjY3FPffc06L6I3fuu+8+JCYm4uGHH/b5+dGjR1FeXo6AgACYTCaPf4WFhcdVu+frfD7wwAN44okncMkll+Dbb7/FunXrsGHDBvTr1++47j9Jamoq7rzzTvznP//Bnj17Gn0u09WuuOKKRsf7wgsvQAjRonYbY8eOxdq1a1FdXY3ly5dj9OjRiI6OxqBBg7B8+XIcOHAABw4c8BBarb3HfC0rWbRoEWpra3HHHXf4rP1tiqSkpEY/q+5CpDWcrJ/V+Pj4RjWycXFxMBqNzf6synG2x/3GMM3BroMM04EYDAaMGTMG3333HfLz85v9YyP/IBw5cqTRsgUFBYiJiWn3MRYWFqJz586u1w6HA6Wlpa6xrFy5EgUFBVi1apUrigXAb61Ia1zFpk6diqlTp6K+vh5r167F888/j+uuuw6pqakYPnw4oqOjceTIkUbryVqz9jofHbGfnJwc15PtlJQUn8t8//33mDhxImJjY+F0OlFYWNjk5Mof8kmxdyG49yR5x44d2Lp1K+bNm4dp06a53t+7d2+r99kazGZzo7EBbRewsbGxAKg+paW1j81xou41AJgxYwbuvPNOZGdnY//+/Thy5AhmzJjhsUyXLl1ctXw5OTn44osvMHv2bNhsNrz99tst3ldQUBBmz56NmTNnYsmSJY0+l4Yj/pxP3Sfgrb2Ovn4XfPTRR7jpppvw3HPPebxfUlLSbhbqf/vb3/Dee+/h8ccfR2Zmpsdn8jq+9tprfp33OnXq1Ow+xowZgyeeeAI///wzVqxYgVmzZrne/+GHH9C1a1fXa0lr77Gmfpf+85//xOeff44JEyZg4cKFuPDCC5sdc0sIDAz0WdPaVsHt/rPaXkRHR2PdunUQQnico6KiIjgcjg75O8kwbYUjWgzTwTz22GMQQuC2225z9VJyx26349tvvwVAzlkATUbc2bBhA7Kzsz3+aLcXH3/8scfrL774Ag6Hw+XyJP+QeT81/fe//91uYzCbzRg5ciReeOEFANQUGKBJSlZWFjZt2uSx/Pz586EoCi644IJ22f+YMWNcgtJ7P8HBwW2yQpaT5P/7v//Djz/+6PHvf//7H0wmE9577z0AcFnFv/XWW01u099TWOkGJh3rJNKxUHIirqUvUlNTG41t5cqVqKqqatP2LrzwQhgMhjafL190xD3gj2uvvRaBgYGYN28e5s2bh86dOzc5Ue7Rowf+9re/oU+fPo1+FlrCzTff7HIK9U5BvPjii1FaWgqn09koyjF48GCkp6e7lm2P66goSqP7b8mSJT5Th9uKTJn873//28iF75xzzkFERASysrJ8Hu/gwYNdGQBNRSmHDh2K8PBwvPLKKygsLMS4ceMAUKRr8+bN+OKLL9CrVy+PFO72/H0WGBiIr776ChdffDGmTJmCr7/+usXrNkVqaipycnI8BHVpaSlWr17dpu316NED3bp1w3vvvedTpEtaGhEG6DxWVVVh0aJFHu/Pnz/f9TnDnCpwRIthOhjp8HbnnXdi0KBBuOOOO5CZmQm73Y7NmzfjnXfeQe/evTF58mSkp6dj5syZeO2116CqKiZMmIDc3Fw88cQTSE5Oxv3339/u4/vqq69gNBoxbtw47Ny5E0888QT69euHq666CgAwYsQIREZG4vbbb8esWbNgMpnw8ccfY+vWrce13yeffBL5+fkYM2YMkpKSUF5ejn/9618e9V/3338/5s+fj0mTJuHpp59Gly5dsGTJErz55pu444470KNHj+M+fgCYNWsWFi9ejAsuuABPPvkkoqKi8PHHH2PJkiWYO3cuLBZLq7bncDgwf/58ZGRk4NZbb/W5zOTJk/HNN9+guLgY5513Hm688UY8++yzOHr0KC6++GKYzWZs3rwZwcHBLkeuPn364LPPPsPnn3+OtLQ0BAYGok+fPhgyZAjS09Px0EMPweFwIDIyEgsXLnRF1CQ9e/ZEt27d8Oijj0IIgaioKHz77bceTmkdwY033ognnngCTz75JEaOHImsrCy8/vrrrT6vktTUVDz++ON45plnUFtbi2uvvRYWiwVZWVkoKSlx1ef06dMHX331Fd566y0MGjQIqqr6TWls73ugKSIiInDppZdi3rx5KC8vx0MPPQRV1Z97btu2DXfddReuvPJKdO/eHQEBAVi5ciW2bduGRx99tNX7MxgMeO6553DppZcC0OtoAOCaa67Bxx9/jIkTJ+Lee+/F0KFDYTKZkJ+fjx9//BFTp051rdce1/Hiiy/GvHnz0LNnT/Tt2xcbN27Eiy++2O7No++77z688cYbrvpHSWhoKF577TVMmzYNZWVluOKKKxAXF4fi4mJs3boVxcXFLgHfp08fAMC//vUvTJs2DSaTCenp6QgLC4PBYMDIkSPx7bffomvXrq4+geeccw7MZjNWrFiBe+65x2Pf7f37zGQy4dNPP8Wtt96KK664AvPnz8e1117b1lMGgK7xv//9b9xwww247bbbUFpairlz57YoHdwfb7zxBiZPnoxhw4bh/vvvR0pKCvLy8vD999+7HvQ1da69uemmm/DGG29g2rRpyM3NRZ8+ffDrr7/iueeew8SJEz3SNRnmpHMSjTgY5g/Fli1bxLRp00RKSooICAgQISEhYsCAAeLJJ58URUVFruWcTqd44YUXRI8ePYTJZBIxMTHihhtuEIcOHfLY3siRI0VmZmaj/Xi7ikng5ZYn3bg2btwoJk+eLEJDQ0VYWJi49tprxdGjRz3WXb16tRg+fLgIDg4WsbGx4tZbbxWbNm1q5Ao2bdo0ERIS4vP4vV2rFi9eLCZMmCA6d+4sAgICRFxcnJg4caL45ZdfPNY7ePCguO6660R0dLQwmUwiPT1dvPjii8LpdLqW8XYe8z5uX05p3mzfvl1MnjxZWCwWERAQIPr16+fT8cz7PPpi0aJFzbrGLV26VAAQL730khCCrvs///lP0bt3bxEQECAsFosYPny4+Pbbb13r5ObmigsvvFCEhYUJAB7nMycnR1x44YUiPDxcxMbGirvvvlssWbKkketgVlaWGDdunAgLCxORkZHiyiuvFHl5eY3OU2tdBzds2OB3mfr6evHwww+L5ORkERQUJEaOHCm2bNni163Oe1v+3BPnz58vhgwZIgIDA0VoaKgYMGCAxzUrKysTV1xxhYiIiBCKogj3P3m+7ouW3APurm/utNQlT/LDDz+4HN9ycnI8Pjt69KiYPn266NmzpwgJCRGhoaGib9++4p///KeHw5svvF323BkxYoQA0Oj3g91uF//4xz9Ev379XOeyZ8+e4k9/+pPYs2ePa7njvY5CCHHs2DFxyy23iLi4OBEcHCzOPfdc8csvvzRylmur66A777zzjusce5+Pn376SUyaNElERUUJk8kkOnfuLCZNmtTouj722GMiMTFRqKra6B7817/+JQCI2267zWOdcePGCQDim2++aTSm4/195uv+0zRN3HPPPUJVVfF///d/bTpX7nzwwQciIyNDBAYGil69eonPP//cr+tgS3/nrlmzRkyYMEFYLBZhNptFt27dGjne+jvXvpwQS0tLxe233y4SEhKE0WgUXbp0EY899pjL8dF9LL5+X3vfswzTUShCtLF5AsMwpzWzZ8/GU089heLiYs5pZxiGYRiGaWe4RothGIZhGIZhGKadYaHFMAzDMAzDMAzTznDqIMMwDMMwDMMwTDvDES2GYRiGYRiGYZh2hoUWwzAMwzAMwzBMO8NCi2EYhmEYhmEYpp3hhsXNoGkaCgoKEBYWBkVRTvZwGIZhGIZhGIY5SQghUFlZicTERI9m875godUMBQUFSE5OPtnDYBiGYRiGYRjmFOHQoUNISkpqchkWWs0QFhYGgE5meHj4SR4NwzAMwzAMwzAni4qKCiQnJ7s0QlOw0GoGmS4YHh7OQothGIZhGIZhmBaVFLEZBsMwDMMwDMMwTDvDQothGIZhGIZhGKadYaHFMAzDMAzDMAzTznCNFsMwDMMwzB8EIQQcDgecTufJHgrDnJIYDAYYjcZ2aevEQothGIZhGOYPgM1mw5EjR1BTU3Oyh8IwpzTBwcFISEhAQEDAcW2HhRbDMAzDMMwZjqZpOHDgAAwGAxITExEQENAuT+wZ5kxCCAGbzYbi4mIcOHAA3bt3b7YpcVOw0GIYhmEYhjnDsdls0DQNycnJCA4OPtnDYZhTlqCgIJhMJhw8eBA2mw2BgYFt3habYTAMwzAMw/xBOJ6n8wzzR6G9fk74p41hGIZhGIZhGKadYaHFMAzDMAzDMAzTzrDQYhiGYRiGYZg2oigKFi1adLKH0WpSU1PxyiuvnOxhnNGw0GIYhmEYhmFOeVavXg2DwYCLLrqo1eueTFExffp0KIrS6N/evXtPyP7nzZuHiIiIRu9v2LABM2fOPCFj+KPCQothGIZhGIZpMZoG5OYC27fTV007Mft97733cPfdd+PXX39FXl7eidlpO3HRRRfhyJEjHv+6du16UscUGxvLDpQdDAstplWcrF+uDMMwDMOcfLKzgTlzgCefBJ55hr7OmUPvdyTV1dX44osvcMcdd+Diiy/GvHnzGi3zzTffYPDgwQgMDERMTAwuu+wyAMCoUaNw8OBB3H///a5oEgDMnj0b/fv399jGK6+8gtTUVNfrDRs2YNy4cYiJiYHFYsHIkSOxadOmVo/fbDYjPj7e45/BYMD06dNxySWXeCx73333YdSoUa7Xo0aNwj333IOHH34YUVFRiI+Px+zZsz3WKS8vx8yZM9GpUycEBgaid+/eWLx4MVatWoUZM2bAarW6jl2u6x3ly8vLw9SpUxEaGorw8HBcddVVOHr0qOtzeb4+/PBDpKamwmKx4JprrkFlZWWrz8cfBRZaTIs5Wb9cfcGCj2EYhmFOLNnZwKuvAps3AzExQHo6fd28md7vyPnA559/jvT0dKSnp+OGG27A+++/DyGE6/MlS5bgsssuw6RJk7B582asWLECgwcPBgB89dVXSEpKwtNPP+2KJrWUyspKTJs2Db/88gvWrl2L7t27Y+LEiSdcXHzwwQcICQnBunXrMHfuXDz99NNYtmwZAGpGPWHCBKxevRofffQRsrKyMGfOHBgMBowYMQKvvPIKwsPDXcf+0EMPNdq+EAKXXHIJysrK8NNPP2HZsmXYt28frr76ao/l9u3bh0WLFmHx4sVYvHgxfvrpJ8yZM+eEnIPTEW5YzLQI+cu1pARITgZCQoDqavrleugQcM89QEbGiRvLwoXArl1AXR0QGAj07AlceumJGwPDMAzD/JHQNPrbW1IC9OoFNASFEB5Or7OygEWLSHx1RKuud999FzfccAMASsOrqqrCihUrMHbsWADA3//+d1xzzTV46qmnXOv069cPABAVFQWDwYCwsDDEx8e3ar+jR4/2eP3vf/8bkZGR+Omnn3DxxRe3eDuLFy9GaGio6/WECRPw5Zdftnj9vn37YtasWQCA7t274/XXX8eKFSswbtw4LF++HOvXr0d2djZ69OgBAEhLS3Ota7FYoChKk8e+fPlybNu2DQcOHEBycjIA4MMPP0RmZiY2bNiAIUOGACBRN2/ePISFhQEAbrzxRqxYsQJ///vfW3wsfyQ4onWacDIjON6/XMPDAYOBvmZkAAcPAm+/Dezf3/HjOplP0xiGYRjmj0peHj3gTE7WRZZEUYCkJPob3BGlU7t378b69etxzTXXAACMRiOuvvpqvPfee65ltmzZgjFjxrT7vouKinD77bejR48esFgssFgsqKqqanWN2AUXXIAtW7a4/r366qutWr9v374erxMSElBUVASAjj0pKcklstpCdnY2kpOTXSILAHr16oWIiAhku02uUlNTXSLLexxMYziidRrgK4LTowcwbBjQqRMQFgakpHTMEyTA/y/X4mJ6v6AA2LmTIluDB3dcZOlkP01jOhZNo3utsrLj72mGYRimdVRW0hwkJMT35yEhwOHDtFx78+6778LhcKBz586u94QQMJlMOHbsGCIjIxEUFNTq7aqq6pF+CAB2u93j9fTp01FcXIxXXnkFXbp0gdlsxvDhw2Gz2Vq1r5CQEJx11lltGgMAmEwmj9eKokBreLrdlmP3Rgjhql1r6v2mxsE0hoXWKY6vlL28POCzz4D584G0NCAurmNT53z9ci0uBtatA2pqSOgoCn3ekamErXma5lbHypwGcDoowzDMqU1YGP1urq6mv/veVFfT527BjnbB4XBg/vz5eOmll3DhhRd6fHb55Zfj448/xl133YW+fftixYoVmDFjhs/tBAQEwOl0erwXGxuLwsJCDzGxZcsWj2V++eUXvPnmm5g4cSIA4NChQygpKWmno6Mx7Nixw+O9LVu2NBI0TdG3b1/k5+cjJyfHZ1TL17F706tXL+Tl5eHQoUOuqFZWVhasVisy+A9xm+HnxacwvlL2ysooeqRpgBBAfT0QHd2xqXPuv1wB2u+uXSSyYmNJ5JhMNI5evWi8ixa1fxphS56m1dV1zNM0X7AhR/vA6aAMwzCnPikp9ADs0CGaB7gjBJCfTw/GUlLad7+LFy/GsWPHcMstt6B3794e/6644gq8++67AIBZs2bh008/xaxZs5CdnY3t27dj7ty5ru2kpqbi559/xuHDh11CadSoUSguLsbcuXOxb98+vPHGG/juu+889n/WWWfhww8/RHZ2NtatW4frr7++XSJIktGjR+P333/H/PnzsWfPHsyaNauR8GqOkSNH4vzzz8fll1+OZcuW4cCBA/juu++wdOlSAHTssqatpKQENTU1jbYxduxY9O3bF9dffz02bdqE9evX46abbsLIkSNdpiJM62GhdQrjHcHxFjjR0SS8hOhYgeP9y9VqpX1ZLPR5RQVNjC2Wjs3T9hZ83nTU0zRfnEoOjKczTdX/deQ9zTAMw7QOVaUsg5gYStW3WgGHg75mZdH7l1zS/inf7777LsaOHQuLnHS4cfnll2PLli3YtGkTRo0ahS+//BLffPMN+vfvj9GjR2PdunWuZZ9++mnk5uaiW7duiI2NBQBkZGTgzTffxBtvvIF+/fph/fr1jRz53nvvPRw7dgwDBgzAjTfeiHvuuQdxcXHtdnzjx4/HE088gYcffhhDhgxBZWUlbrrpplZvZ8GCBRgyZAiuvfZa9OrVCw8//LArijVixAjcfvvtuPrqqxEbG+shQCWKomDRokWIjIzE+eefj7FjxyItLQ2ff/75cR/jHxlFeCeGMh5UVFTAYrHAarUi3FesvAPZvp0m8enpNPksLwd+/BEIDgbMZpp8lpYC559PtVpWK71++un2T51zT2EMDAR+/50ETVUVjefss0n8AfSLNycHeOIJoE+f1u2nqTodTSMxs3mzZ40WQAIwKwsYOBB45JGOre3x58B46BD9oTmRDoynO7m5JFJjYnynonTkPc0wDPNHoq6uDgcOHEDXrl0RGBjY5u34SvXOyCCRxX/7mDOFpn5eWqMNuEbrFMY7H7q+nkSM0Ui/3GpqSHwEBNDyHVmImpFBAmLhQhJZMuqcmEjRLimygLZHlpqr05FP0w4dIlGVlKSLnPz8jnua5g4bcrQvJ7O4mmEYhmk9GRn0N47NiximeVhoncLIlD0ZwTGbSWjl5gI2G1BbS9Gk7dvpF19AQMemzslfrrm5wL/+BezZQy6D7r9cZZ72wIGty9NuaZ8ud8G3axdNwgMDaX8n4mlaexlysMMecbKKqxmGYZi2o6qcZcAwLYGF1imMdwQnKIgm5hUVZD4REkKOg4WFlGIVEwOMGqULnI6YzKsqOR3efrtuVHC8kaXWRolO5tO09ojAsMOejvfDBO900LaIdoZhGIZhmFMBFlqnODKCs2AB/dM0imwZDCSyLBZKKTx8mFIKp0whwdHRvbeaiyzJyFdLhFBbokQn4mmaL6F6vBGYlkbumhtHR4jKkxFla890UI4SMgzDMAxzKsFC6zQgIwO47jpg0yagd28SH/n5ZBJQWkoCq0sXmpSGhLRv762mJq/+Iku7d5NpRUsjNqdinY6/qNPUqW2PwLSlvutERb9OZpStPdJBOUrIMAzDMMypBgut04TqahJUqakUzeraldIF6+spwhUSQjVTViuwZInnZL642H/vraaaC7dk8uodWWpLxOZUq9Np7hgmTWpbBKa1kbu2nMuOON4T4aJ4POmgp8L4GYZhGIZhvOHEmtME7x5SigJERFAaYEQEuQAGBtIk9Xh7b2kasHw5MGsW8OuvtE5Lmsi2tSdSa5sgdmSjYE0DvvoKOHiQjlvTaLLvfgzbtwN33QUMGEARxZwc+jpwYNOT+tY0XJbnsriYnB1ra3UB0p79pU6lPlZStPfpQ19bmi54qoyfYRiGYRjGHY5onSZIMbJpE9C5M7kOms1602CZshYW5jmZd28urCjkTFhZSVEtf1EUWQ9WUgJERtL2OncGQkNp0n/4MImRa68l4SejD01FbABabvVq+jdihD6Rbk2dTkeniK1cCfz3v7TtgwcpihgTo1vYy/N17bXAo4+2LgLTmshdXh6wbh1dgz17dFt/OZaWuhs2R3u5KJ4sTvfxMwzDMAxz5sJC6zRBVYG+fenp/IYNJLLMZpqwh4ZS3dUll5AzYWAgNRIWAigooGiWnNjbbDRhN5vptXv9k0zBOniQhFhiIn3dsQPYupVEV0gIjWXnTmDjRtqWFDuZmb4jNsXFNBkuKqKmyy+8AAwf7imOWlKn09EpYtnZwDvv0Hg7d6ZzZLcDR46QYD37bDoH8ny11pCjNQ57335L5z0ggCKWJpPnWAYP1qNf/miJOcSpWB/XGk738TMMwzAMc+bCQus0ITubaq8iI2nSLSeY+flAfDzVDWVk0OQ6KgpYtowm1bW1JEzq6mi56mogIUGPhMkoSkgImWWUlNCE/OBBwOkkcSQE/XM4aJ3cXJr0d+9O4kqKnZ07SZi5R2yKiykyU1NDwiUigow4fImjpup03FP6UlIap/Qdb6NgmYJWVUXnT1Xpn9lMkSwpFvv0aXu9WEsjdwClbDoctG8pit3Hsn07ieum3A1bEvk71erjWsvpPn6GYRjmj8e8efNw3333oby8HAAwe/ZsLFq0CFu2bDnhY1EUBQsXLsQlcgJympCamor77rsP991338keSpNwjdZpgHsdyrBhwNixwOjRwJgxNDFPTqaJt6aR419hIQmsujoSZqGhwLFjlIKmqjThlvVbsv4J0FOwAgOpzqWoiARVcDD9q6mh+i5Foe2UlnqKnfp62mdeHm1b00hQlZXRGOrrSWQlJfmvn/FXpyNT+g4cAH75BfjxRxIjxcX0uXtaYlvqcWQKWno6iRmrVa8XUxQ6xuJiqsdyrxfzvk7N1Y7JyF1T9V15eXQNk5KoZ5p73Zqi0LHm55Ng9jUOGfnbvJnEW1P1da2tjzvVON3HzzAMwzTP9OnToSgK5syZ4/H+okWLoLilh6xatQqKoqB3795wOp0ey0ZERGDevHl+9zF79mwoigJFUWAwGJCcnIxbb70VxXKi0YE89NBDWLFiRYuXVxQFixYt6rgBuSHPvfe/vXv3npD9z5s3DxEREY3e37BhA2bOnHlCxnA8cETrNMBXHYr7PSdrl3JzSZA5ncD48SS6Skr0NEOHg6JhFgttMz+fRMWUKfTkX6ZgqSoJo4MH6auikPByOGjbAG2jspIESUQELZOcDOzfT0Jt7Vra97599FlpKW2rf3/9GFpaP9NUSl9hIQmP6mr/aYm+8E6rs1rp+ENDaeJutdL+wsMpfU8IEqsZGb5dBVtTO9acw56soevbl9JE3cdhs5H4MhqBc85pPI7WWsi3Zx+rk8HpPn6GYZjTkpPQuDAwMBAvvPAC/vSnPyEyMrLJZfft24f58+djxowZrdpHZmYmli9fDqfTic2bN+OWW27B4cOH8d133zVa1ul0QlEUqO1w3KGhoQgNDT3u7XQUF110Ed5//32P92JjY0/SaE6N/bcUnn6cBrTUrS4nRxdkcXHAuecCF1wAjBtHqYWZmTRpX7AAWLWKhEpVFfD118DRo3oKlqLovzPr60lgORz0e1XayXfqRKKrvt5zHGYzRd2OHaP6MGniEB5O28/J0aNQ7i57/mgqpS8khPaxfz/twz0t0Z8zIkDvz5kDPPkk8Mwz9PWjj/S0x9hYYOhQ2l5ZGZ2nigqatN92W2Ph1JoIkqQphz2ZDhcURHVhCQkUoSwtpa9RUdRPrV+/xlG03NyWm0NIWhJlO5U53cfPMAxzWuHrj+icOf7/6LYTY8eORXx8PJ5//vlml7377rsxa9Ys1NXVtWofRqMR8fHx6Ny5My6++GLcc889+OGHH1BbW+uKrCxevBi9evWC2WzGwYMHYbPZ8PDDD6Nz584ICQnB2WefjVWrVnlsd968eUhJSUFwcDAuvfRSlJaWenw+e/Zs9O/f3+O99957D5mZmTCbzUhISMBdd90FgFLmAODSSy+Foiiu1wDw7bffYtCgQQgMDERaWhqeeuopOGTdB4A9e/bg/PPPR2BgIHr16oVly5a16LyYzWbEx8d7/DMYDJg+fXqjlMP77rsPo0aNcr0eNWoU7rnnHjz88MOIiopCfHw8Zs+e7bFOeXk5Zs6ciU6dOiEwMBC9e/fG4sWLsWrVKsyYMQNWq9UVSZPrpqam4pVXXnFtIy8vD1OnTkVoaCjCw8Nx1VVX4ejRo43O8YcffojU1FRYLBZcc801qOzgIm6OaJ0GtLQOBfAUZNICXhIURMYAUVEU3UhIoHTATZvIeMFgoEn64MFUzxUXR5/bbDTBN5tpP3FxJGzcTTXkOMxmvY4qMxP47Tcaj3vN1q5dJERaUj/jntJXX0+iRz7EKCnRxURVFe0zKYle+6vZ8meoceAARcfq6oBu3SgaKA1FFIVE5ciRlLLpTluaEDf3INDbNOPcc/WeaQEBdA0HDaJxezeGjoiglM8uXXyfT3/mEM1F2U7Cw8tWcTx9uBiGYZgWchIbFxoMBjz33HO47rrrcM899yBJ/sH3wX333YePPvoIr7/+Oh566KE27zMoKAiaprnESk1NDZ5//nn85z//QXR0NOLi4jBjxgzk5ubis88+Q2JiIhYuXIiLLroI27dvR/fu3bFu3TrcfPPNeO6553DZZZdh6dKlmDVrVpP7feutt/DAAw9gzpw5mDBhAqxWK3777TcAlDIXFxeH999/HxdddBEMBgMA4Pvvv8cNN9yAV199Feeddx727dvnSq2bNWsWNE3DZZddhpiYGKxduxYVFRUnrL7pgw8+wAMPPIB169ZhzZo1mD59Os455xyMGzcOmqZhwoQJqKysxEcffYRu3bohKysLBoMBI0aMwCuvvIInn3wSu3fvBgCfkT8hBC655BKEhITgp59+gsPhwJ133omrr77aQ/Tu27cPixYtwuLFi3Hs2DFcddVVmDNnDv7+97932LGz0DoNaKlbXY8ejQWZEHpa3O+/0+uhQ6l2C6DJe1kZpfgFBNC28/IoOpCYSNs2m0kYDRxIk/TCQhJfiYm6qYYcR9euJIZSUnSb8iNHaBlZ61RSQml+BQW6y54/ZDTPO6XPbKbjDAjQe4TJ2jPAd1piU6IoM5P2deQI1bIZDCRIAwMpOud00nHv3u05obdaaT+taULckibQ3ulwMoVTpnv26QO8/rr+ty44mM7nmjX0NTiYlvEeU1Pi1p+LYkdb6nvTVlHXWhdIhmEYphW05cliO3PppZeif//+mDVrFt59912/ywUHB2PWrFl4/PHHcdttt8EiJyutYNeuXXjrrbcwdOhQhDX80bTb7XjzzTfRr18/ADRx//TTT5Gfn4/ExEQAVG+1dOlSvP/++3juuefwr3/9C+PHj8ejjz4KAOjRowdWr16NpUuX+t33s88+iwcffBD33nuv670hQ4YA0FPmIiIiEB8f7/r873//Ox599FFMmzYNAJCWloZnnnkGDz/8MGbNmoXly5cjOzsbubm5LpH63HPPYcKECc2ei8WLF3sInAkTJuDLL79sdj1J3759XeKye/fueP3117FixQqMGzcOy5cvx/r165GdnY0ePXq4xi6xWCxQFMXjWL1Zvnw5tm3bhgMHDiA5ORkA8OGHHyIzMxMbNmxwnTtN0zBv3jzX9bzxxhuxYsUKFlp/dFpah5Ka6inISkpoclxSQsseOULL2u20XXdHwKgoSvPr0YPExLp1FLlSFBI1UhCpKkWshKB6KafTcxznnAN8/DGNT1Ea1zsZjRQdy86myFFz9TPu0bzYWEql27WLzkVdHdWcBQdThMc9XddX5Ka5nkvSiEJa1ldV0fcpKfR3o7iYasViYugc1dXRseTm0nH7ijb6ss9vyYPApuzup0yhdE/5t66khExAcnPpWtbUkENlTg4t37Wrp/lJc+LWnRP98PJEizqGYRimhZwijQtfeOEFjB49Gg8++GCTy91yyy14+eWX8cILL+C5555r0ba3b9+O0NBQOJ1O1NfXY9SoUXjnnXdcnwcEBKBv376u15s2bYIQwiUQJPX19YiOjgYAZGdn49JLL/X4fPjw4X6FVlFREQoKCjBmzJgWjVmyceNGbNiwwUM0OJ1O1NXVoaamBtnZ2UhJSfGIBA4fPrxF277gggvw1ltvuV6H+Ktl8YP7OQOAhIQEFBUVAQC2bNmCpKSkRuewNWRnZyM5OdklsgCgV69eiIiIQHZ2tktopaamukSW9zg6ChZapwkt6TMF6IJs7VoSVnY7Led0kmgwGID16ymqtXu3Hg0Sgupa4uIouvP772TfPmkSGTLs3k0T98BAmugLQZEw+Z4cR1AQ1YDJqJq7OCopIWHicJApxowZzU+evaN5sbEkdA4dItdBu52ERNeunuv5itw0V+vmcND5OO882oesR5PNnqurScCcdRaNSdaIbdtGQue88zzFnvs43O3zW/og0F86nPvfupISqrcrKKD1AgPpOOrq9L93PXvSv9ra1plDnOiHlycxI4VhGIZpjlOkceH555+P8ePH4/HHH8f06dP9Lmc0GvHss89i+vTprvqm5khPT8c333wDg8GAxMREmN3rI0CphO4uh5qmwWAwYOPGja4UPomMAAlvS9xmCAoKatXy7mN56qmncNlllzX6LDAw0Oc4FG/B7IeQkBCcddZZjd5XVbXRdu3yab4bJpOp0X61Bmvmth6vO0IIn8fi/X5T4+goWGidRrSkDiUjA7jrLuD++ymSFBJCE+aEBEqzi4wkY4ctWyhiI0VEfb1ec6WqtJ/SUopwjR3beJ+A/35X3mmOUhyVl9Nkun9/4LnnaH/N4S+aJ134HA7PlEFAt7nv3p2+lz23mqt1s1rpa2SkZ20bQMIyL4/ESkqKvn5SEkXmcnLo2KKj6fx611MBbXsQ6CsdTv6tCw6m61hcTOdS9lcD6LWi0HizsujcT5oEzJxJ1zY3V792SUkU6fK+lify4eUpkJHCMAzDNMUp1Lhwzpw56N+/f7NRkCuvvBIvvvginnrqqRZtNyAgwKeg8MeAAQPgdDpRVFSE8847z+cyvXr1wtq1az3e837tTlhYGFJTU7FixQpccMEFPpcxmUyN7OsHDhyI3bt3+x1/r169kJeXh4KCAlea45o1a/yOoyXExsZix44dHu9t2bKlkaBpir59+yI/Px85OTk+r2dAQECjY/VGHtuhQ4dcUa2srCxYrVZknOQntCy0TjNaUocSEkKugMnJNNk3m+l34m+/UZQrLIwmtEKQoBCCxIF7I2P3B1Nyn7J2ZudO/7UzTaU5FhSQKJkxo2UiS+IvmjduHB2PrNkKCaH9btxINWSaBsyeraeepac3XetWWkoplL7GZrVSjVZoqG48AugphzJNU4os+S8+Hpg+3dM+332f0uTCYCBR1JIHgfJv3ZEj9E8Ius7V1RS59BZFstm07G3mbqBRV0c1aEYjCbeoKDqeSy/VI2Mn4uHlKZKRwjAMw/ijpQXjJ6BxYZ8+fXD99dfjtddea3bZOXPmYPz48R0yjh49euD666/HTTfdhJdeegkDBgxASUkJVq5ciT59+mDixIm45557MGLECMydOxeXXHIJfvjhhybrswByyLv99tsRFxfnMor47bffcPfddwOAS4idc845MJvNiIyMxJNPPomLL74YycnJuPLKK6GqKrZt24bt27fj2WefxdixY5Genu4aa0VFBf76178e1/GPHj0aL774IubPn4/hw4fjo48+wo4dOzBgwIAWb2PkyJE4//zzcfnll+Pll1/GWWedhV27dkFRFFx00UVITU1FVVUVVqxYgX79+iE4OBjBwcEe2xg7diz69u2L66+/Hq+88orLDGPkyJEYPHjwcR3j8cLPhs9AZB+mxEQSXLLPVefOek8rm42WLS2l340GAwkR+XvT+8FUa9xc22q33VTD34wM4NFHgaefBp54gr6+9BIwa5a+n99/JzMIgHppDRlCkbRNm4BnnwW+/ZZSJqOjSQRarSQmrFYSjxERZJu+e3fjZsPShj4+XhejkthYGl9dHQkPmXKYlETbXLLE0z4fIHH466/UePnnn4EVK4C9e2m55nBv0ltfrzfqtdv16KQUX9Id0uGgiNAjj+g29KpKaaQbN9I52rOHrPJ/+olS+LzH7E17PrxsaQuD9hB1LWkszTAMw3ghn6TGxDT+I5qVdcIbFz7zzDMtSssbPXo0Ro8e7WFz3p68//77uOmmm/Dggw8iPT0dU6ZMwbp161yRlWHDhuE///kPXnvtNfTv3x8//PAD/va3vzW5zWnTpuGVV17Bm2++iczMTFx88cXYs2eP6/OXXnoJy5YtQ3JyskvUjB8/HosXL8ayZcswZMgQDBs2DC+//DK6NNgQq6qKhQsXor6+HkOHDsWtt9563CYQ48ePxxNPPIGHH34YQ4YMQWVlJW666aZWb2fBggUYMmQIrr32WvTq1QsPP/ywK4o1YsQI3H777bj66qsRGxuLuXPnNlpfNnCOjIzE+eefj7FjxyItLQ2ff/75cR1fe6CI1iaPniSef/55fPXVV9i1axeCgoIwYsQIvPDCC0hPT29yvZ9++gkPPPAAdu7cicTERDz88MO4/fbbW7zfiooKWCwWWK1WhPsKlZ+C5OaSEIqJoUiWtFSXphjuURSbTY9kJCXRBF7+Dh04kCbmu3f7rp05dIiW9SeeZATMatXT0iwW35Gw5kwQmnKi0zQSCM8+S2Jl4EBK/1MUOvbsbHJVDA+ntMWG+lSUldG+6uv1fdbX07YCAvQHc9XVdA727iUB510PJgSwfDmdjzFjaDuytgugcynbY2zZQsJs/XqqB7NYSAgVFtI6554L3Htv87VI2dl0vKtWUSTMaKRomvw7EhhI58fhoOubmkpCKjYWuO46upZff62fz/p6qq8LC6OvMTHA+efrY3Z/eClE29JAm8L7nvXGaiUx/fTTxxfRYrMNhmH+qNTV1eHAgQPo2rUrAt1TM1qLr1+kGRmeBeMMc5rT1M9La7TBaZM6+NNPP+HPf/4zhgwZAofDgb/+9a+48MILkZWV5df95MCBA5g4cSJuu+02fPTRR/jtt99w5513IjY2FpdffvkJPoKOwZcAcY/ue0/qw8NJXNntes2W2az32CouphTCtDT6nQn4r53JyKAo0ttvkziQjXfdx3T0KDkYSpc+XxNbXyYIVVUU8dmyBRg/XrdW97WN3buB996jyJDRSBGZmBgy9tizx9NVUdZNRUcDN91EIurLL6m+KSVFT7vcuJEMRYqKaDvnnEP1aocP6721JOXlFBVMTqZteKe+JSXRGG+6icTYzz/raYV2OwkIi4WibcXFLatFysgAHn+cxrdhAx2jHFdAAIloGeEKC/NsOH3oEP19rKqiz0wmWs5mo9eVlfTZrl3AtGl0bDINtLaWIkH5+XSug4KAuXOPX6i0d0aKr58Lfw8M2GyDYRimFXDjQoZpMaeN0PLOZ33//fcRFxeHjRs34nz56N2Lt99+GykpKa7O0RkZGfj999/xj3/844wQWk09nb/0Uvod6D2pLymhOp7oaJqQWywU8Sgtpcm51UpC66676Hdpbq7v2hkZJSsooLS7Q4eo0XHfvuTCt2sXiQAZHRo0iH4ve09s09MbCzm57eJiEmorV5L4GTy48TYmTaLUvH37aOIfF0fHV1BA2wgMJIEgBO2jro6OPS+P0gxVlc5PZqZ+fF27UsNf6bwoReTu3cC//kXCJjpaj0ZlZ9NXX32rAL2WqVMn4IorSHg6nRRRMxrp2nTuTGMMCyNR05JapMxM4JVXKKVy82a9X5oUTUKQIA4Joe3Z7SQKV62i6BegR6IMBjoPTietU1FB4+vUSa+PW7eOGls7HHRO+/She6c9hEpLWxi05O+4r5+LHj3o+ren2cap3sSZYRimw+DGhQzTIk4boeWNtcEiLioqyu8ya9aswYUXXujx3vjx4/Huu+/Cbrf7dEWpr69HfX2963WFnJGeYrhHgZKS9DTpX3+lyd+99/qe1EdG0gQxKoom4rW1ZL8ua3vq62lCLoOEvmpn3PtvhYfTuiEhFEn6+GMaT69eNGkGaP87d5KRRGys58T26qs9hZz3tk0mGmN9feNt7NwJvPYaTXIzMkiUOZ26+Ye0PJeGD8eOUS2SjLrl5pLAkQLJ3ZzCbKbJeVkZLS8n0NKEYudOeh0VRTViQUEkOnzhXct01lkkrhwOXUhs20av5X62bm3Z37DMTIoo/fvfVIN28CAdb0AAjS00lESePC/SWVKKqro6GrvTSfs2GmldedxhYTSO7t2Bxx6ja5GRodf9Ae3nCtjSFgZN4c8ifvVqPfWzPcw2OAWRYRiGYZjmOC2FlhACDzzwAM4991z07t3b73KFhYXo1KmTx3udOnWCw+FASUkJEhISGq3z/PPPt9gG9GThboUdG0uT9JISfaK+dy9NLq+7jlz+EhLoM7OZJoW//EICBiAhZbNR5AKg5XJydNMBbzdXIWhyKftv1dfTtqKiaGJstdK2NI0m6zJyJqNUMTG6MceGDTRhLy2laICvbdtsdEwBAbTt7Gx9G+HhJJwuuIC2ExNDIig2loQDQGOorKRtyj5TcXEkKA4epM+6daN9yTo2h4M+j4qic+bdbHjUKF3YlpTQ8XftSsKyubS3vDwSNiYTnaOsLD2tU9qzl5VROmOPHi2btGdkAC+/TFHIhx6idEtp0y8dBTt3JuEJ0HkLDtZTBKXwCg3V75H6etquTNXLz6f0zeRkuiZWq94aoD1dAY8nI6Upi/iUFPo5kePzFlutcVDkfl8MwzAMw7SE01Jo3XXXXdi2bRt+/fXXZpf1bmAmvT/8NWl77LHH8MADD7heV1RUeHSaPhWQVtjBwZ71V3Y7pesdO0YTzgMHaAIYEqIbOJSX08TbbicRICMcEu/oi3ftjBQX0uhB2sIDJJhiY+lraSmJEVn/Ex5O61mttO+sLEorLC6mCW55OdWFyW0rCk16KytpAl1YSJPtykqa1Kel6ZboViuNNz2dvi8upmOrr6cxyGhNeDiJCCnGIiJoe+vW0bbcBY/dTuJCUWj5339vPIGPjqbzKlPdpJthU2lv8nxu2kSCSopKGVGrr6djKy1tXPvWFKpKkbLnn6f0xrw8uq5bttAxHzpE21dVui8MBjpOm43GERJCwlI6J3buTPVZ7hG2LVvoe02j+yYmho4lNrZ9rd7bmpHSlEW8vKcLC+ke8e6T1lIHRe73xTDM6c5p4oHGMCeV9vo5Oe2E1t13341vvvkGP//8M5KSkppcNj4+HoWFhR7vFRUVwWg0Ilpaz3lhNpsbdQI/1aisJIFRUqJP1GtqKGJht3saGjgcFMEKCaFIjsWiR35sNhIHQugW4d6mA961M4GBtJ7ZTIImOJgm2zYb7SsykoSeFHF2Oy0bEEBjKiykiJvVSuv260eT14MHaT2bjcZYXU3bl5G40FASTJWVZFRht+uRt40byfQiJoZS3I4epdQ+adstU+CCgmhc1dV0/JmZNIY9e+g8uEdOpDW6wQAsW6ZHc/ylnZWWktnF+vVNp73J87lzJ9WVyRTO+noSrbLJcGEhnW9Z+9bSlLSMDBJnCxfSuIuK6BgsFoo0Ohx0n9TV6euEhen3TH09iawnnqDzA1AE54svaHzR0XQt7HY6h1YrpZ4GBJywPpV+acoi3mKhdM29ez2PHWid2Qb3+2IY5nRFlkvU1NQgKCjoJI+GYU5tampqAKBVzZd9cdoILSEE7r77bixcuBCrVq1CV2+PbR8MHz4c3377rcd7P/zwAwYPHnzcJ+5kEhZGouPoUT2yVFJCk9/gYL0ep76eJumrV5MpxtixNEkODaWJvBRHK1fSE/nQUN1t0P1pvHvtzO+/k6gDqE+XjGjISFl1tR7tcE/ls9lowp+XR8tqGgm/yEhK3Tt2jIQVQO8VFpLgCAyk7ckUtaAgEpA//0zLRkfTMkFB+sS/Z0/6Xho82O20DSHo3GgaHXd5Oe23tpYEidNJYkRapYeEUJRi1y76LCKCzrm0bpcTbXezi0cfbT7tLSMDuOoqvS6rtJT2GR5Ox1ZR4Vn71tqUtIwMEpzSyCQ+Xt8eoNevGY00th496BypKq07bZpuhGK1Ah99ROKkWze6LmFhdA5iY3X7/KgoMjw5AX0q/eKd5uqOotDYDh/W0zfbYrbRkn5f7RXZYxiGaU8MBgMiIiJQVFQEAAgODvab3cMwf1SEEKipqUFRUREiIiJgMBiOa3unjdD685//jE8++QRff/01wsLCXJEqi8XiejLz2GOP4fDhw5g/fz4A4Pbbb8frr7+OBx54ALfddhvWrFmDd999F59++ulJO472ICWFnpyvX09Co76exI8MxMl6G0Whr+eeq9en1NRQyp6MTEhDhPx8mpBPmuR7Mi9rZ3JzKTVtzx4ScXJiarHQNnNyaOIeEUGCR6bySRG4Zw/tz2gkwfPJJ/S906lHm/bto+8jImi7MsXO6aTJtKwR6taNGgzv2UPiJCyM3t+xQ28uLGvBZL2XolDKXHU1CZzgYD0SU1FBQicuTu8pFhlJ0baiIkq5k4YR7mlz7mlnTaW9ubvURUdTD6qAAP3f9u00BlmfZjTSWENCaAxffUWGFC1JScvPJwGZlkZi0h1Vpf0XFpLgu+MOOgZ3G/Q5c0iklZaSkEpIoM8qKuh6hofTmM1mul6JiSeuT6U/t7/mLOJrauj+jomhY2yp2Yb7/qxWOmZfYg5o3ybODMMw7U18fDwAuMQWwzC+iYiIcP28HA+njdB66623AACjRo3yeP/999/H9OnTAQBHjhxBXl6e67OuXbvif//7H+6//3688cYbSExMxKuvvnraW7urKjBlCrB0KU2WpWuc0UiTSZNJj3SZzTSprqkBZswAvv+e1h88mCai0mVP9pfavh2YONH3hFlVaeJ+++1kBpCd7VmPJPdrMtGEPDKS0s9kqt/Ro7RcVBQtV1JCYiAoiLYTFaXXDzkcNLkPDKTXR47oE2e5TmYmib+oKN3IQtP0vlR9+pDokc2V5bjKymg81dX6xLhLF90CPiaG+mbJeibpXigEfeZw6NEz2fuqubSznTuBefNonJpG56aoiI5p2DCKrh05QmOsq6NxABRBlFG4I0coTW/s2ObvEZkG2LcvCU13cWSz0XkwGkmEp6Xp63kbPQQFkZAtK6Ptde9O4y4poX3I2rcrrzwxBhDNuf01ZxE/c2Zjs42kJPp8+/bGUUjv/ZnNdB8XFdF1O95+XwzDMCcSRVGQkJCAuLg42O32kz0chjklMZlMxx3Jkpw2QqslRWnz5s1r9N7IkSOxadOmDhjRyWX0aBJEy5frDYgBmijGxNDkMiGBBEdFBU2Yw8MputGzpz6RdDcFUNWW1Zf4s+EeNYrEjeyjJd+/+moSLHv20AQ9PJwmqppG+6+t1Xs2SXGVn08T4WPHaDJ71lkkCGw2Eg4hIRSxAigCFBNDwqemhtaVaW5yvEePkmCSaYNms55KaDLRWBMTaVvu6XsbN9Ky551H+y0tpfdlROjnn0msNBXNWbwYePppPe3QbCZhBdDX5cvpehUUkKiUPbCio+k4TSYSOQUFwDvvUJSuOVEj0+iCgkicSSFaUUHbNpv1XlgSX0YPmkbbCAqidYuKSIRWVHi2A+jXz/9Y2qvfVEvd/lpiES/v7+xsssf3JdwA3/srKqJ7bO1aOk9t7ffFMAxzsjAYDO02kWQYxj+njdBiPFFV4E9/osnhwYM0Qayqosl5ZaVuUgHoT9nDwlpeX9Lc5LgpG+6JEz3f1zRg9myKom3bRuOVqY7Sur2ykkRPUhIt98MPNIk3GEgYVFVRRCchgbZnsehRO4BEgRSNgYEkII4do0iSjGyoqi5iAgJIPAAk0Kqr9fRDs5kEVU4OLT9sGL131ll0XFVVegTRZKJ+Zf6Ez86dwDPPkEhKSqL92u00tqAgep2Xp6dOAnTMTictJ0WholAUrKqqZa523ml0555LYnf3bjrOkhLa5qefApddRuP3ZfTgbp4SFqaLtYgIGm9WVtMRnKYiUK2xcW+N25/7vWm16tsPCqLtuEer/Am3vDwaq6/9DRtGIgugz9vS74thGIZhmDMfFlqnMe4Oc+vWUW1SYaEeqQgIoAmofMoeFOTfLEAIEgO1tWTl/c03NClvqhmrez1SU8Js+3baTmgobaewkKICRiMJmaoq2q/NRmNcu5YiPSaT3mQZoO3v30+T/9DQxuejqIgiTCYTOe7ZbLTdnTspmhQXRwK0tJT2I63mw8L0JsVWK52LwkKaYFdUULRk61YSPQYD7Tslhc7r0aN6DzJvNI3SBeU1CQyk96WRRFERCU5FAS68kCb+R4/q6Z82G03kg4N1G/0ePVoWdfR2iwwO1mvXADoXffqQZXt+PkWBHA5PIS4bOEvTC6uVjkmOubkITlNCZts2ikiWlbWs4a8/tz85xsBAijjm5lLkU1Xp2i9Z4l/kNSXcNmwgcTlqlG93wV69aN0//Ynux+OJ1DEMwzAMc2bCQus0x/3p/datwK+/0sS+rKzxU3ZN820WIJ3j9u0jcfb00/R10CDadnPNWJurm3F3g4uNBQYMIJEh64iEIGHRpQtNxvfsofdHj6Z1ZBNhac8+YAB9716Hc+gQHbsQlOaXkkLr5uXRhDspic5FZCQtt2MHLRscTMfgcFDqYHk5nZf+/YExY4DvvtNrs2R/rfJyEkEyKubP+ECKA2lv746i0Dk5coTGFBREkbxVq3RxJfuGFRTQZL5nTxJ5BQUtc7WTaXQLFtC/khLaV2ysbuQho1KLFlGKp7xO3g2cpQjTNIrgREc3HcFpKgIVG0u1gkFBwLhxdEzN3WO+3P5kE+ySEroeNTVk1HL77fR5U2mGl19O961MY3V3klQUOr6dO+m4fSGjv7IWkGEYhmEYxhsWWmcAMrKUmgpMnuw/suQd5UhKIhGyejUJMznhLy+n1LWdO2kSHBvrvxlrS+pm0tN1gRcbq5tLyH5GqkqGFrJhrkyXq672rAeSroplZZSuJ3tW5edTf6SgIOD88ylaA9AkOjOTohM1NbqdeefOJEpljZbsIyYNPHr1IrG6fDkJJE2jr4riGY3atAm47jo6j7m5jc+5TMEMDNT7iXlfN4dDbxptt9N+pPW8nPQnJlKdVWysHr1pqatdRgaNcdMmcmiMjva0pnfv/QTQdfrpJ4r6uTdwttnoesbGArfeSmK3qQhOUxGo3btpPWkwYjA03/DX27q9uJiiuHKM8tzu2UNiy1/an9zHBx/Q/Sjr0LydJGVaank5nTNv2F2QYRiGYZjm4ESXMwwpuqTjnq8eTvfcQxPlkhLgt99o8t6jB0Vx6utpYhkXR5PYXbtoMuzdjBVoHLUID/ecNJeU0KQZIIFnMFAk49AhEjRyoi0b9paX0wRaVUlcuNcDdepEX0NDSYzJnlVPPw3ccgvQtStFR6TIkigKHRtAE3whaBuRkTROu50EkdFIwuHss+lrWRktP2gQRZeKi/WITn09iSGbjcYxdy7w5JNUi/Xkk2SNnp1Nk/CoKN123tvPRYo8i4W2tW4d7SM2lgRraCiJHLmedLXLyGidq53sbZaaSufQOxUuJIT2W10NTJ1K10H2y5Iiq6KCUv06daJ7ork0OX/9pqxWuq5RUXT89fX6sXmnALoja84OHaJrsGuX3qw7IIDSTxMTKSqYl0diMSnJd9pfcDBFNY8do+OLjqb3jhyha1BcTOcrKooEp/d1a+t1YBiGYRjmjwVHtP6AyHTD1auBF17Q+0YVFZGAMJloQhoeTpNiq5Um6N7NWP1FLYDGwiw9nSbqQUE0QZd28NK+vb6e6pPS0uj94GCaPMuJuMRXzyoplHzVbQH0fqdO9DUrSzdGkGYWMTEkqLp21S3vVZUm9CkptJ5MUZP7Skoikfb99zTx9hXNu+suOtdFRXQc7hbr8nVUFIk+KRzi4mh/hw/T8UdE0H62bKEardjY1rvaNdXI1/ucAnSdjEbaf1UVfZ+QQEInIKBlNWLSVKSgQO+3ZbHoIlUIPZLXVAqgTCF0j8b+/jttNzyctldRoZu/yB5h/tL+hKCx2+0U2ayq8t+AedQoivj6s4pnd0GGYRiGYZqChdYfFFWliW9QEEUCZFqc0ainuUk3QCl2vCfk/qIWEndhlpdHUaJx42iyW1xM6WwyRaumhvY7eDBNkvPyaGzu6Xb++hS1REjExQE33UTphjJNrqyMbOAzMmiS7b6PjAyKcMi6MmkfL1MYhSDjjaoqYMgQ3+lp33xDqZxZWbo4ra7We1IlJQHTp1N05ddfaXIvU+mCgvToi+yhdc451AtN1tu11LGvuUa+7ud05046vgsu0IWue/2Sw+Eptv1RXU3Cefdu3aI+JobuNYOBoklxcXRc2dm0Xe8UwFdf1VNP8/JomcsvB77+msYpm09LESivYVNpf1arHq3r2ZOujb8GzDNn0jruVvFmMwnyc85p7GLIMAzDMAzjDgutPzDeAsXdyjs2lqILMurgS+S0JlIiRVloKE20LRaa8Mp9BQRQmpbdThPr3FyaxAI0wW4qktBSITF6NP2TxiFffkljknVR7vuYNo0m9O7blPbxQlB6G0Bjdd+fIjREWPMwxGzFscVWrNtQgcEHShFWGoPS+hCEG2sQFQV0HRiMCedVITW4FP3SYmD6LQSBZTXQSgC7KRijwmrQOR3QgoIhqqrgPFKKS5Jj0O1oCPJ+rsFvq4F9BcFQaqsQo5UivFsMho8LQ2q/Bt97N+XlqzbPX3RGXtOaGs8ea76uqT+ys4HXX6fzYrHQNTUYSKgUFVEESgo1aXwSGamPSaYAZmdT37CYGE8HzNhYSgeNj29ccwZ4pv2lpXl+VldH++7enQRTWJhntNJXA2Zvs5kjR4CPPyaDkaacEhmGYRiG+WPDQusPjC+B0rMnPfUvLqYJclISLetuEy9FTmsiJbIvkRRl3vuSPbVkPdCQIbr9d0FB032KWiMkAN04pEePphvbqiptM3unht5huUipy0FdjYaCY8E4R9SgVgVSS4IReLQKwTWlMNhr0fnIJkSX5SC0NBcBVWUwwwZVEQAEABUOGKAUAgG7HTAspPEMEwKDHSo0lZpHKqIhTJINKM6G/DcBGB8UcEJFrNOAqQIwwgFFoS2LtQqcnwWgPi4K5u6ppCQGDqSQWEwMMkJC8Fj/BoG2IxhKXQ0sJmBgWjBG9q9BykEVMPdASkoqevZUW3RNJZpGwjgnh75fuZKu6bBh9HXLFhIy0nBE0+j6OBx6Q+SKCroXOnfWUwCDg8me/ayz6D15TQ8epOUVhQTV0aO6C6TZTNfSX9pfXh5tNyWF1veOVvpqwCyt4pcubb5ZMsMwDMMwjISF1h8YXwIlMpKc+jZu1NMJy8p8i5zWCBxfoiw2lswnpLV8eDhNmOW+WtPQVpp8NCWcGq2TriH9qlwU/ZaD2ioNgVHB6BReA/UAgKPByKiqwuzA7SjbuRZhR/YgqK4cgc5KmIQDilGFww4YNjeIHSFggBNCKBCKCiE0KBAQUKAIAZXkEAwAIBRAA32qKFAAGIWAcAJoWFLR3B0YBAAFwk7fmwDQWmhwaqB1DA47lIJqiKICKL/8Arz3HoWSGi5WisGAZADCqUEoKqAA6hoNymcGUh+RkVD79MH0zHEIrU5GyY/BSIyqgTlIxaGgHthekYqYWNVDtGZnA//+N5lPlJWReKqtJat+k4kiWFVVuumJ00niavhwukZlZbQdafoRGkr3jaylqq2l6y4jptJJUqYcbt+uN3Y2GOh+TU6mlMxu3RrfD+ecQwI7P5/SF202PTUS8N2AuTXNkjmNkGEYhmEYiSKEt6cW405FRQUsFgusVivCfeXHnQH46oOVnk4iqFOn5kWOr/UzMhoLHHcreHdRdugQTXavuooiCcfT+NVVu2TVEFmZh8QQK9RKK832ARIUNTW00IoVNFMvK6MZvcOh71g6NjidEEJAqAYI1QBFaBRx0jRILaSg8Y+QjGEBZO0pv1fcPle9XgvX94prm+7reW9DAwkuEnSqaysKAJXUX8MKbt/L45NWkhJpAdlQdOQ0mOB0Ag5hgM0YjGpzJKzJfRBxxTgknp0MhIZif6kF/3zPgu92pkA1qoiMpFN74IBePxUZSbVYdjtFJouLaTdJSVRbdfQoiSvZz6y2lurDADIaAYDx40kIyahTdTVFyY4coc9tNhq+00nb6dYN6NvXs77LXaz/73/kVnn0KN13ZjOJptBQSjX0jk7l5pKbZEyM7xRZq5XSFJ9+ummDEIZhGIZhTn9aow04osV4ND1uSfSorev7izoNGuQ/6uQX73y1BgGlAkgtLqb8td27qRDMatXz1GThl92uiw1pMSjDLoCHp7eChhQ+p4OWNxgA4S5tSDRp0EUQCSOlIdakv+f+1R13wSVFluL3c4L2L5fVXJ8pXuP3+F7TdOElhC6wHA79facTBk2DqigwAgh01iO8ugyJ2Xug/H0RYDZDBAQgUgvHzfUJuCAgHVnxF8BaEwe7DdhnCMWRGgsq6i2wh6XA4VBRUEAiymCgIdhsZFYh+6aZTPSZ3a43sZa1VDYb1UaVlNDnMnKmaVRHFhtLIstgoHWkk+GiRcAjj+jiR6Y1zp9P911Skl47mJ9PqaqTJun3oRTtGzeSkPJn5e7txulNa4xLjmed41mPYRiGYZj2h4UWA0C3Su/o9Vss6qSQ2rWLZs0xMTSjbS4apWmeIkra+MlojbuYkjidTQ/aPSIk89/QWDB5CyPFh8hqcjd+3vOOl4kmlm3JfhoJL1/vi4a0RiEAzVOEoa4OwmZHuKMCvZGP3vUbMKXyI9gRAKdqhF0z4hgsOIhU5BZ0x67ygajSgnHMGIeNznQUBKTC6VRhMpHIqasj0WUwUJ1VURFdZpOJhvLrr7orocNB2lnWUnXqRDVernOgkCjq2tXThj47m8wrFiwgwRYZSbdUWhpFsgICSCxt3w5MnEgafeFCWi8/n0w7Cgspyuvdq60pgxBf0d7mDDTass7xrMcwDMMwTMfAQos54agqkJrS8OjdagV+9Urt27WLnAe2b6dZc309TfBVlWbS/qJR7qLBHff3PSwC/aTXeeOrY63chNvb/oSXd9pfU7m63tvwXrYpcdfueIsweY4azrdeHUbpiwGog1MzwggVIahCZxTgHMcvUCvegwMm1MGMUkQjS/TFcucE7K9JRw9RA6GoOBzYA1k1qajTVGzcSM6DZjO1AAD0BtfHjpGwkbVeR46QUJItBmRLAhndqqzUU1YPHqRbKTGRtlVYSKYaZ5+tbz87m6JeCxYA+/fTbVlRQcIlO5v2N2qU3gTbn0EI4Jkq66/PWkiI5wOH3bubXsef6UZz+2KzDoZhGIY58bDQYjoOzU1MWd3EVFOpfQ6HPouWaXoAvWe3N95Ha6JRQMu+byXuYse91gqgdEIVvsWVX5HkNmbv7clolncNmHfaYpPCsa24C0wh3CJowlVXZoADgKHhexoVCTI7gqChC3LRxZmL8c7FcNYb4YARtQiGtT4Se4P6YHfncdhYlAwcDsWwsyxYccSCQ0oKjh1TUVSkByeNRvoqA5wpKSQuZEsCp5MiOiEhwGefkQBJSSGxJR0uZYPiXbv0gGl+PvDttySySktp+xYLRcUOHSKht3IlCaPgYP8tB5oz0Fi7FnjgAYqO1dfTWHv0oOVba7rBZh0MwzAMc2rCQos5PlorphwO/6l9Tmdj8SMFl+IlS9oajWoNLRUrcjlZ7yT0yinFrYoKEC4DC1WBpyGFt1mF3HTDZ1qDz6Dn9kjiSJHlkTrY1Lj9HVcrxZlnNE8flQHuqZUCThga3hOuGjQDnFDhhAF2BKAeEc4ydKvag7HZi2CDGfayANiKw3GDMwF7DOlYZr8ABY44GE0qDgb2QJ6SCqGoqK0loVJSQsKnooJqraxWqv0DSEglJ9Ot5N6QW1FIjJSU0PKKQrfgwYMkssrK9B5vZjMJtcJCen/VKmDoUP+ulnl5+n69b93iYtqH1Qqcfz4JrJoaYPVqYO9ecmT0dbsnJXmmQ7ZkX02t195wfRjDMAzDeMJC6wyiwyc63gYUrRVTmuY7AuUv5c/dtMGbdopG+UWKO/dj8P7cff8NJ1pRFAhFgVMzQIETKhQ4FQMgQLbvqgEmE6Bojsbry+hdg/MfACgOB2SZlFxOqAYSLA5HQ9BPQNGcULxTKd3Pny/XQW9aIc7c5/Oa23vuETeJCs1NFrqvS2YhChyQElKFE2bUwSjsCKutQCTy0QcbcBk+Qj0CUWkPRzUisFtJx/eYgB1aBursFlSVWlCgpiAohGq/YmNJAFVXU9pfSAgduntDbkXRUw1raylipaoUASoro++rq12tyBASQkLFaCQx96c/ASNG+P4ZkyYbMqVRcvQo9QYrK6PTuXkzCb2ePenndds2XRB5iyZ/phv+9tXceu0J14cxDMMwTGNYaJ0htOtEx5ejn6ybyskh5wLptd1eYsoXHSGm3KJOfqNi7lEm6cjgcOjrG426KHK4CaaGmbs9PBoHRRdsremBTWIA6gzBsIXHIH1QCC4ZV4OUZNA5raqi0Im70QegW9A3fK9WVaF4eyl+zorBvsIQKHU1CDAB8WnBGDmoCinBpaQUNm2i65ObSzN5m83TXdBbyPkzDvF1Db3Ov3B1CYOHHb3rVMBdeOlpkL7e904/VKC5PlPdpJkZtVDhQLS9FKnIwVgsQR2CUO6IxBEk4JCWjkPdxyC8Rz+MmmrBWekpyM1TPRpluzfJDg+nQ62tpQiVbFhcXk6HGxlJp6Oqit7v3Jleh4bSP4vF/4OMsDDPBt0APY9YuVIXcYpC4ic3l8aUkUHryWcWERGe2/RnuuFrXy1Zr73g+jCGYRiG8Q0LrTOANk10/Nij+3T0s9n0tD6DgWaIsqbKe5u+6KjUPn+oKoRM4WvYtwKKNrnGI4/DXVDIEIeqegqrgAAgOpqafF10ERW7uIsiN8GUWxqGj761oKDagsAeKbA7VVitdG2cMcC48wG0YdLZ+WLg6uYilt5pnBUVzQs5r6iksFqh2RwQDgdUh52Ej3tqpNMJTegVYtIQw7OOzL0PmKcIc68jc095dBdegJ4iKUWYoyH90AwbNNmgGQ6EoBqhqEZn5GNo7e/Ajk+hFMVC2d4d+Lw7UgYOxgS1C37N7gEMSUVsrIqzz9bNLI8epVugUyc6LbW1tH+ZThgRoZ+ukhJyOIyMBKKi6Br4iyJ7N+guLgZ+/pluFWPDb12TiW4zeSkOH6Zx7NtHD0uE0J9nSFfEQYMam274agbuuhZNmHW0B1wfxjAMwzD+YaF1mtPcRCd7p4Yf389D+rVujXv92aMDjR39pDGFxD0NzWDQhcqJFlNNNOTVBCCEAg2y35UGTVGhmEwwqLSMKzJlsVAeWM+eZCcnvbvdBVRcHM0UU1ObnC1qGvDJHGArgF5D9GsRHU1248c76WzWQr+tHv033gjk5WH/Zit+/Z8VR/dVwWYDorRiDK9die5iN0LKKcwibDZotfVQBEWghKJChQYhNOjiS/gUT2gU+fJOMvTEXYTp6YdS2MmaNa3hc0Xvd3bkCHnE//or1HnzcEVwOEYhBvu39cGhnuOQGtcFnVJ6YK0pFUJQo+X+/YGffiIRFRpKl762lgSUjGyVlZFYCQ2ln63qamDOHP9R5EsvpQcdO3dSE+eqKr2OzGik7ZhMJLRsNvoZ7t6dGjtnZdGPZ0WFHmmLjwemT29876iqvq+sLM9m4P7MOtqLU6U+jGEYhmFORVhoneZ4T3QUocFizUNAnRWdjm7FxF0rELVqNxyfFCKg2kozOveUP5lCJut63GnO0c9fD6bjwWjUx+OvNqqJaJQTKhw2DU7FCEdAEOqCInE4qg/WhY2DLSEZU64KRmpcQwhB5n9ZLO1S0HbaTjpVFdm1qXj1B6CkHEjuTxP1PdXAyoM3ItGRh6nTrMhItEJUV+G7f+dhYNkKJB3bjsDaMpjstVA1BzUNBgl2oSiUAugmwLwbMSuqCmh0DfW0Qd/SS/WKdqFhWfmOAgFFgE60dK9suLdNVeXoBCs6VezB0IKvUW2KQFVgDC6J64HNpmGoi+qPQ6XpcNpTYTKpMJuBLl0o4FtfT2LHZKJbMyyMem/16QO8/nrzUeR77gHee49cBuXtGhxMX00mGq7ZTPupqSFhN3w43SdHj9JnZjPVlIWGUn1Xt26NI9T+moH7M+toL06F+jCGYRiGOVVhoXW60pCzZNtgRWqeFd3sVYjIyUPngk2ILstBZOkehNSUQNGclGal6qlfHuLFl5hqytGvibopXxNkxcd7zdZJGQy64JLjMxr1OiM/0SgtNg5ffAFkHwhG96QaQFVRGt0D1ohUaFCRlQVUHwEeualjnu6frpNOf1HR+nqg9JiK9ftSsXI/RX0iI4GcSODguJmIqsxFdEkOAA12UzBqS2pQuiUPacc2YWBIDjrV5yKotgxGex25DioCirx+DgfUhntTRr480wobpx+6izDPaJdwuTM2uj8bhJeqKBAKYBQOhDvLEV5djsR9u5GJxbDvDUJlUBxGoS/W103CttgxUKJTYTaryM0lgSNrqs49F5g2Dfj665aly2VkADfcAPz+O0WXQkPp1j18mISVtJq32UioxcbSOU5JIcFls9EyFgvto6moqHczcHkfVleTaOwIF8CTXR/GMAzDMKcyLLROF9zrb7ZupdS/3buRml+IvxRZYRI2GDUboMnJJ9lpoyGlCppo3Li3DWLKH3rVjnytNvzvVuMj9yW/Slt3ac4gjSdMJpolRkTQzPHss4G+fT3rjHxEo/Jyge8EEDMY2Oc16VPQ8RGlEzHpdK8Jcp9IH4/LpK9IXHExsG4dne6oKAoQBQSQ/fj+/UCnTirUrmkoj0zTN5QKHIgGFm3VMKRTHsKEFRZY0T2+AhdEb0en/WuBPXs8zFQU0bgGjNI93Q00PEWYe8qhoihQmkpZlZ+JBpHndJLjo6JAUwAhBIyOWkRV5uICHMSIyu9RdjgB+0L74rewCdhtyUBKHwtyyyxIHJaCvz+nIj+/dZFLi4Xes9moD1d4OBlrlJTQ+bXb6V/fviTiPvqIrqWve6i5e1hmj2ZnU/+wjnYB9K4PA1pWV8YwDMMwfwRYaJ0OSEvBdeuALVvIlqwh0mMSAoqmNtSwyHoVwsPZ7XjElPe67rbnoMmq5vKfg/teycRAoTopDzFlNOrOAn36AOPG0cxVOg+oKjUYaqYuyp2TEVHyFj7p6XSJOsKUwN1ZsqiIUssAMlCIi2v7RNr7vAlB+6ipoQiLEFSqFhAADB5Mx7txI6XXuV8a2UD48itVXHttqpcAvBjQNGj7c1H0Ww5qqzSE1hYjevtKqO7tARpSW4UmoAkFGlSocDY4ERKqe2hLijOg6TpBoFGqq2IwQHXqDyQ0CBhhQ7ztIOLLDmJ42RLUK0GoORKJqrAEREX3hHHldajsNBZ1dWqL77OUFLomRUUkQKTjYVISLXPkCC3zxhu0fEvuYauVolS+jFFOpAuge33Y2rVUh9aSujKGYRiG+SPAQutUR86a9u+nf9ImrWHSqAgBY4PA0uNXotH/fp/6twT3yaxM+wPI1U+ltrNCCBigQYMKzUBiSlONqFWC4AyLRNSoPlDbQUw1xYlOY/JlqR8VRYG69jYlcJ88BwfTZL26Wte7nTq1fSLtfd6kS6LFQtuuryddLNPcBg0C1qyhdLj09MbHeNllVMfU6Bh2q1i4MA27dqXpUZYeN+KqG/NwVqzV1fC6YH0eDs9fgYTi7Qh3liHAUQvVaYOiaWRsoqoU8fK2ofdVz9fEAwWl4WdIgFIV1QY/Q60hhqbCgUBRjeC6asTaDkP9bjOwahG69RqE8dpVqBEZMEZbUB9ogdWSAqHQhfW+z9zFCEDnUYqRujogIQG49VbdxbC5e7i+nqJeR482jlalp594F8CMDGDSJODpp1tXV8YwDMMwZzostE5lZPFMcTE96ZddTmX1vNMJNEqd8jQNkLUszdIwKRWAhyU6FIUmpIpC0SghaL8NdVKawYQqRxDqgiNRHN8HB7qNg9WSDLspGAH2GlRVq9ij9MDdc1ORmtaxj7VPpM21v6jBoUMktDp3pihQe5gSuNdQZWQAv/1GjngJCfR5cTEd2znn0LhaO5H2Pm/19ZQqKC93RQXtS9YJJSfThLp795Yfo98oy1YVhw6nkjg8n4513hZgy8CZGJ6Yi5hSvQYsungXOm1ZijR7DmJFMRSrtbHQ8iesfEW7hHtKov6Zu6shoFF6otxeRQWC1qzENOVn1G8IQn1IJKrDE1Ac3RPb+1yHA2ljkZ+vNrrP3M0qsrPpR7m6mo43IoKs39evp+cOUVF0H/m6h7OyKPPSZKLte0erLr/8xBuyaBo1Wm5LXRnDMAzDnMmw0DqVkcUzFgt5RMtaJvc6J+Hp5OZOI5HVVIPeBpHlhBGaUGCAk/ocCUBRATUwkB6LR0TQrLyhn9TBrBp8+qmKoH49UBmd6nqqL3E4gEM5QGV1u50Vv5wom+uW9A6KjaWJ9fHWTwGeNVQVFZ7RJrnfkhL6rC0Tae/zFhZG71VWkugKDqZLLvcnTRsuv1wvmWsqMNmaXkvyWJNSVFjD02CN0kNjeamjUNF9JpSDufjLJTmIr83V2xQUFenKBdBNX9ybT7vj/rOjKFC9o11uqYouJ8OGBw4KAFU4YBbVCKyshqXyMDod2Yz07EXYHTYI8alXYdCwDGCrBYjUawjdzSq2bgW+/JIiUu6CaetWEuq+oqKHDpHIslhIyNfW0pAsFv08fvstvX8i02flNWtrXRnDMAzDnKmw0DqVkcUzQUH0mBjQZ7Lej6sBz4mhnGTKSaJXrymPHlgANEWBHQFwKEbAYESV2YLykHjkBvREbtoojL0mDildGqf6qanA7rVATAAQ7kPtnWjXsRNhc90SG/fdu+kU9elz/Ptzr6EqKdGjTZKAAF0URUe3bSLtHXEBKOoiU75iY+k9GVUBgP/7P9pnc0YLrbG9b67OLjhURY4pDcUD0xDfB8DMmXrj7dxcYNMm+n7PnoYu0Q33uBRgbmmvLlQVilPazOu4HA59pN3S3U8tlzUA0AQCtQr0K1uJ3mU/w7E1CNaQSJhTExA8oCdw3XXA2LFQVRUpKcAnn9C5y8z0LTyTkujBwO7d+j2clqZHwlatovvAaKTlevakdQ4epG21JX3WX/Pl5jhd3TYZhmEYpqNhoXUqI4tnpOWbtD232fybALjXULm7uUnBpaqU1xMQ4LJH19J74puqUdhxNA4pKYDDHIp6M9WelIenYGe2iqNlwCO3NZ54nch0vZbibXN9vBElb070xNK9hspsplvAbqfvAbodZA3V8QhbfxGXgAC6BaurSQTk59OkPja2ZUYLrTlfra6zU1VSILIozI87pygshFZaBtTWQdEcFO2VN6ubyNIa/A5lg2QNFO1y3dZuoksFIFQFqqwdgwIVgBEOqM5qmCuqIbYdhpa9GeqiRVTcds01KEgfg93ZqUhOVv0Kz9JS4O679chiWBid4wUL6HpERJDYttvJTMNqJaMSgwFITPSfenjoEJ0qaaYhfy581Ru21FyFLd4ZhmEYxjcstE5lpIrZtInsu0pL9R5Sdjst4y2yZP8pd2EVHk6Pvd3d/dzs0fO0FHw1W0XMUMDRytSfE5Wu11qkzXVHcKInlu5iNiODzumRI3qUSdZQhYfTdToeYSvPW2oqBS/dI4NS2CUlAcOGtdxooTXn67iFu/uF79cPuOEG7F2Zhx+/tqJwtxWxpVkYVLQUadXbYakvgrG+FgAaPDtVN5dDXXgpaMLV0O21u2ZSG6JdAoDTIaBUVED58Udg9WrERifgDtEXhf0nIfesMbBG6Cm3QtCP9tGjFJi78EK9p/gbb5DgjY3Vr4U0nSgupgzKtDRg8mQSZN4/j7K+y24H/v53XUz17UuGFW11KTwVH7YwDMMwzKkAC61TGXcVU1lJj7GLi/XPpajyjlRFRJD3do8e9BRdfu+niKZy+/FFaE5Eut6pxImeWLrfBtnZNHk+dgwoKKDPw8KoZic7u32FrXdk0GoF/v1vmti3xmihNeervYV79m4Vry5IJRHRG7CFnI89VTNRtysX6couXJ2+FbH71+LYLzsQbC1EgKiHJkhqKdCo0TIa+sQJuJlneEa73FspuI6twckQQoNQVTKVqa+H6chBDNHy4Cj8HpVhCSjq1Af5nYchJ6g/1pSlY/OxVFTXqnjnHXrGcumllD1cWKhfe7PZs1QzLEw3RBk9mu4H95/H+noSWRERlA4qz+mmTdR8OSKideLZnVP1YQvDMAzDnGxYaJ3quKuYwECa7ZWU6MLKuw+Vj0a+zdEeEZqOTtc7lTgZE0tvMRsXpwdT4uLoa2uFbUtqctwDRNu304S9tYK8teervYS7PxOOMIuK0KFp+CUrDdVxE3H1nzW88ZdcDK1cjj75/0N0aQ5CqothrrNCFQ6gwYVQyixyJfSMdpFLoZ5iKNz+J3RTDgUKnIoKVbMh0pqLKGsu0nMWYwSCcLEhDjnmvtjaeRLqIsZgy6ZUHDqk4qKL6Nz37Qts2KD34goIoNTRigp67nLOOXQe3X8ejx0D3n6bjEt69qRrrSh64+QNG2hdb1rjUvhHe9jCMAzDMC2BhdbpgPusyar3G2qvPlTtFaHpyHS9U42TMbH0FrNS8LTF2bAtNTktEeRmM92e27d7jqm156s9hHtLTThy9qo4ZEpD8PCZ2KXeCkt5LqJLchBuzUW3/SsQue93RNgKYVbs0IQ0f9ejXe5QZMtTeAnoPbsAinwZFIp0oWFZAYEA1CLJmYukmoMYue97VBYloCi+L34OmYRs5xgEBqTCZlNx1ll0bFVVdG6MRrKEj4mhbEmJqpID4SefAMuW0XJFRbp5RmysbsVeUUHXLSLC83haU294oh+2tNW8g2EYhmFOFCy0Thc6UMVw6k/bOBlRvPa4Dfz2tGqmJqc5QS7dCP/9b99uhK09X8d7rC014QDcBaSK8sg0lEeSucaqbjOxvS4Xw6uX4/KQ/yGhsmXRLtEQ7aLvFc+0QiGgCvmZ/FyDwZWAKGDQbIi0HkSk9SC6qUtRlhWPncFD8LlyHUoiBqA6KAWhYeRg2KkTHcegQZ4PQ+R13rePRFZcHPl+SPOMs8/WmwvX19M/b3xFs5sSON7XTNPIdKO9fz6Ox7yDYRiGYU4ULLQYAJz601ZOtyhea3pa+Uoj9CfIW+pG2NHny10EWK26E2NTKbE9evgXkLX1KnbZ0uDsNROG825FhNUz2pVQ8DuCKwphEnY4oUe74CPa5cK9ITjQ4HNIuOq63AScUbMhtu4QRtYdwtlYiv2VPbEndABWmidgf9kAlKekoGs31eNhiHeT66NHSWS5m2fs2kWphuHhdO0CAjyH6Sua3RqB01FiqK0PChiGYRjmRMNCi3HxR6qz+qPSmp5WvgSRL0HeVjfC9sZ7Ym82k8AoKvIcF+ApIlJT/QvIvDxq2JySAkD1jHZtGTQTSl4uErOXY7Lhf9Cyc2AsL4ZFWGGAA5pbAqFoEFOKPAFuzZLl9951XYpLbNH7KhSEoAqZts3oUbYdF+IzHCrqinxrf/QaNQbJtn5ALtVm5uWpruscFubpVCnrs0pKSIyGhpKp6eHDdH38RbNbI3A6Sgwdz4MChmEYhjnRsNBiPDjdIjRM62iPHmDt5UbYnvib2BcVkWBYu5Ym4v5EhL+I7jnnUMTr8GG9B7hEg4qsqjSYps5E9IO34u3HclH4cw7Sg3KReWQFuhT/jrCqQhicNmhQoCgqVIMKxeHQN+LWD8+7rguud12tk2kVOGEEEAAbejq3I71wJ5QnPkX9P2OhdesOc58eQOYEhJUOQGhKChRFRc+edJ0KCsjBMCCAzDGys8mFcNIkYNs2/9Hs1ggcoOPE0PE+KGAYhmGYEwkLLYb5A9FePcDaw42wvWhKBAwbRiILoM+bSon1F9HdvZtEXFP1i3n5KlYeSENIzzRURQP7wmciwpqLrvuWo/ve/yH2yHaEVhfC7KwjuSQVhlPv2SXcXAylfbyUXPSdLr8McJJ4g6D3NQeMJUfgLCmGY8NaJAR8jntMPXBk7wAU9JsAp3kATIYUVFaqKCqic2YyAeefr0eXJk7U/Xbk8QcF6emYLRU4AC2blETbqq+n6KLFcvxi6EQ3C2cYhmGY44GFFsP8geiIHmAnuoGzN82JgF69SGT96U802W8qJdZXRNdfumTXrhTxOnyYelFt3EgTfZMJiIlR0bNnGsoHz8SWQbcipCQXIeuW49rw/yHi4HZqilVXRwNUFCgAVM3pEc3SoMLQ0DxZiipvFFBzZAG1YQ07nJoJal0V0uo2I6VyO+w5n6F/QFcMCByAHxOuw7qksSgsUmE2kythTg4JqpQUer1kSeO6qszM1gmcoiJKESwroybLRqPudhgZ2XYxdLLvNYZhGIZpDSy0GOYPREc4TJ7oBs7etDTKYbFQy7m24B7t2roV+PVXqnl66y1g/35axmikCb6qejr7xcaqOByQhtJeMzFh9q2IQC6wfDnwv/9ROLCoCKithQpAKGSAoUDA6LJ/9xRd0q1QcYt2wa2aywAHBFSocMIAwAQbeth2IM22C2OqvsbGoHPxcfS9yIoajd9+U7F9O9C/PxAdTfrP6WxcV7VzJ0WmWiJw9u6lcyIE3U8mE2C36+ekV6+2i6GTfa8xDMMwTGvgcmGG+YMhIzQDBgClpRTRKC2lCWpbTAqkeIuJIfFmtVIUw2ql1x3dHsA9yuGL9opyyL5US5eSZXlMjG6Jrqok9kpKqP4pNlYXKUeOUPphejqQkqoCaWnAzJnAV19Rg6vPPweeeQaYMgVK165QA80NfoMCitEIRZFOhHq0i+q54HrPXXQpbnVdRjghExEBgWCtAudWL8Vz+TfiiayrcZm6EIm2XAQYNSxfDqxfT2MPDwcMBr2uqr6eji8vT2+ULZECJyODhPvatXQOTCb6Knury3OyaROJpbaIoZN9rzEMwzBMa+CIFsP8AWlvh8mT2R7gREU5vGvBrFZKjYuOJkFht5MYKS4mkVFZSeYT+/eTYOnRgwSX61yoJLq01DTk9ZqIyokaIq25SMxaDvXLz0mR1NRAcTqhQECD2pAm2HBs8GyO7P6erPCS3xmgwAgHNBhggAPR2lGMq1qEc6u/R76pK4rsA2DQrsMq41js3q16GJsoCkW49u+na9pUJDQ/n4T7oEEUBSsupmMPCKDmyA4HRcyGDj097zWGYRiGaQ0stBjmD0p7O0yerPYAJ6rhtnctWH09CQeTiV7HxgLHjpGoOHSIhJcQtHxGBgmQWbMomDV6tG6ZrlvSqwgMTEPPnjNx6au3IuPwcuCTT4DNm6EePAilrg7C1rDRhm5dhoZEQl1godH3qqstslxLfuJEiKhCum0Hzsrdhf74GlvDzsXH2r2o6D0alkj9hIWEUFTqyitJQPkTONu3k9hMTyfb+F27SJhWVlJqZVISbadTp+O7FtyKgmEYhjkdYKHFMEy7cbLaA5yIKId3LZjZTOLBbqfvAwIo3S4gAIiKov07HBRtO3yYojtlZcCePcAVVwD9+pHxhLSkDw6mNMNVq4CdO1U8/viFyHxvLKmJzZuhLF0KbNoE29ZdMDjqIIRoiG8JV8ctDQrVerls4hWPKJh7iqHaECVToEGBQIiowPCKpehZ9TsOLT4XewZfh6MJA2C1pKC6WkVgII158mT/Asc9jTM2lkSuu/MgQOegJWmc7s2nw8JIpOXne+6XLdwZhmGYUxlFCO+Me8adiooKWCwWWK1WhPuqAmcY5pTBe3LenlGO3FzgySdJPISHU2BJmmLExpKYsFppWYsFqKig5RwO6lllsVDaXGUlCYSyMiAigizoS0r06I/dTkKlZ0/g5ZfJ8c/94A59uxmF//wUcUe3Iay2CCGiGgqcUBsiWxpUGOHwcCvURZhn9MsBAwwNUS5nQ1ohLW2APSAYJdHpONhlJP4bOh0BAzJx/fV0HP7Oq6YBc+b4TuPUNOD334Hu3YF776Vz4O/aeDefrq+n+rigIBJs0g3x0ks5VZBhGIY5sbRGG7DQagYWWgzDAL5FRHExsG4dCSm7nYRTeTlFuoKDKa2wokJv5qxpZDzSvz+JtKQkihKtX6+LMVnfVVYGnHsuMGMGpdq5C8fsnRpWvp+L4l9zgLxc9KjYhF62zUh17EGAqIMRdhga4lpKQ72Wu+gCZBWXHvESDdEt+oOgQgHgVIyoVkNRZuiEZd1vx/YuU1AVlYL0DBVTp1J0z1vUujePlmmchw6R/b3NRl4gcXH+hZJ38+naWuC33+h8REUBI0bQuT10iERvWwxcGIZhGKatsNBqR1hoMczpSUdEt5oTEQkJ9DohgQTFtm0kCmTaXF0dCYc+fUhcBQRQ1Ku8HB4GFJpG6YaKQmM/6yyK5riLE3l8VitQadUQVZUH2/rNqP1qKQJ3bcJZjl0IFLVQoDWkCRqgNjREBiiaZXR77d2rSy6vwQAFGjTVhKOd+mJPwih8ap6Obc5MxMf7jjC5R6SKishIIyCATDJSUihi50soeYtZQI8axsTQeU9IIAEKUE3ewIHAI49wfRbDMAxzYmiNNuAaLYZhzji8U8/aK9XMXy3YtdfKnlnARx8BBw6QCJNmGQClGlZUkFCIjiaBUlVFAi062jPNzmol8RUWRuOPj6ftbN5MAkWKE71GSQWQCkxMhePxqZh7Vx7q123G1JpP0aVoPSyVBVCEo6EmS0a69DRC77RCdydDpUFsGTUb4gu3wlJ6AKn4BgsCrsM6+41In5CK6lrVY2zp6cA119A5+uILOrYhQ3QxJG3js7KARYtoeVVtbDhSXk7iymKhz8PD6bXVStHDpCS61nl5ra/X8hCqDWK8qbRIhmEYhmktLLQYhjmj8E49c2+86y5S2kpzjncBAbT/gwdpMl9fT6KhooKiWz17kkiQosFo1MUYQOscOULGGklJ5GTocJAY8yVOvDEGqLj0/lS8+moq5hRPRe+wXIzY9yF6b/8EgdVlCLJXwAg79FbHcH3V3QrplXytAnBCgSLsCLSVIwXHcK/tWRzZMR9FNcORNWQ6wjNGY2e2infeoejT7t2UJpmdTeKytJSEqERRGgslb8MRd2dHeW4rK/X+ZbIZdWVl666hFOLr1lG0rbqatpWWRoKZa78YhmGY9oCFFsMwZwzeva5klMhfBKWtNOWuKKNeCxaQYCooACIjSWz07EliQwiyP+/UiYSUFGw2G43d6SQR4nCQEJOph77ESVNjWLhQxa5dadgRPwtpEVfgkvJ5SD24CiGHKK0QDZErJ1SPNEInDDDB6ZJbGqRNPKBCgwMmGGFHgj0P8TmHkX7oB+Qnj8C3SXfivS1j0a27ip49Kd1xzx6qr1q3To/6SbyFkrtroey/pWkU2QoOpmXcz0dbmlFLIb5/P10fh4OEb20tRSLr6poW5B1puMIwDMOcWbDQYhjmjME79cydloqU9iAjA3j8cXIUfOcdShHs0YPEldVKNuVpacCf/wy8/jpFf+rrKXITHU3bCA+nKFBCAqW0SVoaxWkcectEdeULeP5veaiu3ozL6j9Fr6r1SEABDHDAARXGhrRC1SOtEFDhdDPRgMupUIEGg9BgqilBj92Lcceen9DfdC6y0++FNWw0NE1FUBAJrooKujYxMfq18RZK7s2nY2Np+WPHyCgkMJCWSUuj89GWZtRSiBcXk3mJw0HGHLIWTr5fXOxbkHdUSirDMAxzZsJCi2GYMwbv1DNv2ppq1hZUFRg7FujcWZ+cFxQ07u/VrRvw/PNkGpGcTJ+vXAkUFpKg6NmTtldeTmJM9qRqSRTHPfJGRhMqDptSUdY3FXcfmIoIYy6uc36IqbWfINxRhnBUwAQbNKhAQxWXe0qh5vpea3Aw1FsiAwLBmhUX1C/F2T9uxP7DF+G3YQ9ie0wmjhyh8brXV/kSSrL59LZtwPff0+u4ODoXNTWUTllRQZGn2trWN6OWQtxioUibxaKLPkXRxW3Xro0FeUenpDIMwzBnHiy0GIY5Y/BOPfOmLalmx0tzNV2ZmcBf/6qLsYoKGqPdDgwdSsv8+mvjHlvV1a0bhxQZKSmUslhcrGJvWRr+ETIL/wu5ApdVzMPQmh/RGztggh1OKDAADQYalFYoXHVbmodNvNrQwYt6cTkRWlOEXju/RHLer0jtej3+T70Re62pcGgqampI1OTn+xZK6elk/hEURO87nZR6GRZGaYMVFcD27cCVV7Y+kiSFeFCQZ+2XRNaAGQz0VQryE5WSyjAMw5xZsNBiGOaMwT31zLthbltSzdqLpmq6gMZi7OhR4L//Bfbtozoiu53ElxB6GuHrr7cuiuIe7QsPB847j6JGJSXAVlMmdgS/gM7mPFwR+C2uLH0LEbYihKGyoYmxCnh4FBK6O6FunkHfaTDZaxBdthcTjj2LYaHzsNZ8AV4LeBCHD2ciOtozqudOXh7VdI0bR8crI3jh4SSySktJZF5zDaURtgYpxGXtm92u13sBVCNnNJK4cxfkp0pKKsMwDHN6wUKLYZgzBpl6dugQRRlkr6vqav8RlFMFdzHWpw/VZt1/P6XahYRQVCUxkYRkTIxnFAVo3qDBO9oXFweMH0/RsqoqIChIhU1Jxcb0u7Hit9G4uHQeLjCsQpo9B4FaTUMzY9UV0aIYlgpDQ7SL2h+7N0UW0BQDVOFAZOUhjKv6FMNDf4Q9ZTqc196IxBGpUI2NL4QUhKGhFFlyJyKC3s/JaRzRa4lJhRTimzZRLVxhod6/TNrvx8fTOZc9v9zHdCqkpDIMwzCnDyy0GIY5o/DX68pfBOVUJSSEUvySkymlzWz2rCmSUZSVK6n5cXMGDb6ifXFxFNnKzqboWXg4ufsFj8vEW2tewFJDHnpUb8akyk8xqPZXhDuPQQgnFEFxK4NbdAsGFQanw63lsZRbFOcyCRuiK3Oh/OdZiB/mo6rPcBRPnA517GikpKouUdSa9E8prrZuJcFYWEgRMH/nwF2IV1ZS9KqoiJaXKY11dXSup0xBm8bEMAzDMBIWWgzDnHE0Vxd1OiD7RaWmNo7sACTEsrPJ1VCI5g0a/EX7AgKAqCiKoF11FdCvHwmYWbNUBASkwhGQiu8DpuJo2Ur02/oBkvJWw2I9BLWhF5ciT6rQJZZMJhSCIlwqBKCoZBXvcEI7kIeA3AJYlq7Ejs4X4fuLH8T5t2ciI6Pl6Z/V1cCcOWQbv2MHCaS4OFonOtq/SYW7EJd9tIqL9Zotk4kifF9/TeesNWM60SmpDMMwzKmNIoTbX0emERUVFbBYLLBarQj39SiTYRimA8jNBZ58ktIEff3qKS8HfvqJBNKQIY0n/1lZNPl/5JHmLcozMjyjfeRQ2FhYKEJD+LFcxC//EBcVvI8IW1GDT7oTQlWhaHovLqgGwO21YjBAaBqEENBgoPouBbCrZhQHJmNF+h3o/Kcp6DE2BdW1Kl5/nerHfKV/TpoELFlCAmnfPnpfCKqxMpmA7t3p2IuLfZ8DeYx5eXSMH3xA209JofNZU0MiLSZGF2ruroO+xsSugwzDMH8MWqMNWGg1AwsthmFOBv7EDkCiYsMGMsoYNcqzz5bEaiXjiKefbmzQ0JJ6puaExUMTdiJq3ksQK1cirKIAinA0JAsqUJWG8WrUbwuKAqEoJLQalqF6L821PxtM2BPYF3ldR+HwuOnoPC4T27Y1FoRTplC0afNmSvH77js6H8HBdAxVVZQSmJJC5w3Qz4H3cSclAXPn+j/H3mK1JSKVYRiGObNpjTY4rVIHf/75Z7z44ovYuHEjjhw5goULF+KSSy7xu/yqVatwwQUXNHo/OzsbPWVzGoZhmFOQ5ow9QkOphis01Pf6TRk0NOeCCDRf62ZDJv4a9x8ow3MxqfxDDNn9MUIrj8DgqAOEE4qiwqXdpE+7G8JVvUURLjNsSK/bii77DqDy0DfY9Nt1uOLZGxFwTSoqq1WXIJQOgElJwOrVFFCLiNCFYnAwvWe16lGpykrfIqlTJzLW6NatZW6CZ0JKKsMwDHPiOK2EVnV1Nfr164cZM2bg8ssvb/F6u3fv9lCcsbGxHTE8hmEYAC2LGLWEpsTOkCHA/PkdY9Agx+9wkI263J48FoCibcWlKnoNTcN6ZRYODLgCw1e/hG57v0NwdQkUTYOiNrgUaprXHoTLoVDawmtQYIAdIbZyhNqO4aKNz6LquvmImDAc6ozpQOZoQFVdDoAOB7kEms20eXl+VZWElqLQMUREkF3+ggWNmw1v3UqphwkJvs+hL7HaEpHKMAzDMMBpJrQmTJiACRMmtHq9uLg4REREtP+AGIZhvPAVOfHlgNdS/EVRAHIbbG+DhqbGLwVGbm7jvlLFcZn4dup/kHpgJQb+8ipSD/+CEGclFKFbwaMhVdAJA4yuJsgku1SXX6EGp2qCQbMj7FgexIIC4MeVwEUXAQ8+iLCwTAQGUsRKUXTRZDCQ+KqqIhOR4mL6GhcHfPut72bDGRnA7t3UALlTp8ZRLXYTZBiGYY6H00potZUBAwagrq4OvXr1wt/+9jef6YSS+vp61NfXu15XVFSciCEyDHMG4F7X1JwLYGvwF0Vp755hzY3/rrvovY0bqf7LW8QJRcWBtLHYmzwaAb8sx93GNxGxYzW0iko4650NYkqB2iC4BAAnVBjgAABXw2P5uQIBxe4g1fTFF8CaNUi5/Q6c3WkK/rcjBUajiogIMsGoqKCvdjs5KQIU7XI6qY5r+PDGQioigs5bfj6Zi0RGuh0LuwkyDMMwx8kZnVmekJCAd955BwsWLMBXX32F9PR0jBkzBj///LPfdZ5//nlYLBbXv+Tk5BM4YoZhTlc0jSJBMnISHk5RlvBwel1SQg2GG2XRHQcytXDAABI+OTn0deDA1ou65sa/fz/wwAPAE08A771HouzHH0kDeVNVo2JP6oUo/89XwCefwDrmCpQEJUODESo0aCCBRYYYoqHZMVzWGIomhZYGKILUUk0NsHs31If/ghlLr8S9BY+ga81OVFdTk2FNI6GlKCSwTCaKBA4YQKvm5FCfrfJy3YleUag5tNFIx2O1UlTMaiXxeio3uGYYhmFOfU5b10FFUZo1w/DF5MmToSgKvvnmG5+f+4poJScns+sgwzBN0pwde1MugMdLe9SENTX+4mLgl1/oGMaOpZqmH38EDh6klLthwwBZ+urLrS93v4bXHszFmCMfYPjvb8DsrIICgQDY4YAKA5wN0SzFJbqErONSFWqQLAQpP6cTMJlgD7WgSInH68b78IEyA1U1KkJC6PiFICfGs8+mSNfSpSTC4uLILCMmhtIhY2PpmPbvB3r0oFqu9nYTbK96PYZhGObU4Ix1HWwPhg0bho8++sjv52azGWaz+QSOiGGYMwFp0hAS4vvzplwAj5f2MGjwN34hqB7LbtcbHBuNQP/+tPzhw8BvvwHnnENRpMOHG0eCUlJVRA9Jw+u/PoX/dR6Eu4ufRKwtH4qzEiocrsgWSSsZzQJc0ksIt2ZeCuBwwFRTiUTNiqcN9+Oy6G/xkvFObLOMhcmsuoQUQKLP6dRTCoOCyBbfagWGDiUROWwY8Je/UKpgewqi9q7XYxiGYU4v/nBCa/PmzUhISDjZw2AY5gwjLIwm0h3hAngi8Dd+q5XSCQMDKTrj/hzKZCIRs2cPCZa4OOrrNXOmp5CQVvVbtgCLnFNQnNYNU8vnoe+xVUiuyUEgaqBAg2iozlIbrDMUGcGSSLElBGC3Q1EUmOyVGHTkW7yj/oQ9wediXa97UdR7NISi4vvvqbZM0ygl8NAhinTFxdFx/fwzcO65JAqNxvaNNHZUvR7DMAxz+nBaCa2qqirs3bvX9frAgQPYsmULoqKikJKSgsceewyHDx/G/PnzAQCvvPIKUlNTkZmZCZvNho8++ggLFizAggULTtYhMAxzhpKSQtGK9nYBPFH4G399PUWDhAASE0moFBcD69ZR7VPXrkBZGa2raUBtre/tZ2SQANuzB9hWl4md4S8gOSwPg42bMaX6U3Q/+isC645B1ZxQFQFFUTwL2mQvLpntLj3dFUo3DHFY0ffwUpz1w0bsO3ARvkh6EHv2ZEIIWiw2lr5WVJAzYWQkCcUrrmh/weNd7+budNirF0XZFi2iGjJOI2QYhjlzOa2E1u+//+7hGPjAAw8AAKZNm4Z58+bhyJEjyMvLc31us9nw0EMP4fDhwwgKCkJmZiaWLFmCiRMnnvCxMwxzZtNcg+FT3VjB3/jr6+kYLBY9HW/XLhJZsbH0eUgIcNZZtExTImL0aBI2v/0GpKSoCAxMhWpJxf8wFan7VyL1pw/Qu3I1IqsOAY6GZliyF5es03JHobRCRQgoBhUGpxPB1UXI2PEl/pT1K4K167Ew+EYUBaciMVFFcDCl8MnoW1IS1Zi1N7Kpsrv9vfuQvRshMwzDMGcmp60ZxomiNQVvDMMwvupy2stY4UTgPX6zGSgqos+GDaOUux9/JFOJgAASLQkJlIKnKM2bfrin1HmLUaOqISM4F702foBxu99AoL0KiiJg1OwkuDRKI1QAzzTCBkEmNA2akJVdAg4YUKgmYUvkBfgq9UEcDM0EQMdltdL1ePnl1oud5gwutm8HnnmGxKbB0Hh9h4NcEJ94glwPGYZhmNMHNsNgGIY5SfhrMHyqRrK88TX+6mrg9dcpWhUYSA5+ZjOJrOBginRJ3dOc6Ye0pJdi7vBh2mZSEnDkiIrtVWkoH/sUajMG4fxlTyK0PB8hqIRBc0CB6tZjC54RLoXcCVWQY6EDBhjhQIJ2CHFln2FA9a/4NPkRLE2YAZNJRVUVRZzas6GzFNKne70ewzAM0z6w0GIYhmln2sMF8GTia/xSHP3+O6UNAlSzJW3SJS0REd5iLiQE+PRTimrJmqa19VPwTXw3XIR5GFS9CmmOHARpNRDQqPOWYoAqHCS4VJXCRJBuhUqD4KL6LYOwI6FuH+7Y9yAyjv2Kt4IeQlBQJiZP9hTAzUWq/BlcbNoE7NwJXHUV0K8ficbTuV6PYRiGaR9YaDEMwzDNIsVRbi7wr3+RqcXgwZ5CpDUiwl3M5eYCu3frNU3SUv4IMlHc+wUsrsxDwtHNuKz2Ewwp/wHBqIbQAAEDDArVaHmgABCyJ5cGJ6jOK1irwOiSL5Bp+g27Bl2P0ak3AloqoKoekaraWvLdSEoCpkyh2jLAt8FFfT2ZgezbB2zbRrb3GRlA376nb70ewzAM0z5wjVYzcI0WwzCMJ03VWcXENLYub21NU3m5XgdmNpNL4P79QKRFwwzlfdxweA5ibYdhgh1GOAFVhaq52cAbDBCaBiGoN5dQ1IY0QxnjAqCqcHZOgfG84Tg0ejpe2DAaxaVkmJGXR82LKypoDBMnAhdfDMyf79nQ2d190WymoNrQobReTAwwaRKJr9O1Xo9hGIZpDNdoMQzDMB2GvzqrgQMbi4i21DTV15NoMZlIHx09ShGmmDgVy4NuQU7kMEzZ9xJGOlbCUlMAg+aAUBSyhAdcToQSTShQoQsxO0wwaXYoh/Lg+LwAlkUrcXnsRfht2IP4ZmcmamrIQTE6GigsBJYvp2MsKtL7iYWHe7ovCkEmIAEBuoX79u3Aww+3fyNkhmEY5vSAhRbDMAzTalpi+tHSpr3ePbzMZmogbLeTqKmoIGETGEjbzTFl4tmU/yBrUC4G7/oQ/Xd8jE7aERhsdaTIGgahAIBBhdHpgGh4R4MCo6IBAlAgIJwOBNUUY/jhL9Fl8UYciXgGWd2muFIDo6MpNXDjRoq0FRRQlCskhIRVVBSlEdbX05jNZk8L9/z807tej2EYhmk7/FyNYRiGaROyzqpPH/rqbSzhXtMUHk5pgbJpb0kJ9dtyOEisZWaSSNm5k9aPiqJliopou3Fxev2W1QoEh6ooMKfhf4Nm4cXhC1E+7krK1zMY4OpSrKpQ3JoeC4CiXkKDAkCFBhUCBmgwOmqRXJ2NvxfegptyZyG+dj8UocHhoHHY7fqmg4Io0lVURA6MQujpghYL7SskhCJ4/twXvdE0qlXbvp2+uvdqZhiGYU5POKLFMAzDtDstadq7di3w2GOUGlhXR1Gh2lrgwAE9MmQyAZGRFC2SzYbr6kiI/PILfQ0MzMSkF/6DsXetpBDaL7+QwnFFt5SGei21wQKe6rUABWgQbxTrEohwluCmvL9j/NGPsDN8ON4X07HXMRrx8SqOHaNxyQhbSQlFuCIjG9vct8bCvSXplQzDMMzpBwsthmEYpt2prCTREBLi+/PaWmDHDvrau7eeVpiXR2Lm5pvpvTVrgK++IkETFKQLkagoEl+FhRRt+u9XKjrfOxYZi0ZD+2E5al96A+bVP0K11wOagHAKaKoKg2YH4OYB5Va/JaBCgwIVGuLq8xBdXIA+WImpQRfhm/oHkRWSiX79qF6ruJjqsSoqgK5dgQEDdJv71rgvtjS90pvmDEYYhmGYkw8LLYZhGKbdaapprxCUIudwkIiQn4eHUwphVhb9e+QRslYfPhz497/JwS84GIiPJ3FVWkqpekOHkvBZtAjQpqj4etOF2NVpLAakvY9LDvwT8XUHYVDqG5wHFUihpUGFCkfD9xTTMtAIoUBAhQPRKMbkui/Rb/dGfJz+DIxdp6BrV0pfPHwYWL+exFFAAB1PayzcvdMrZTRMpldmZdExpac3rn3jCBjDMMypDwsthmEYpt3xNrhwTx8sLycxkpQERER4ruduJJGXR7VfY8fSZ3/5CwmLsjKKZiUk6A2TzWZKRdy5k0RXVZWKLcot+CF6GKaVvISRtUsRgWNQFEBtEFwK3AuhVBjghEwpVBpiWyoEFFGLbrZdeHjvTKzadBRbBt6CiAgVikI1Wj16UPpjU+6LvmhJeqX7eQDaHgFjGIZhTjwstBiGYZh2R1UpwuKraW92NgmlPn0aCwyAljt82NNIolMn4KyzKJrlcJCwslj09YODqddWWBiJH2nRXh2RiX/F/wf/3b0SV9V+gBHKanSqPwQD7A1VWdTM2KAIQIiGWBeJLdX1nYCiCITWFuHCpQ+g277vsa339fi1agCGnZOCvzyitsnCvan0SiEoanf0KIkxmYLYlggYwzAMc3JgocUwDMN0CP76bfXvT/VWwcG+1/NlJBEWRuuYTGS57s2RI9TY2GzWe1tJIWIOUnEofSzuzxuNc5NycWXdBxiX8wYC7ZUwQIOqaFBUFXBobomFcNVrCQBOQTa9ZkcVemYvQmrOMgyPSkdY/EgYd09HamZmq8+Pv/TK4mI6XwUFdCzvvANs2kQpkq2NgDEMwzAnDxZaDMMwTIfhq99WUhIwd27jtEIhKK0wO5vEWFKSvp2mUhGl+URAADkXuke6JGYzEBKmorpTGuIffQrHNg9CwptPwHBoH5T6elpBVSG0BmHlkUqouhlm0P9Bzip0ObYV6jf7gU3LgKefBqZMadW58XVMxcXAunUksBwOIC0N6NKFltmyhcw3unTxvT1fkUCGYRjm5MHJBQzDMEyH4t1vy2iktMKYGEp3s1rJPXD5ckp927cP2L2bxFh2tr4N73UcDvqalUURrMREElomU+Mx2GwktoQgIZZ69xSY//sJlKuuopUNBgiDwZVOqEKDywIegMHNEl6FgGpQoDrtpGp27QJmzgT+7/9a1QDL+5jKy/Vjk+Ps1Uv/WlVFqYRVVb631xpLeYZhGKbjYaHFMAzDnHBkWuGAAVRbtWyZbpAxbhzQrRtFcV59VRdb7uuUlgI5OfR14EDg8ceBwYNJaNlsnvuSDYXDw8kW3iVEMjOB//wHmD8fuOIK2BOS4YTRZZJBHbdkaIy+o3otuN6F00niqqgI4sEHUXHVLchZuLPFTYfdjykvj86F0Uii8eyzdct4RSHTDYBEqBB6BPDoUeDYMaqHy8ho3lKeYRiGOTEoQgjR/GJ/XCoqKmCxWGC1WhHu7VHMMAzDHBcOBzUt3rqVREJEhGcqYVYWCalHHtENHvz1kNq5k/pvHT4MdO5MESybjURWUBBFjkaN8tyWC03DrqW52PzQB5h84A0E2CqhCg2K0BoMMxyu+i1FUaAaDCSyAIqGOTVoQsCmBqE0uDPWpl2Po2NvxOibU5GR2fwzTU0DfvgBePllSrWMiqLzIARFuOrrAYMB2LOHIlxWK0W2Kiros/p6Mgp54gng4ovb59owDMMwjWmNNuCIFsMwDHPSyM+niEzv3kBkpGdtlbfBg8Q7FVGKpsxMEhqJiZSKWFhI6XSRkSSy0tKa6G2lqgjslYbvBj+Fzy58D6UxPeE0BAAAHFChNSQPAoAmVAins6EvF6BpAk5BIixAq0Ni1T5M3fYsLn17HKyX3oSD7y5vNrylqlSvFR9PqY+yXuvXX4EffwR+/hlYsQIoKqLjPnaMzl19PQlKaZW/ZIkeAWQYhmFOLmyGwTAMw5w0mrI4B1pv8HDxxUDXrsC8eVQ6pWkUHerVq/neVtKc4vvNU1B3WTcMWPkP9N67EGatFk4YGlIKKa7lngwiNGpwTBpRwKGaYNDsiKvNQ9TeAtjuXwnt14ugPvQgqcFm9r95M6UMrl+v29QbjSQcbTbg229JVA0frteeWSy0DbZ4ZxiGOXVgocUwDMOcNPxZnEvaYvCQmQm88ILv9MKmcO/9tao4E59Fv4vzrediRsUriKgrRLiogAk2aFBd0S1FAFS/5eq4BRVkEw8hYIADQZXFEF98CWzaCDzzTCN3QvdUyKFDgYMHKYIl0wHtdqpFs1hIiP3yC0W93NMsJWzxzjAMc+rAQothGIY5abTEtn3gwNYbPMj0wtYizSneew/YtEnFp0G3YLN5GG6wz8MI+4/ofGwHDE470CCthGoANDfrd0VxpQkq0KAoKhQhoNTVQsveBdtNM5Ez7ShC774FqWkqdu/W+4zV1ZGoNBqp/MtoBMrK6GtCAp0nTaMIVkUF1WlFRHiOny3eGYZhTh1YaDEMwzAnDfcoUlYWRWRCQiiSlZ9PtVV+66o6iIwMYMQI4JtvSNjsRSbmhr+AzLA8TDF8i9HZb8JizYPBUUcD02SjYxJVqpsVPBq+E5qAUxMwWovQ5dUHsG7e9/hmwPXYFzYAR80pSEpRXce9fTt97dOH6suio/XoVXk5CS1pgOFNSyOA/gxFGIZhmPaDhRbDMAxzUpFRJBnZOXyYxMLAgc3XVR0P/sRGdjbw3/9ShCk6GggNBex2FVvKU5ETfDf2jBuNsVv+gT77FiIYtXAqBjiFgFBVGDQ7AL1+SxMKDND0VEMAoajCBRWLMPSnZdhvTEdJ7/9n787DqyyvvY9/7+fZOztzGBLCGGZhG+OAAzi1auuMqNVqbWurpVpPB6sFa23FKnqqtVh8aetRW9TaVo9aRKV6VNRaZ0UNiCHMCQkhgYQhc/b03O8fa+9MBEggIMP6vFfeTfb4JO11ymKt+7e+zMoBV1GTmU8oJGmCmzfDBx9IemJOjnSzcnJkfDAzs21Bc3vd7QAWF2/fRRs/XgrevfW7VkqpQ5HGu++CxrsrpdS+sS+7LDsqNi68EJ5/Hj79VMb2qqqkwElErVdXy7mp/n09ruJRzl0+G690HV5zCCw4xOLLjiGGGz+vZfHiIb/xgcPW72P4aPZl0NxvKM9NmMkj1VNobJTOVVOTBHuEw5CaKnu1srOlANu2DYYNk6/OHcDrr99xwVRcLLvJamo6vra8fNevVUop1bPaQDtaSiml9gu7e66qp3ZUbBQWyi6uhgZZmJybK2ehqquli5SUJGN7a9bA4FMdRv1qKo43Cee++wgveBm2bsWL0RqMkSiyEnmEpt1IYaL48hPBRutJrVnO5a9dS0X/jbx72FTS0hzWrpXPz8mR81iLF8tZrVGj4Pzz4bPPdtwB7KpoBSkua2o6nofLzJTvNbFQKaV6lxZaSimlDhmet/Ni4/33obJSzkdlZkoXaflyeX59vRQgmZnw9a8nOj/58Je/kPTGG3iP/pXoO+9hKsoxsQgW09q5soCDjY8QWgwGA3iASww8hyxvE7/YNI237Hv8PXc69Tn59O8vRaDnyXWdfDJcfbV89nnn7Xj0satu3QknyH3Dhm2fVth5Z5kmFiql1J7TQksppdQho6xs18XG6tVS1AwbJt2k7GzpKCUCKCIROOqodi90HPjqV3HOOIOk0lL461/hT3/C1tdjw158/5aDJRGaISwGFxkk9OKlV4pt4LTqZxizdRHPj76Rxq9cTW29Q1OTdK6+9a220b6uOoA769YtXiwdsuHDu/7daGKhUkr1Lh0OUEopdcjY1YLkQYPksfJyOZMFUoD16QMDBsjrDz+8bRTP86C0VJICS8scvBGj4I47JB9+3Hg8vyRWyMktt7XIiuLgEGsdLJQyDMDgtyFGRFbxozXTuPBfUxkTLiItTYI5EouJu9K5W5eZCa7b1q1raICNG+W2K7uzs0wppdSOaUdLKaXUIWNXC5KbmuQMVEbGruPmd5reN2UKzujRNN06C/+L8/FFmong4o/3rpzWJcfx+PfWk1zxs1zGEojUk//50wys+JS/jrmTARdM2Wma4K66dYcdJp26FSvg+ON7b2eZUkqprmmhpZRS6pDRnQXJkybBlCmSPrijsImdjeiVl8fT+/Lz6TtvLhX/fQr8v/vxb64ikzr8hNulENLuVkYME8OFBnCjIfpXF/Oj2qk0H/NDWPtdSp0R1Dc62yUz7qpbl54uAR/p6fvPzjKllDqYabz7Lmi8u1JKHVzaF0ldFRuJiPMdxc17HtxzT9fFmufBxx/DmDFw6aUy6peVBaa4iE9/8hiHV/+bEQ2f43qR1o6Wh4tDDCCeUpjod0mXS0YLPazjsjVrOMVZJ/JG3lWsyTuDcUGndf9VaSncdpv8DF39z1Vtrezn+s534KOPOnbigsG9u7NMKaUOFj2pDbTQ2gUttJRS6uDT1dhfd4uNHRU01dXyfiUlsoMrM1OKrFGjJIBi1QqPrx5WxpDFC5j0yQPkNJcRoIUoPlyi8f1bBhvvapnEOKHjYDwPDynKPNdHS3JfivPO4amh02gZnc/110ss+44KQGulizVhAtx8s9zX051l+3LPmVJK7a90j5ZSSim1E8GgFCa7Uzh0NaJXXQ0ffghbt0rYhOfJ49GoFF5bt0JVlUNS8ggqmn/CoIFn8P26WZy+dT4BrxkPJx4B37ZvK9HT8jzTGhLvYDFejLSmao5e809GVH/Es5tv5Llnr+bmW6S7VV7evdHAnkS47/Q8mnbBlFKqS9rR2gXtaCmllGqvtBRmzJAFxomvpUslaKKlRUb0fD7pZAUCUoTl5sKmTTKu2L+/xMY7eJxd9SiXls9mQHMpKTTHz2o58VFCi4cbj4dPLDo2RB0fjo0BBmscIm4Ki4ZdzJj/mc6Qs/L3qFvXlR2dRysv7zhqqZRShwLtaCmllFJ7SWOjxKSvWCFFh7XSserbV1ILQe5PSpK9Wz4fVFVJx6y8HJqb5f6kJIfn+k7lzaZJfKtqFhdG55FGI1580bFpHR+08TJL4jKM9TA2MWZoSIrUc0Lp09gffwqz7iQ4Zcpud+s629WC52XL4LnnpDuoY4RKKdWRFlpKKaVUNxUXwx//KAVHVpYsLwYpvsJhKaAyM9t2cTU1QSwmz3McKb6ys6XYqq+XIswOzmdm0lzKOYlrSmeQxVaMNWBt/LyWbNhK7NnCyp9svABzAJ8XwildjnfNtTgbN+JMncqIEXte+XRnwXNxsTyvJ6OISil1KNBCSymllOqG9t2dSZPkdvly2LBBiqhQSG4zMyUMIxKR0UFf/H9pa2ulGBs2TDpMoZA8bi28+abDM841VA7L5eaGGWTXrsEXC0kchpVwjESXqy2T0MFYDxkihFjE4m7aROSn02h+5T3Sfz0dpyB/j37mXUXGp6VJ/H19/R59jFJKHZS00a+UUkp1Q+fuTk4OnHIKnHWWBENkZ0NqqhRgkYj82XWluMrIkP1VKSmwcqUUY7m50KePfPXvL2e5ikZN4bmvP8Gy/MtoTMvBc1xiuICJx8C3hWS0lVuJAA2J0DDNDfjmP0P1Vy+n4q65UiHupvYLnrvS2CiPZ2Ts9kcopdRBSwstpZRSqhu66u4YI2ezjjtOiiXPkyIrGpWOVWOjdLkcRwqtSZOk8Pr4Y+lwRaNQVwd+v4wi+v2wJjmf+ef/hX+c+Thv515KpTsMz/jwE8FC/AxXW5HVNlToteYV+rwQfTetos/MaWy9ZCoUFe3Wz5xY8FxeLp239hILnoNBeZ5SSqmOdHRQKaWU6ob23Z3OQVM5OVBQIGeytmyRIiQUksKpTx85yzR+vBRloRCMHSvLgysq5D1PO01e/9ln0jWrqHBYl/5VVl5yBtsWl/Jt8zeO+vwfZNRvwBdpjgdjOBhird2txN6tRG/LGksgUo9vwVN4xR+w9fL/ovrEKSQflkfeCKdb4RWOQ48i45VSSrXRePdd0Hh3pZRSIN2qXS0EHj4c1qxpSx1MSpJCKitLnl9bKwXW7bdLcdI5FbDzUmDPg1//Wt5neEMRZ342i6PWzCMQaSSKG19yLP8z3tbdkp1cnnFxbbT1GqPGz7qsI1k56DQqzryKL12X3+1Y9t6OjFdKqQOVxrsrpZRSvaw73Z2rr4bnn5dibPTo7Yux9ethwgRJ6OuqC+Q4HdP7ioraR8nn86x/LpcOPYlr180gy9uKxeDEgzKIx2LYeLElu7YkndDDxW/DjKxdwsCWUrY89jJvLbkB/nQ1wfxdt6P2ZMGzUkodqrTQUkoppbopGJQFvYnuTmL0b8KEtu6O4/TOqF1XUfKOz+Fv/mtYkZHLz+tnMMpbTRIt8TFCL97hSqw9tvHYDINjwFqDY6Mkh+oZHKrj4nensfrb7+A93r10ws5FoFJKqZ3T0cFd0NFBpZRSnXUe8evc3dnTUbvOY4qJKPmaGgnQ2LoVxseKmO7M4uRN80n2muMRGR6JJpoTD8eQLpeLIQbx7lYMF5coYZKJjRpD5uw7YcqUXf5cSil1qOtJbaCF1i5ooaWUUmp37Kpo2dnjpaVw223SAUv8T4+1csYrFJKvmhoYf5jH0Nce5ayi2QyJlpJMc3zFsYODnM+SIitxlsvEkwtdXDxiGKxxcfr2ZdNP7+SvzlSWr3Rai8Px42VcUs9hKaWU0DNaSiml1BdsZ6N2XXW82hc1O4qS79NH/hyNQkMDfOtKh6wfT2XNgknUPjiLI1bOIznaGB8l9EE8CF7C4IHEKGH8JJcBsBZnyyb63jmNiYPfw/nydBrH5dPYKB218nIZl9RiSymlekYHApRSSql9qLgY5syRIiY7W0ImsrPl+zlz5PHuLgrOypJi7is/yef4JXNpmnkfsX45+JMMbpITL6gS64zbkyh4g40HdhiSog2cUvkMV718OccunktWhtc6tvjcc217jz1POm5Ll8ptV/uQu/McpZQ62GlHSymllNpHPE86WTU1HSPiMzPl+2XLpKi56SbpcO0oSj6RXth+UbDjc8i55RrIz5W5w/Xr8erqIRKNjxJ68f1brVeDiQ8axgAHgxsN0X/zKs5aOI289e/w3onTGTo0n+JiGXNsbt55Jw523a1TSqlDhRZaSiml1D5SViYFyLBhHYsnkO+HDpVCZf363V8U7E2eQmXyaPxPPEZm4Zs4q1Zim5tw8PAwxHDxxaPgMUZGB2kbI7QGkkL1HP75MwxYt4g3j7qRF/pfzZIlDi+/LEXisGGQmgqVlfDmmxJD/8tfyvXMmdP2nMQ16wiiUupQpIWWUkoptY90dfaqvbQ0iYyvr4eCgl1HyXfW1k3KJ9T8WzIHlDGIQiateYKTG18hncb4iS1HBgqtjBAmdm9hIIaDi8GJhsjZuooL/jONtKx3WBSbTo0vv3WccMkSuY1EZM/XjTfKNbXv1lkrXbz+/WHdOnj2WbjlFk0yVEodGrTQUkoppfaR9mevugqrSpy9ysiQ73uyKDhx9ivRTWpqcnhz6Qi2bh3B37Mv5Jvpc7m+egaZ8UXHBhsvqmKYRLllXRwbaz3VZbGk2nrO2vY0h734KfOOupPVKVNYtgyamuSMmN8PgYB0tVavhlNPlSKrurpjJL3nSQds4kT46lf33u9YKaX2F/pvSkoppdQ+kpcn55XKy6Xb017i7FUw2OnsVTy9sKBAbrscF+x09qulBRYulMImFoPNWx3+6r+Ge0Y/TGlgPCGSJOzdcTBOfKFx/BSXQ7Q1KAPkLwp+QoyKLOfaj6+l/3N/pnydR2qqFFiOIwVgICCFYlkZbNoEH34on5+aKh2tjAyoqoLf/Q5ee00DMpRSBz8ttJRSSql9xHHk7FV2tpy9qq2Vbk9trXy/s7NXO9P+7FdNDbz9ttympso4YkoK1NXBwsAUHjvzCT4eeRnNvgxcFxzHwTgO+H24RIDEKCHxc13CYMlmE7fXT+O+2qmklRa1piKGw1JoZWRIcbV4sXS8cnLk/uZmub+lRUYOb7oJ7r5bunBKKXWw0kJLKaWU2oeCQTl7dcwxsHkzrFwptxMm7H5YROLsV2qqFFyNjZCUJF/GSLGTlCTFz6KmfF77xlz+dvR9tOSNlQcBY2WhsYkPFmISfwYnfguGVBq4MPIMD2y9nNPWzgXPo64OBg2CIUPkZ6moaH1bGhvl+9paSE+Xbl1LC7z7blucvVJKHYz0jJZSSim1j/Xk7FV3JM5+VVZKJysrSwqcWAx8Prl1HDkXVl0NK1c7LBk4lYk/mcQxb8zCeW4+pqkZL/7vrwYpvGR4UIIyEt0tgyGJEGO8VfxyyzReW/wOTw+dji83v7XIq6uDbduksIrFpMBLS4OBA9vOqOXlte3oGjdOAzKUUgcf/T9rSiml1BegO2evuqv92a9IRAqc1FQIheQsVCgk3zuOnJN6+21ZJHz/wnzuGTOXimn3wdixeP5A63t68WRCEz+xBRY3/ieDxcOSTj3nNzzN7yq+Se5HL7Bhg3y2MTIuuHGjFHZJSW0R9eGwFH/JyW1x9mVle/KbVEqp/ZMWWkoppdQBLnH2KydHukUNDRJA4TjSWXIcOae1bp081qcPnHyyPL9wicNdG6ZScvdTNJ3/dcJ+yZ63GLz40KBtHR30WkcJE7u3kmyIQXXLuWXttVze8Gcy0jxycqSoysyUoqulRa7TWul2ZWdL1y0tTR6rr5eCsLQUli6VWw3LUEod6Iy1nXOPVHt1dXVkZWVRW1tLZldZvEoppdR+oqhI9lmtWCFFTCwmYRuuKwVXQ4OM751zDgwYIK+xVoI4JkyAm2/yqPzvuWT+bgZJTVtxrIfBw4vHwAPxssuJl1zS8Yri4CNKE+m8NeASHs+ezjtb82lpkbFBz5Piqk8f6axNnChFXm2tnOn6znfgo4/kfFlLi3S7xo+X4nFXZ9Y8r/dGMJVSald6UhtoobULWmgppZQ6kBQVSaLfpk2SQpibC6tWwTvvSBfpjDM6FlmJYqexEWbNglGjwHv+BSK/mIFZuxo33EIUHy5RHLzWhcdgcaTWIoqLSwyLJWRSqUoewT8G3Mij9mo21TiEQtJRO/JIOPpoKbISBd7QoVKMlZdLF65PHxktXL9eirOdBYS0LWjueYGmlFK7QwutXqSFllJKqQNN5wKkuVnG8U4+WTpa0HGhcDgsxc5FF8F118WLlKIimDWL6D/nY5uawZPulol/hjHgWA/PcfC8tvFCLz5YGHJSeCv7Yh5Mnc7bW/JJSoKzz5bzaI2NUkj17y+f++mn0oVKhHdkZ0tARnW1FGZXXCGvad+x6rygOS1NnlNevusCTSmldtdBW2i99dZb/O53v+OTTz6hsrKS+fPnc9FFF+30Nf/5z3/42c9+RlFREYMHD+bnP/851113Xbc/UwstpZRSB6L2I3W1tfDQQ9JJSiQPfvihFDlZWVLg1NfDyJEwfHi7IsXz4NFH8X4/G1tSimlplkLLdTAxGSW0joMXSyQUtp3tcokRNsmU+sfw4OA7eStrCqNGtQVhBIPSWfvtb6W7lZ0Nfr+EedTWyojhkCHyM4wc2fa68ePhwgvh+eehsFAWNBvT9nN3GIW8WccIlVK9qye1wR7Fu7e0tJCcnLwnb9EjjY2NHHXUUVx99dVccsklu3x+SUkJ5513Htdccw1///vfeffdd/nhD39ITk5Ot16vlFJKHQh2dE5pxIi2x996SwqTYFA6WYmFwiCF1+DBcNxx0ilqi1x3YOpUnEmTZK5w3jxpGxkjH2AtxlqMAWslBj6xe8sAPhtidHg5t62/lldGbmTY9VPJ6uuQlSUjgz/7mVzHyJFyjgxk/1ZOjnS8NmyQjzniiLZOWGGhNNsaGmD06I5FFsj37dMME78DpZTa13pcaHmex3//93/z4IMPsnHjRlauXMmoUaOYMWMGI0aMYOrUqXvjOgE499xzOffcc7v9/AcffJC8vDzuv/9+AILBIB9//DGzZs3SQksppdRBoTvnlBKphOXl8PHHUsBkZkrse12ddI/Gj5fndVmk5OfD3Llw0kkwYwZs3Uq8ugLHwYnFIB6bIcVWYucWeFj6RDdx4X+m8cnU9/hg8nS+dF0+69dLMZWZ2RbY0V443JZQ6Pe37QE7/HB4/33ZGVZQ0PXvJC1NliTX1++N37hSSnVPjxvqd911F4899hj33nsvSUlJrfcXFBTwl7/8pVcvbk+9//77nHXWWR3uO/vss/n444+JRCJdviYUClFXV9fhSymllNofJc4pFRa2nWvKzpbv58yRxxOCQRkJHDNGukj19XJ2a9CgthRA6Bi53oHjwDXXwMMPS1WW+DuA44AxcjLLGDCmNS6jrbtlSPEamFj2DGc/ejmL/msuSwo9XFfCOmprpWZL2LoVtmyR66irk+LwnXek85boWDU2SrHVlcZGKTgzMnrpF62UUruhx4XW448/zsMPP8y3vvUt3Hb//HTkkUeyfPnyXr24PVVVVUVubm6H+3Jzc4lGo9TU1HT5mrvvvpusrKzWr2HDhu2LS1VKKaV6xPOkk1VTI12ezEzpCiW6PjU1MgLYfh9VMAg33ADHHitfp58Op5zSVmRBN4qUKVPgiSfgssvanuQ40tkyBtdYXAMgH+yaxAUYfLEQQxpXcfG70xg0YyqjW4rIy5OOWnW1FFYNDdLpammRomrAAHm8slLOlVVXS3GYliYdus4nza2V1weDMkLZnd+j7u9SSu0NPR4drKioYMyYMdvd73neDrtEXyTTaXg7kf3R+f6EW265hZ/97Get39fV1WmxpZRSar9TVibjgsOG9eyc0ogRcharsFCCMDoHSaxfL0ESOytSvGA+ZTPm4o44hQH/mE3ShlJMc7N8tuNgozJKiDGYdpWQsRZrICVaT8HyZxhauYjnRtxI+glXs3ylQ3W1fH5jo1yL60p3KxSSTl1jo/zMBQUSQ5+R0RYRn0gdTMTCX3TRroMw9mY8vO73Ukr1uNDKz8/n7bffZvjw4R3uf+aZZzjmmGN67cJ6w8CBA6mqqupw36ZNm/D5fPTv37/L1wQCAQKBwL64PKWUUmq31ddLcZCW1vXjOzqn1P681u4UKW3FiUNLy1RGjZ/EN9JnccSKefhaJCjDOg54trXIMngyTGjAGgcT724NqF3Fd5dO44Oad/joS9N5Py2f0lL57NRUOZvl80mXKxSSzlt1NaxcCaeeKs2155+XQqmiQgqlCRPk+ndVKO0oHr6wUH43exIPr/u9lFKwG4XWr3/9a6688koqKirwPI9nn32WFStW8Pjjj/Ovf/1rb1zjbjvxxBNZsGBBh/teffVVjjvuOPx+/xd0VUoppdSey8iQv8A3Nsq4YGc7GwFMnNdKFAPdLVK6Kk4aGvL5VWQu5zsn8b01Mwg0xYMysK0x7xY5txXDxbExTDwM3jGWlGg9p2x4mtELPqUk5U6i0Snk5kJ6upzTCoel4Gpuhqoqeevx49uuMxjseeeo89hloquXGLtctqx98mI3/wPZye+otwo4pdSBpceF1gUXXMBTTz3Fb37zG4wx3HbbbUyYMIEFCxZw5pln7o1rbNXQ0MDq1atbvy8pKWHx4sX069ePvLw8brnlFioqKnj88ccBuO666/jjH//Iz372M6655href/995s6dy5NPPrlXr1MppZTa2/LypODY0S6pXY0ABoNSSHS3SOmqOEksPa6udvjNlmtY3ieXXwZmkF27GhNrwcPBwcPEiy5jDMaLQLzwimdnkGRDDGtazt2N19Kvz0b+kzuVQIpDSookJNbWyn6thgbpdKWnt11X+xj77trdsctd2ZsFnFLqwLNbe7TOPvtszj777N6+ll36+OOPOf3001u/T5yl+u53v8tjjz1GZWUlZWVlrY+PHDmSl156iRtvvJE//elPDB48mDlz5mi0u1JKqQPeno4AJt6ju4VE5+Kk89Lj5GRYWD+F8NDRfHvTLI5fPx/T3IxnHYyxxIwPnxeOv5uHcRxMPHnCIOe3+kQ3ceuWaRwXeo8Fh01nmcnHWgk4TE2Vn23UKEkinDNn97tDuzt22dPfUXu630upQ88eLSze10477bTWMIuuPPbYY9vd9+Uvf5lPP/10L16VUkop9cXY3RHAruwqvKF9cWJtx6XHxsjrGxuB/Hzm5M7lu0ecwhlLZuOUleJGWsBaLAYHT+LgE3u4oF10oCHZa+Dchmco+GwRD6fdyOPO1aSkODQ3y89WUCALjjt3h3oSPrEnY5c7s7cKOKXUganHhZbjODtM7AOIxWJ7dEFKKaWU6r6ejgB2pTvhDe2LE8+T8bj2qYXhsARXJCfDkGEOT22eytEPTGLQE7Owz87H19yMwcF40sFqLa48DxwnHpgBYEiyIUZEVnHrtmkc43+Hh+x0VvjyGTVKiqzO3aHm5p6FT3R37HLoUIl87+7vdW8VcEqpA1OPC6358+d3+D4SiVBYWMhf//pX7rjjjl67MKWUUkp1z+6cU0rYUXjDp59CUZGsyzrqKCk6EsVJ//4QjUpIBUhxUlcn+62ysiAWk87NlkH5DHlkLpx6CsyeLVVLPAYex5EnJpYcW4txiK/fshgs6dRzceRpJjR+ysND7yRy7JTWoijRHVqyBF5+eefhE10VorsauywogHvv7Vly4J6em1NKHVyM3dksXg888cQTPPXUUzz//PO98Xb7jbq6OrKysqitrSWzq3+eUkoppQ5Qngf33LN9YVBdLQXYmjXSmTn6aCkujjwSXnwR1q2DkhJ5zBgpslJTYeJEGSWsrYXNm2HmzHYFYFERzJoF8+ZJReO6cgHt/xrieXjGwVokoZBYPErDpSWlL29+5U4WT5iKNQ61tVJcDRwo9VtXhU2iiMrOhhUrti+YYPtOWDAoRdaLL25fvJWXy3vt7GxY+8K1qwJOUweVOrD1pDbotUJrzZo1HHnkkTQ2NvbG2+03tNBSSil1sCothdtukwIg8T9x7UMuAgHpXJ1wghRT2dlw/vmweLHUSzU10LevFFfjx8ttosCZMAFuvrnTqJ3nwdy5MGOGbCL2vNbRQRJHD6zFOg7WGLyYBQzWODg2StifTnH+Jbw7aTpv1eQzciRUVsrndvU/0SUl8P77MGaMXF9XBVPnbtfQodLJ2lFXaoc/WztdjWIGgz0/N6eU2v/0pDbolTCM5uZm/vCHPzB06NDeeDullFJK7QOdwxs6h1x4nuyuqq+HwYNlVG/pUvjFL2DSJHj4YYlcP+wwiVyvrd154qGHQ9mZ10BLLoMemEFS2WpMS0tbmoUnQRkmPkqIA9azeBbAkBRp4PDPn6HfmkVkTbiRrG9ezT+edLoMn7C27fxWXl5bIdY5av3mmzuOXZaW7nlyYG+cm1NKHfh6XGj17du3QxiGtZb6+npSU1P5+9//3qsXp5RSSqm9p3N4Q2IcLytLiq2qKimkCgtl9C49HT74QIqpr34Vhgxp69xs2LDzxMOOXZ4pjBo7msuTZ5G/aj5uqFni3ukYlOF4Hp7j4FovfpfBjYYY0rCKbyyaxpY/v8P77nQaG/O362jV1sr1p6fLdVkr94VCEhefkQHvvSdfJ53UVgT1VnLgnpybU0odHHpcaM2ePbtDoeU4Djk5OUycOJG+ffv26sUppZRSau/pHN4QCsmoYCQihVNDgxRdAwfK/Vu2SPGyZIkUEd3t3HQVuFFWls931szl/NRT+AGzGdhSSpLXjGPAuG1BGY4xWCwYsB54WDxrcZrqyXr1aa5J+ZS/lt+JvXBKhw5US4tc09ixkoj4zjvy+Y2N8nMl/Pa3cOKJbSEXmhyolOotPS60rrrqqr1wGUoppZTa1zovPc7IkPG4igopKNLSpMhyXfnKzJRC69134YIL5PW76tx4nnSyamrazjxVV0s2Rsw6zMuaStXwSfygYRZHrppHcqwRg8FxHGlDWYsBPM/D4gBWggot+GMhhjYs5ydLr+Xx8EYqzppKarpDY6MUf6mpcs0ffSQduqQk+bkiEanjHEfu65xQqMmBSqne0K1C67PPPuv2Gx555JG7fTFKKaWU2rfaLz0uLpYipK4O+vWTIqv9+a36ejmjVFm58zNK7ZWVdTzz1PkcWCgEhc35PH3WXDaOOYkvL5xBWmQrfidxTsvBxmLEyy0MrRnwGMAxln7eJn6wYhqfhN7j+THT2Zybz8knSzfr9del2MvOlmIqGpWfqalJCq1t2+Dkk+VnT5zZ2lX0e1fnz5RSqrNuFVpHH300xhh2FVBojNGFxUoppdQBpv0I4PPPw+9/L92gRAJ7ONwW4V5QICOEuzqjlND5zFP7c2DGSEepvh5awg6Lj72GaieXs96awdCW1bjhdkEZ1gPadm4JKb/AkOw1cFLFMxwdW8S64I2kX341q9c6/N//tRWJjY2yVLm5WT43J0eupa6uY8hF++Jz+XLp8O3s/JlSSnWlW4VWSUnJ3r4OpZRSSn2BEiOAF14Ib7whBUhDgxQoPp8sIx4/XgqUnpxR6nzmKXEOLLHsOByW9w8E5PvSI6Zw99bR3OzOIu+T+TihZqxxkKB3WoMyDIlRQo/EPYRCpJStYuSfpvHpgndYNGk6Awbk4/dLsZSIWk9Pl85USors+wqFZAlz+5ALTQ5USu2pbhVaw4cP39vXoZRSSqn9QF6eLB7+9FNJFQyHpQjKypLHE3ukuntGqXPgRiAghVUkIkVbXZ0UcVlZcnZr8WKorMxn/fi5XJh/ClPWzKZfbSkm1gyAxcEhPkrYrrsV37iFNZASrWdi2TPkbVpEQ+qNbDjrakaOdHjnHemsJRYtt7TItSQlSfhHc7N03BKrvTQ5UCm1J3Z7j9ayZcsoKysjHA53uH/KlCl7fFFKKaWU+mK0D8jYsKHtjFJd3e6dUeocuDFkiJz/Wr9eipy0NCnEampkUXJVFQwfDkdPcChumsqyjElcsm4WE0rmEYg0yiJj62Cwrd0tiwc4ElxhHAwGXyzE0KZV/KJxGq+98A4VV0xn3bB8KivluqyVnykzU3aDrVkjf37oIXjrrbYUwq54nna6lFK7ZuyuDl51snbtWi6++GKWLl3a4dxWIvL9YDuj1ZPtz0oppdTBouPeKxm5CwZ3/4xS+/fbuBFWrpQzYEcdJSN6//kPrFsHubmyDDknR15nLRQXeVy8dS7nvjeDlNBWHOvh4OG1drdkrNAzPkkk9GKAwTMOxsZoIZnK1DG8cMKd/KN2Sut4oN8vY4xNTVL8nXSSnEMrL5eC8vrrO/6snidjlQsWyHNcV8YPx4/feWGmlDp49KQ26HGhdcEFF+C6Ln/+858ZNWoUH330EZs3b2batGnMmjWLU089dY8ufn+jhZZSSqlDVW93btoXKkVF0r1qapKCp6lJ3v/oo9uKrITaWjlLdePYFxj0pxn0qV6Nz2shhg+XqIwM4oDjYrwI8dNcxHBxiRHDYHGpc/vyl+F38kzGVDwcmprkfNbo0VIktS/uEiOSN98sP3NxsXS7XnpJRgzT0yWVMS9Prr2rwkwpdfDpSW3Q49HB999/nzfeeIOcnBwcx8FxHE455RTuvvturr/+egoLC3f7wpVSSim1/+jtM0orVsC8eTImeOSR0rmqrJSiprkZjjhi+yILZLywogJ8F01hwEWjaZg5C9/L8/GHmsE6eFFLzPHh8xLHGSQow+ARzyoELP1im7i+bBqXTnqPpWdN509v5jN8eFv0fIIxHVMIm5vh//0/WXoM8juJRqUzV18PJ5wg58uee066czpGqJQC6PH/KYjFYqSnpwOQnZ3Nhg0bAAnMWLFiRe9enVJKKaUOCp0XF2dmyhmtYcOkUDEGPv+89dhVB42NMrqYlgZlGfmsu3UuW2+9D8aOxQQCUiR5MkBoWmMxDIn4d4PFMQCGQLSBQe88wzG/vZxjCudSutajpmb7z0xLk5HJ2lq57rIyKaD695eRwUBAisKmJikghwxpK8yUUgp2o6N1xBFH8NlnnzFq1CgmTpzIvffeS1JSEg8//DCjRo3aG9eolFJKqb2kN8YDu/MenRcXt9enj3SQ1q+XBcJ9+7Y9Zq3cP3QoPPmkFDUtLQ7JyVM5+SuTuKx8Fhmvzcc0NsdDMlpfGf+zLDm28e4WGJIIMaRpFTOapvHqp+/w9KbpcHp+h25aorirr5frzs6WM2SJWHqQnyMzU4rHaFQKs+7uF1NKHfx6XGjdeuutNDY2AnDXXXcxefJkTj31VPr3789TTz3V6xeolFJKqb2jq8CLngY7dPc9Oi8ubs8YWYRcVSXvl58vz2tslCLLdWXEcP16KdQSj71Wns+S3Ln86uen0P/x2Zh1pbgRiYH3cPAhoRgYA9Z2KMIskGYbOKdhHoetLeIp/23YiyYnnsr69XJGKyNDrjs3ty2WPrHzC9oWLtfW9my/GGh6oVIHu24XWkcffTTf//73+da3vkXf+D81jRo1imXLlrFlyxb69u3bmjyolFJKqf1bcTHMmSPdmPbFS2GhJOp1J9ihJ+/ReXGxtVKchEJSuCQnyxmtww6Ts08VFXLf0UfL+aeKChk5TPxVIzNTvl+2zOHx4VO5+dlJcN8sok/Pw21ujA8RSgx8W+6XdLcAYsaVu6xlSLiEy1fMZMHKkWwemN8hxj4lRa7D55P7KitlZDBxHeGwFIKbN8Opp3Z/v1hvFLlKqf1bt//dZOLEidx6660MHjyYb37zm7z++uutj/Xr10+LLKWUUuoA0dV5KddtK15qaiTYwfN67z0Si4vLy2HTJgmW+Pe/ZWfVG2/Aa6/BqFFw990wcybMmCG3V1wBW7Z0HDm0VkYMN22SAm7ZMjm75TwyF9/999GQkoONn9OyQAyHdiuN8XBwieHaGMm0ECDEyFAxp7xxB1uro0yY0FYkJq57/XoJukhNlcKvpQViMSmwPE+e1939YokCtbBQirdx4+S2sFDuLy7eg/9wlVL7jW4XWg899BBVVVU8/PDDVFVVcdZZZzFixAhmzpxJmZ78VEoppQ4YOzsv1Tlxr7feI7G42HXhlVfk/kTARXOzhEpUVsKqVZLqV1Agt42NHUcOq6s7FmmLFsHixbBkiXzIilOv4cFjH2a1fzwhkgCwOHitJ7YcHGNxbBSLwRgIECKVJk7ZuoA/br2CmycXtXaVEtednS2fnZ8vY4S1tVBSIj/rmWfCT3+6606U58HatfDgg3LeKxjcvSJXKXVg6NEkcHJyMldeeSVvvPEGq1ev5sorr2Tu3LmMGjWKs88+m6effnpvXadSSimlesnOzktBW+LezoIdduc9xo2T3VMpKfLV0CCjg3l5cM450iHqXGS0HzmsroYPP5SCLDVVEgB9Pqirg2eekd1c8+fDxwOnMO9rT/BW7mU0Ohnx/MFEsWUweFhrMMaT9lj8/JYbDZH+5ks4V1wOc+e2XkgwKB2uY46Rp2dny31f+xrcdhtcdZX8PInr9jwoLYWlS+XW86TovOcemD5dfsaSEnj3XfmZErpb5CqlDgw9DsNIGDlyJHfeeSczZ85k3rx5/OAHP+C1117jsssu683rU0oppVQv63xeqrNE4t7Ogh125z3KymQM8MwzpWBJnM/KypIiIxBoKzIS+7sSo3uffiqvbWpqOyPleVLI5ebKCN+jj8r5rqFDYYvNZ8GFc6n8/BTOXjab/vWl+GLNslvLSlfLWskmdIlhXDnPRTgsbbVp06R1Nn065OcTDEqhmAiv2LhRir5XX4UXXmg7Y3XkkfDZZx3PXvXrJ0EfsZgsOk5Lk99LZaV0xiZObNsfltgZpumFSh34drvQAvj3v//No48+yrPPPovP5+Oaa67pretSSiml1F6SKF4KCzsGTEDHxL2dBTvsznskumDp6TIu11lXRUZidK+oCNaskaIlcUarslKKl8R7l5TIe2dkSFEWjTq855vKu4dNYuq2WRyz9p8Eoo1yZstK+8kxHsYYHOu1phNirbzh009LhXfnnTBlSusC5+LitsXL7UNA/vMf+Mc/pNBLJCc2NMDChVIgnnOOpBT6/fJz5eRIRysRH29M94pcpdSBocchomVlZcycOZNRo0bxla98hXXr1vHAAw9QWVnJgw8+uDeuUSmllFK9qP25o2XLpKsSjcrtsmVtiXs7C3bYnfdo3wXryo6KjGAQLrtMOmfRKGzYIIUcSFEzeLA8tnmz7Nlau7ZttDA1FT6oz+eG9Lk8f8StNPszcZMcXBd8jsVxnfgy47j2G5NDIamCrr0W/vxn8LwdhoBkZEj0e22t3GZkyP3Wyu/AdeXaMjPld1NbKx+R2MNVW9tWoCZCOJRSB7Zud7SeeOIJHn30Uf7973+Tm5vLd77zHaZOncqYMWP25vUppZRSai9InDtKRIwn4tQnTJACqTsR4z19jz3ppB11lES9+/3S3TIGBg3qWMg5jrxPU1Nb1ygQkM7Rpk0Ov3FuYtSxGzi+6gWcjRulInJdaYs5jlRxiZnE1uuysGkTsRumUTP/PVZMns7HH+czfHjH66+tlUIvJ0dua2tlEXMoJG/ft68UVHV18juorZVuVkaGTCtu3iy/v+4UuUqpA0O3C62rrrqK888/n+eee47zzjsPR/8vgFJKKXVA63zuaHeW5vbkPRJdsPJy6XoNHdpxMfHOioy8PPmsd96RlMLs7LbnWSuFijEyyrdpk4wV9u8vo3rhsNRQMeMQ+/51OB+14L26EDZuJOZPxm1pwEQikkuYGB8EPM/iWTDWQFMD6f/3DLmvLWJ48o0smXA1wXyn9WxVKCSf0bcvbN0q34MUej6fvG00Kvfn5sq5rOXLpTvX1CS/g+OP736Rq5Ta/3W70Fq/fj0DBgzYm9eilFJKqX0sce5oX73H7nbSEkXa4sVy/io5WRpP4bB0iQIBeV7//tKo6tdPCrL6eil0hg6V52ScEGRN7g3UfJZMcMM/CLQ0gPUwGFwHjLVYwHoeNn6WS/pbhiRCjIys4rbINF5Z9A7zNk2H0/PJyWkrqBob5TZxPVlZUhSWlUkyYeL+nBy51o8/hrFjJR5+xAjtZCl1MOl2oaVFllJKKaV6w+520oJBOS61apW8LlHUDBok57Q++0zuS0uDSZOki5RINgQp0DZuhHkvBtk8ZDYXpx/J6Ytn02dbKb5os0wMOg7Gi+HFFx4T//+FFGHp1HN+0zMcs3oRz9sbafr61WRlOfTvDytXwmGHSYEFcg3jxrXFvIN0thJdvOHD4brrZFmzUurgYqxtf+pTdVZXV0dWVha1tbVkdpVfq5RSSql9xvPg7rtlB1VennS2EkXN22+3FTqnntp2hspaGVU8+mj5fvHitjNiOZuKOPHdWRy+fB7+cCMRXFw8QP56lFhzLPlhhgh+fETiO7kcWkwKReMu5j/HT+fNjfmsXy/ds8MP7zgW6bqyQ2zLlrbY92BQRwWVOtD0pDbYo3h3pZRSSql9yXFkUfD69RIuMXSohE00NkoARlaW3NbVbX/+a+JEePxxOceVKMKqB+Sz4KK5lH16Eif/3wzSY1uxGBwsMRxcYhDvbMVw8RFr7XV5QKpt4KiV88itLmL0+beR+pPJrXu0EmORRx/dtisr0cHLyur5eTil1IFFCy2llFJKHVB2dM7rtNOgoIDtCp3E+a9oVLpJaWkd388ah3l9ruF/A7nc0jSD0awmQAseDgavteiyGAwRDAYHiDkungdJSZbhXgkjl83EGTmS887L326x8eOPt3Wyxo+X82ZaZCl1cOtxoTVq1CgWLVpE//79O9y/bds2JkyYwNq1a3vt4pRSSimlurKzc17nndf1/aWlbXu82k/8WCsjh0VmCuv7jOYmdxZf3jKfZNuMxcHDEsVHgLA8H4uHi2tjuMRwoi2YqIHlxXDHHThPPMGIEb4dLjYuLJTkxeuv17FBpQ5mPf63lNLSUmKJNezthEIhKioqeuWilFJKKaV2JZF2WFDQMbFvR/cn9niVl3fcS1xbK1/GwLr0fGYfPpc5efex1h1LCEnSMFgkm5B4p8vi2ijGlftsKITX2ETs+QU0XHgF0SVFXS42zsyU72tq4LnnOqzsUkodZLrd0XrhhRda//zKK6+QlTh5CsRiMV5//XVG7Gk+rFJKKaXUXrKjPV6bN7d1v5KSZJTwjRFT+TxtEpeUzuKspvkkk+hueYDBMR7GGBzr4VmD51mMBRsO4b78Eps+KCZt2I3knX41xnT8d21j5LOLi6Xztjt/ffK8Pdt/ppTa+7qdOphYUGyMofNL/H4/I0aM4L777mPy5Mm9f5VfIE0dVEoppQ4uxcVt57taWuTsVkmJFCsVFbJAODMzvuy4xePUNY/yvdrZjHJL8UeaJUjDcTDW4lmLZ2Xflo13umz8ZFczKRQddjGFX5lOzYD8DtcQjcq44owZ0nnbk+tvf+5LRxGV2rv2SuqgF+9tjxw5kkWLFpGdnb1nV6mUUkop9QXofL4rLQ2efFJi3084AVaskNG++npwXYeXB0+l//mT+LkzC2feP+WgleNgPU9G/2xbl8tagzEWgyXN1nPMqqcZufVT/n3GnawaP6X1GhobpUDKyOjZtRcXw5w5eu5LqQNBj8MwSkpK9sZ1KKWUUkrtM4lzXAmJyPjqaukwRaNybmvzZul0XfjTfJxxcyE4XhZ5hULYmCddLRwcY2X1lgFjLRg5CO+zIbJrlnPBgmv5d+NGFk+YiofD+vWShpiX1/1r9jw6nPtKRNQnzn0tWybnvsaN0zFCpfYHPS60Zs6cudPHb7vttt2+GKWUUkqpL0LnyPjESN6pp7ZfKuzATTfBhg3wwgvYqo14sQjWcbE2hjUOxkYlBt7KaS4Aay1pTZs465Vp5K55j6eGTid7dD4XXbTjgqirM1hlZXJt7feAJfTGuS+lVO/qcaE1f/78Dt9HIhFKSkrw+XyMHj1aCy2llFJKHZB2FhnfynHguuugpYXYiwvxqjYScZMJRBpwbCT+JFlwbJC0wlj8Pn+kkSNXzWNYXRGxi28jL9j1ufYdncHKz+96D1hCWpqcMauv763fiFJqT/S40CosLNzuvrq6Oq666iouvvjiXrkopZRSSqkvQueRwi4Fg3DDDfgCyTT/5R8ktTRgrBdfaAxgcQCLBzi4yGOO6xAIwKDmNTgP3QqnDN8uCWNnZ7CKiiAU2n4PWEJDg4w8VlRoEqFS+4Nupw7uyueff87kyZMpLS3tjbfbb2jqoFJKKaW65HlU/PejxGbNpm9dKck0t+7ZcolhAYyDsfH7XOQsV3KyVEQFBTB3bmux5Xlwzz1SVLU/gwWy96uoCOrqZB/X8OHyNllZ8rxNm+Ctt8DvhzFjICVFkwiV2hv2Surgrmzbto3a2treejullFJKqf2b4zBkxlRe7zOJbbfO4sy6eaTRGF9s7OAgARnWGBwbkz9jsOGIFFqLl8Dll+Pccw9MmbLLM1hpaVJstbTA559DejoMHCjdrc8/l2Ls1FOlk7Wvkwh1r5dS2+txoTVnzpwO31trqays5G9/+xvnnHNOr12YUkoppdSBYPQF+dz24VxqS07ia5/OIDW8FWMNWNmpJbdgLMQ8WZnjWov1YtgVa4hdfS3+ezZSf8JUWlqcLs9gVVfD0qUyHnj00TJauHGjjBpGo9C3L5x5JgwYIM/fl0mEutdLqa71uNCaPXt2h+8dxyEnJ4fvfve73HLLLb12YUoppZRSB4K8PBh/uMP/ha4hMCyXM/49g35bVuOLtuA5DsaLyUhhvKPl2pic4jJgPYu7ZRPRG6Yx/Nz3GLh1OqWl+fTv3zYWaK0UMfX1MhK4ebN0rFy37fv0dMjJ6Xhd+yKJUPd6KbVjukdLKaWUUmoPOI50b8rL4bmaKZSfPZqvLJ7F4SvmkxRugtaIDNotNY7fOg54BtPUQOCFZ/ixs4iHUm7ktaFXkz3AYfx4OXdVXQ3hsHSvfD4pwvr0kV1f1dWwbh2UlMCoUR2vbW8mEepeL6V2bo/+a19eXs769et761qUUkoppQ5IiT1cxxwDy8hn1ri5PDHhPrZlj8TxuVJkYbHWkYLEAsZgrIRmeBicSIgR4VXcVjeNW9dNJWlVEe+/D2vWSNcqEpGxvJwcCASkeElJgdRUKcJWrJDuV3uNjfKajIze/5l7stdLqUNRjzta0WiUO+64gzlz5tDQ0ABAeno6P/nJT/j1r3+N3+/v9YtUSimllNrfddzD5ZCRMZX+tSfQ/M3v4S/+DIuHZ1wJxjASAG86VEYWDKR59ZzX8AxHhRbxYN2NvNJ8NVlZDqGQnMFqX9QEAvLV0iLdrdpa6XSBFF3r18OECTLe2Nvq63Wvl1I70+NC68c//jHz58/n3nvv5cQTTwTg/fff5/bbb6empoYHH3yw1y9SKaWUUupAsP0ergK23fcI9rLLGdC4lqgvBV+sGdeLtrafEvu2AKxxcDAETJgR0dXc3nwzp4U/470jruPvnwTxdfE3t6QkOaPV0CCdr7Q0qKyUIisnB6ZM2TujexkZ0i3b0V6vvdlNU+pA0ONC68knn+R///d/Offcc1vvO/LII8nLy+Mb3/iGFlpKKaWUOqTsKtp80FkFPHPhPZzxzx+SFanB8WJgLR4ODjHkDJclhouJySihtYZIUipJsSZOqFrAmMEtLPHdwOqqIP37S3EVDsterb594Ygj5BoqK2HxYily0tJktPD55+V6ejuUIi9P0gV3tPdrb3bTlDoQ9LjQSk5OZkQXsTUjRowgKSmpN65JKaWUUuqA0J1oc8eBI385hWdq4Ky3ZzCoaTVJtBDFwcXDwRLDkURCong4YC2+SDOOFyXLq6JvyUJ+np3C9ZHf09TkUF8voRiDBsm4YnU1fOlLEkzhOHJuatAgKbjeeUeKr2uvhTPO6L3uVvsQkGXL5ExWInVw/XrIzoaLLtIgDHXoMtZ2Pja5czNnzmT58uU8+uijBAIBAEKhEFOnTmXs2LH8+te/3isX+kXpyfZnpZRSSh06dhRtXl4uRUbnaPN//Qv+9osiLl47i7Ob55NCMwYPgyWCjwCR+LJj2b9lcXDjHS832U/MF+C5w2/h0X430S/bIStLiq3166XD5boyOnjccVLcVFdLAVhdDVu2yBjhpZfC177Wu92trorNYFCKLI12VwebntQGPS60Lr74Yl5//XUCgQBHHXUUAEuWLCEcDvOVr3ylw3OfffbZHl76/kcLLaWUUurQsKsRwM7PveeeHY/NLVsmY3M33yzvkXj+p59CZrrHgJce5Xu1sxkULiWZFiK4+IiSeBuLg4OHh4PPBcd1IBIhmpLG0sMu5elh01mTnE8oJMWN58HKlZJAOHiwhGasWgVNTRIFH4vJzzVyJAwf3vv7rXryu1PqQNaT2qDHo4N9+vThkksu6XDfsGHDevo2SimllFL7je6MALbXk2jzESPanp+XB83NDgtyplIyYBJTVs3i/LB0tywu4GEBF08i3/HAxrcWG4OvuZGjVz3D4U2LKDrzRmZvvRq/3yE1VRL+MjJgwwb5rORkuQ5jpBBqbJTPr6np/f1W24eAKKV6XGg9+uije+M6lFJKKaW+EDsaASwslDHArro/PY02b/98z5ORv+Xk8++MuRR6pzC1bjZDYyUk04KB+PktKzu2rIFoDMfngudhQiGSSlcR/Ms0rsp9h2XnTWdNIB+/XwqezEwpttoXUeGwfGai+GpfBCql9o4e/zvGGWecwbZt27a7v66ujjPOOKM3rkkppZRSap/wPOlk1dTICGBmppx1ysyU7xPdH8/r+Lr20eZd6Rxt3v75WVlyhquxEYzr8L+pU/lhv6dY6DuPCBIsZuPhGBYD1sOzBs+z0tmyFmvBH2rgpMp5XPTC1Ry38V9kZ8serVhMfoZQSL6slXTC7Gz57LQ0Kfp0v5VSe1ePC60333yTcDi83f0tLS28/fbbvXJRSimllFL7Qk9GANtLRJuXl7euw2qViDYPBtuizds/H+TPWVny3KYmWBLNZ2ryk/yfO5kWUokQwGJwsFjj4hkXG4vhWfAsWMeVOHgsfbaW8OX/zOS0AUWkpsK2bdLNikTkvaur5ezW+PHyM+1sv5XnQWkpLF0qt50LTKVU93V7dPCzzz5r/fOyZcuoqqpq/T4Wi/Hyyy8zZMiQ3r06pZRSSqm9qKcjgO2dcILEpi9aBIcdJkuDdxRt3lUU+vHHSyGzerV0oRzHxx/73c5h9esYHC7BWEvARPBw8XnNWABriVkXE4lJImGkBYwhp6aYi5bcwaYvP0HRCh+1tdLNCodhyBApsnJydr7fKnFOrbhYUgodR1531VWQn99bv3GlDh3dLrSOPvpojDEYY7ocEUxJSeEPf/hDr16cUkoppdTe1H6kr6sAsa66P+2DM+rqYONGWRScmytpfxMmtEWbd07j+/GPZYFwInTjiCPk+Rs3ypmwqsx8Hkq9jR9uvoMRoRU4kShePO7dwUoaobHgxWTfFuBGQzhelHGrFvADruD5Y26ntjafhgY5gzV+vBSBtbU73m+VOKe2di00NLQVoJ99Bm+9BTNmwOTJe/M/CaUOPt0utEpKSrDWMmrUKD766CNycnJaH0tKSmLAgAG4rrtXLrK9Bx54gN/97ndUVlaSn5/P/fffz6mnntrlc998801OP/307e4vLi5m/Pjxe/tSlVJKKbWfS4z07SimvXP3p3NwxvDhUpisWCHFzHe+07YUeEdJhhdeCN/8Zsco9NJSmD5dOmi+/pN5IzSSE9+bxdii+QRizdh42LsxBoMXL7wkmdB6MrvoRkOMXP4SX1tVTG2fG/nkiKvxPIeSEggE5PPbF4EJiXNqa9fKHq5EJHyfPtIRW78e7rxTouG1s6VU93W70Bo+fDgA3hc4rPvUU09xww038MADD3DyySfz0EMPce6557Js2TLyOve/21mxYkWHnPv2RaJSSimlDl1djfQlUgc7d386B2ckirKsLBkDXLZMxgjPOKN7SYYFBW3XMWKELBouLJT3qzH5/P20uWRsPoUrN89mUEspAZox1sPiYIwEYhgMDjFiOIDFZ8OMiq7i17XTWLrxHf53yHSq0/O59FIZHczIgJSUtrNXiTNqixZJwdjUJM9L/GzJyTJ6WFUFf/2r7ALT/VhKdU+PFxY//vjjO338O9/5zh5d0M5MnDiRCRMm8D//8z+t9wWDQS666CLuvvvu7Z6f6Ght3bqVPn36dOszQqEQoVCo9fu6ujqGDRumC4uVUkqpg1hX3adgsGP3p7QUbrtNiq+u/kpQWysdodtvh//93+4vM25/DYnibOhQKcxefRWGbCviusZZXBj9J2k0EsWP2zpQKP+/weIl1h0bB8fGiPmS2dx3DA8OvpMPc6cwYICc20pOhn795KlbtsjY4uefyzmx3Fzo27fjz+V5UmiNGwd/+INGwqtD215dWPzTn/60w/eRSISmpiaSkpJITU3da4VWOBzmk08+4Re/+EWH+8866yzee++9nb72mGOOoaWlhcMPP5xbb721y3HChLvvvps77rijV65ZKaWUUgeGYFAKifbnqfLyOhZC3Q3OWLmyZ8uM21/D9de3FXzr1kkhVO/P577gXOrrxvPt8rvxeSEMHg6WWHzfFoABKbis/NmNhcnevIL/2vojyivAmTKldXnyggVS9J16qgR5rFoFmzbJ9SUldfwZw2EZPfQ8jYRXqid63PzdunVrh6+GhgZWrFjBKaecwpNPPrk3rhGAmpoaYrEYubm5He7Pzc3tkIDY3qBBg3j44YeZN28ezz77LOPGjeMrX/kKb7311g4/55ZbbqG2trb1qzyRw6qUUkqpg5rjSOFTUCC3nUfkurs7C3ZdkO1oj1UwCL/4hXTFjjoKBg2SLlNWX4dnht/EKwO/y9akgYTxx0stF4uJR73LPKBJnN3CoZlk+kSr+WXdLxhevxTHkZHIQEC+1q+XDtbAgbJ7q6VFOmqJeafEDq7MTOmCdRUJr5TqWo87Wl0ZO3Ys99xzD9/+9rdZvnx5b7zlDplO/zRkrd3uvoRx48Yxbty41u9PPPFEysvLmTVrFl/60pe6fE0gECAQCPTeBSullFLqoNDd4IzDDut5kmF7jiNf27bBpElQVCS7sDIzHV4YdB20tHDsloUMYCMxfzIpsQZcLxJ/tVyUxcFgSfJCuMQYFl7Dd96cypNJc6mpKWjd4VVTI4XU0UdLB622Vr5vbpZrqKuTM13p6fIz7+RIvFKqk147zui6Lhs2bOitt9tOdnY2rutu173atGnTdl2unZk0aRKrVq3q7ctTSiml1EEuEZyRnS3nrGprIRqV22XL2oIzEpHq3V1m3JX6eil2MjNhzBhJAGxqgk+bgzyScQOvBC4gbFJIiTXGAzJkcFBIQAbWwyGGg4dLlMGbl3DVgkuZXD2XJJ9HUpJcfygksfRf+pJ8XnOznNtqbJRuV3Y2jBq1fSS8UmrnetzReuGFFzp8b62lsrKSP/7xj5x88sm9dmGdJSUlceyxx7Jw4UIuvvji1vsXLlzIhRde2O33KSwsZNCgQXvjEpVSSil1kOt8jqqiouvY9O4mGe7Ixo2yyPjzz+V5ritdpbw8SE0N8of3ZlO+7Uh+FJlN//pS/BFZaByL79sSBhfZt+UAxvPIri/h5/XTmBR5hycGTafWl09SknTP+vSBiRMlqn7QoLbQjMMP3z4SXim1az0utC666KIO3xtjyMnJ4YwzzuC+++7rrevq0s9+9jOuvPJKjjvuOE488UQefvhhysrKuO666wA5X1VRUdGajHj//fczYsQI8vPzCYfD/P3vf2fevHnMmzdvr16nUkoppQ5e3QnO6G5B1pXiYvjnPyESkSTA7GzpPG3bJsEUxx8Pjs/hheypDDp1Eie/N4vDl8/DH24EHCweFnDjZ7Wc+NktjAHjkubVc3rNM4zdtoh/5N7IGx9dTX2jQzQqxeC4cbLTa9Cgrn82pVT39LjQ+iL3aF1++eVs3ryZmTNnUllZyRFHHMFLL73UuuOrsrKSsrKy1ueHw2GmT59ORUUFKSkp5Ofn8+KLL3Leeed9UT+CUkoppQ4CieCMnelOQdZZYlfX5s0yyvfRR/LnzEzo319i1t9+WxYHGwNv1eRTcvpcTh5yEl99cwYpLVsx1uIai3EcjLV4ngwVWgwxx8XnRfETYkR0FT+rmMbYje8wN2s65Zn5ZGXJ+86fL4WiRrkrtft6vEcroaamBmMM/fv37+1r2q/0JCtfKaWUUmpPdN7VVV0tHbGaGulqeZ50xn73O1kk3H7318TqF/jO57+g/7Y1ODYGPh9Eo3jWYq0hQhIuUYyN4cWTCn3EaCKVlc54/pB1G9FzJjN27I53fSl1qNtre7S2bdvGr371K5566im2bt0KQN++ffnGN77BXXfd1e2lwEoppZRSanudd3Xl5EjRVVsroRWuK+e3cnO76phNoX/tSJzvf08Od/l84Hk41mL9fpJiHjYcBQyugYh18ZBhw9GmhGlNM/l/a0bC2Pwd7voCKfZ60qVT6lDV7UJry5YtnHjiiVRUVPCtb32LYDCItZbi4mIee+wxXn/9dd577z36dl4nrpRSSimluqX9rq7EP5YbI0EVIAVXSkpbNPz2I4wF8MgjcOWVkrxhDITDsgonEo7HZFhiVrpZLjECtIBnGB0u5rLiO3h14hOkZfqoqNh+11dxcccuWnKyJCxefHH3wjK0SFOHkm4XWjNnziQpKYk1a9ZsF6c+c+ZMzjrrLGbOnMns2bN7/SKVUkoppQ4F3d3VtdN9VgUFcNddcMcdEiEYjeLFLJ41GCwxHMC2JhIC+G0Ilyhfql9A1vwrePO020lOzu+w66u4GObMkTHGYcPakhQLCyVh8frrdx3ysSdFmlIHmm7/G8Jzzz3HrFmzutxZNXDgQO69917mz5/fqxenlFJKKXUo6e6url12gSZPhsceg0suIZqSTjQmS4y9+L4tJ55IaPCQmAzpdfkJceT6l/ja05dzTuVc8oZKCFoipKOmRgrAzEwZY8zMlO9rauC55+R5XUkUaYWF8jOMGye3hYVyf3Fx7/z+lNqfdLvQqqysJD8/f4ePH3HEEdstE1ZKKaWUUj2TiIY/5hhJHFy5Um4nTNh116iD/Hy8P8/l5TPvozJ9LDF/AAPxIsuhrVnWtm/L4uAjzIjwKqa8OY3ar0+FoiLKyqQTNWxYxy4byPftz3R1tqdFmlIHqm6PDmZnZ1NaWsrQoUO7fLykpOSgTyBUSimllNoXdicavitl6x2eTp9K0cWTOHPxLMYv+yeBaGN8ibFUNk67fVs2vvDYOpAcbcD30jy8iiL4zm00N08mEpEwjkCA1ih4kDHCrs50AT0q0jROXh1Mul1onXPOOfzqV79i4cKFJCUldXgsFAoxY8YMzjnnnF6/QKWUUkqpQ1F3dnXtSiLFsGFcPgtGzKVmwHhOeetuTDgEeBgsURxcrAwVGtm5FTUunkHOdq0uoc8fZuKZkbz2eT6OI4GG2dlyxionR85qJSfT4UxXQm2tdOSSk6VrlZXVdn8iSbG5uesiTakDWbcLrTvuuIPjjjuOsWPH8qMf/Yjx48cDsGzZMh544AFCoRB/+9vf9tqFKqWUUkqp7vM8KWaam2HDBhg61OH9k24io34Dhy1/gdT6jbheBIuLIQbGwRDDw8WxMRwbw8Ra8JoNSWuLmZpyBz8b8AT9BvmIRqGyUt7/hBNk31dXIR3FxfD3v8vtqlWSmJiSIo81N3fcDbZxo+R4KHWw6HahNXToUN5//31++MMfcsstt5DYc2yM4cwzz+SPf/wjw4YN22sXqpRSSimluieR8FdcLEuQP/sMRo+GYNDhk2OvwxdpYcSqhaTWbyTsJOPSgOtFsRiMY+NLjeNziqEQSTbKVxoXMKfmCv7g3U71gHz695di67XX4Kij4Pjjt7+GOXOkCBs0CLZskftXrJDbvDzo1w+qqiASgX/+U5YwawKhOlgYm6iYemDr1q2sWrUKgDFjxtCvX79ev7D9RU+2PyullFJKfdE6x7A3N8O770qh068fnHQSDG8u5oh3H+SU0n+QGmvAMTGM9eJdLQ9rDWCxGDwcXGKAJWRSqPCP5NE+N/Jk4Goamx1iMelEDR/eFtc+bhzcc09bTH1NDXzwgcTAR6MyopiaKqOGqakdu2I336y7tdT+qye1Qbc7Wu317duXE044YbcuTimllFJK7R2dE/6MkXS/U0+VAmzNGim6Go8OwtWzGbbxSLL/Npt+daUEbDPYRCJh4t/hTbt9WxafDZMXXsXNm6ZxuO8dHkiZzoa++YwbB+npbTu1LrmkYwBGTo5cT0WFFFHWQkODdLWOPloeDwQ0FEMdXHar0FJKKaWUUvufHSX85eRIeMWIEdI5+sEP4KSTHBxnKmu+OomVt8/isKUdEwkt4OIR67B3y+IBaTQwJTqPYGMRs+xttLRMZvBgKaaWLYMFC6STlpbWdg3p6dJRS0+HWAzq6qQTlpMjj+8suVCpA5E2ZpVSSimlDhKJlMH2BU6CMTB4sARPNDRAUZGc3xo5OZ8jP5pL889uxaZn4ibJXw9dLBYn3ssSDhDDJYoLWIZ7JdzUPBPfiiKsbYtrLy+XYqqxse3zAwHw+yVl0O+XkcHk5LbHd5ZcqNSBSDtaSimllFIHiYwMKVYaG2VksLPycli9Gh5+WCLak5MT56ocgnffBC0bCM17Aa9yI34bIYbbGozhI0YUFx8xXGI4tGAwHOYVc1nxHbw68Qky+/lIS5NiavBg+bzECGNWlnTVKitldHDw4Laod2th/fqukwuVOlBpR0sppZRS6iCRlyeFU3m5FC/tbdoE77wjCX/Dh0tgRXa2nKuaMweKVzhw3XWEvnQW9SkDiTpJNJHWWmR58WXGPqJ4yFxiEiFSbBNfrl/AN1+8guxNRTQ2SoT7BRfI+y9bJjHwsZh0u0Ih+RoyRO6rrZXnZGfDRRdpEIY6eOh/lZVSSimlDhKOI6l/7QucaBS2bYO33pLi60tfkk6S60rXK5EK+Nxz4I0Lkv6rG1hbcAFNNoUU2xg/nwW23VktJ77s2IkPFfoJccS6l7j0mcsZ+9ZcguM8zjgDrr8ejjlGFhavXCmfP3kynHYabN0qRV5NjXSyrr9eo93VwWW34t0PJRrvrpRSSqkDTWKP1vLlcmYrGoWSEjjySBg5cvvn19ZKMTRzpgRmvPqyx2vffpSrt81mWKyUAM0YIBZPJDSt57fapRQaB4tDKCmd8Ne/Rd9fXEdZWpDaWnn/hgYZD1y5UnZnbd0qheH48XDVVZCfv49/SUrthr0e766UUkoppfZfwaCMBpaVSUBGRQXMnbvj80/tE/+Ki+FfLzn8M2kq76ZP4geNs7gw+k/SaIzv1JJEwvbdLYuDZx2igVRSTRNJ/15A4fIWnhl8A4tDQTZulIKvoUGKq6FDJXEwNVUCOf74R+1oqYOPjg4qpZRSSh2EHEe6UwUF0jVKSemYAtheIvFv40Y5r7V6NfTpA9Fx+dyfP5f/6XsrjSaTxF8dHSwxHFwDjmNw/C5+n0eqacFEIniVGxm89BW+svh31FRFaWiQM2L19TK2uHUrfPyxnNXqMLro7aNfjlL7gBZaSimllFIHuZ2FZCQS/8aNgw8/lKLnuONg0CApjDKyHF4+4iaey/ouWwID8Vw/FgccF8fn4CT5cb0oDhbPg2gMXC9Mn1AVJ5Y9xa3FV3C4LcLnk+Kvvl52ZzU1yWgjSIcrsaxYqYOFFlpKKaWUUge5HYVktE/8mzgRVqyQZceJs1OpqbLguCXs8OLQ63g7cBZb/AOJuUk46WkYx5EYQyDm+rHhMMSjMwACtPDV8IvcVfJNTq9/gZQUKbBCIQniqKmRa0hLk9HC9suKPU/GCpculVvtdqkDjZ7RUkoppZQ6BASDcg4qEZJRUSHjghMmSKx6NNpx2XFOjhRfy5dLQfR5LMjmtBvIHZHMqev+ga+lQaofY7Cuiw1HQHpdrWe4DJBEmLHecu5vvpZ7fRv5e9JUolGH1FQprEIh2bPVfllx5zCPtn1feo5LHTi00FJKKaWUOkR0DsnIyJCxQseRrlHnZcc5OdLtSqQSNjYGGX7vbHz/ORJmz5Yow1AIa22nmUSDS0wWHiOtqGyqmVH/c8YFPuffkesoDQfx+SApqeOy4uJiOSdWUyPdtbQ0uabCQhl91NAMdaDQ0UGllFJKqUNI+5CMESPaFgTv6ByXMRJg0dICxx8PI0Y5MHUqPPUUnHceBAJYTLx/JbcGL753SzpcUVwi+AjQzFnhBVy24X76bSwmPV06a9nZMGWKFHsPPgjr1kkxlZm5g31fOkaoDgDa0VJKKaWUUq3nuMrL5dzW0KFt3aT166UYuuiitsKM/Hx48km49lpir75OrKoGJxbGxSOKi3S1osRw8RHDJYYFBtiNHL/lFcItEf43/2FGjfKRlwePPSZph59+KmfDQiEp/HJy5OOMkWtatgzee0+Kv/YdOaX2N7qweBd0YbFSSimlDiVdnY8KBqXI6nJkr7gYb/b9bHtmIWm1lTjGI4YPv9dCLN7lSiw2BjCAdVxwXYpGnMdfBt/Oy+X5RKPQt6+MNGZny21qqpwTSxRbVVXw7rvSiUtJ0bNbat/ThcVKKaWUUmq37OwcV3ueF39ONEi/y24gqSmZyDOPkxKuAywWE/9/Mkboc+LzfsaAA16khfFrXuSn61fi9b2TwmFTqKmBbdsgPV2Kq+pqKfiys2Vs8L33oK4OBgyAwYP17Jbav2mhpZRSSimlOkic49qR7bteQcaPn8151x7B6EdnkNS0FcdKJ8vi4Dq2NRjAArGYnOjye2GGNy/nzsi1zE3byIsDp7Jli0N5OQQCbRHw27bJZ27ZAocdJiOExrSd3Vq2TM5ujRunY4Rq/6H/VVRKKaWUUt2WSAUsLJRO07hxclu4xOGh6DVsvudhGDcek+zHccBNcnBM2+stBuPF8HBb7+sTrea6NT/nu5/dyJhIMfX1UFQk4Rj19dI5W7MG+vWTrpVp936Js1u68Fjtb7TQUkoppZRS3eJ50smqqZFOUlepgE/WT8H/9BP4Jp+PSU7GGCPVUOLL8ySZ0FiMkURCOdPVzOn1C/hB8/0EKSYWk/dbvx7WrpXPOOmktvNa7bVfeKyLjtX+QkcHlVJKKaVUt5SVybjgsGEdu0rQqbN0RT4j4omEvP66VEzhsFQ9rouNGRwbxTOSSOjEYzOy2cgZsVdwTIRb+z1MfbOPWExeOmaMhGN0pbFRgjE2boQFC3TRsdo/aEdLKaWUUkp1S329FDBpaV0/3r6zhM8HN90E55wDubnyvc+H8ftxbbS1q+VD/mwxBAiTSxWX2qf407YrOCapiNGjpZCqrZVCr3NetrXS9erfH/75zy5GGgtl1LG4eK//epTqQAstpZRSSinVLRkZ0iVqbOz68URnKSMjfkcwCDfcABdcIA9Eo5hoBONI7HtijNDFw+AhURmQTAtnhl/k4cZvcl70BZKTJeQiOVmCL2prIRqV22XLpMiyFjZv3vFIoy46VvuaFlpKKaWUUqpb8vJkFK+8fMedpWBQntcqGITZs+Hee+WAlTE4TiLu3ZG9WhDfuEW86AI/YUa0LGf6qmu5rOHPOHgcf7ykIdbUwMqVUlhNmACXXiqJhLscadSwDLUP6RktpZRSSinVLY4j553Ky6WTNHSojAs2NkqRlZ0ti42hLTFQ9nA5ONdcIyOEM2bAqlWYUAg3ycFGY3heotgyOCQSCaX91CdazbSqnzOq4XM+stfRmBdk4EA45RQ46igp6oqKdj3SWFERH2lUah/RQksppZRSSnVbMCjLgRN7tCoqZKRvwoS2Iuuee3YQSDFlCoweDbffDi+9hInF4mOEEPXAIZ5IiMUAIVwsELDNnBVawBF1Lbzh3MAnpUHq66WAqq+XEcJAQAq+zMztr3m7kcZdaF3GvJOFzUrtihZaSimllFKqR4JBCZvoXIysWCHBEzU1MsaX6HYVFkoX7PrrIZifD50SCZ1wGJ/1iFgXMDhE8XBxicW/IDu2kdS1r+ASYf3Eh3nzHR8ffihphInEwU2bYNKkjuODiZHGCRM6jTTuwPbLmDW5UO0eLbSUUkoppVSPOY6cl0rovGMrUewkAimWLZNAinHjwEkkEvr9sHAhVFbiOB4+44NQS2tXy0cMDwcH8Hlh0huryP/8KaaU1LMm83ZK0/IZOFDeZtMmKag++EA+Ly0NGhrkLFd6Ohx//K5/psQy5p0WilpsqW7SJqhSSimllNpj3d6xlQik6CKR0PUiOK7BdQ2u8bDILa2JhAZftIWTa1/iDzWX87XaucQiHpmZ0skaMgSam2H1anj7bXjzTaishLo6ePxxGWncUcx7d5Yxa3Kh6gkttJRSSiml1B7r0Y6thC4SCY0xONbDOPFEQitphMRPbnkY/EQY1rya6Rtv5rIPbiS7upiaGuk+lZdDVZUUW9u2wciRcNxxu96pVVoKH38sNV9tbcdURU0uVLtDRweVUkoppdQea79jq0eBFI4DnRIJCYXAdTCxmGzWMgbP8eN6YTwMMRyaSSXZNlFQugA33MKb5ga2eEE8DzZskILOWvjPf+T7Y4/tYoQx3nIoLob/+R/45BMpCP1+KczGj5f6DzS5UPWcdrSUUkoppdQe260dW8goXmkpLB05hYrfPYF33vmQnIwxBuMawCFifTheBIAoLg4eKTTjtxHSG6sYtWYhkzc8RJLPo7ZWRgXT0qBvX/nstWvhww9l/K9zZypxLmv1anlNRgakpsrI4YcfQnW1PK+nyYVKaUdLKaWUUkr1ihNOgMWLYdEiOOwwCaHovGOrfUz69gl/+Yw/8kn+y7uWvp+8jlNTA+EIjgXrGWL48BPBAq5BRgyjYfpRycWRx1jbPIg5STcRCDgkJcnIX2oqRCIyDrh8uZzlSnSm2p/LOu44aaRVVkoXKydHiqzly6F//54lFyoFWmgppZRSSqk91L5gqquTqPXKSpkGHDCgbcdW+8S+HSb8LfUxJ+kmfjLRT59FC2HjRow/Gae5ETcWASweDlhJJBQeabaOaaG7GBNbzpzIdMrdfFJSJNAiFIKUFPmsqqq2zlT7AA/HkY5cba0UWJmZUihu2CBnt4YP375QVGpn9L8qSimllFJqtyUKpsJC6VodfzycdhoMGiTFyne+Azff3LHI2lXC3+exIA+l3MCigRfQEEvBq28kFvEk8c8xOHiAweBhsZh4ImEqjVwcfYZHWy7nws1zCbd4xGJSHCUnS2ervLxthLFzgEdODkycKNfe3Czx8E1NMHasRrurntNCSymllFJK7ZYdFUxZWVJwWStjhJ3tKgo+NRUefT/IzD6zefFLv2Vr/zF4Pj9gsTELjoPrxp+PwUeMGC4WhyTCjGU1v4ndzC9rbmRofTEpKVJkNTZKhy3RmWof4JGQkwOnnAKnny4BGsceCz/9qRZZque00FJKKaWUUrulx7uz4nYWBW+tPL+5GYYNd1hx8lTmff0pVo09j5iThAWinhPvY7V1txwsDuBhaCSVZJo4N7aAa5vv57BYMVu2yGjgLbdI0eR58tW3r/wM7fdjGSPFYkuLFIztFzMr1V16RksppZRSSu2W7uzO6ioSfWdR8LW1co4qPV2eA7DM5PNc7pNMX38FJ9e/TMwaUmItOHh4uBjH4HhRYjg4WFJoxiVKLlWcbRaSaVJ47uTfc8uvHPLz286UJYrA0lJJHZw0SUYKdxbgoVR3aaGllFJKKaV2y+7uzkpEwRcWyshh+25YYqnx2LHSVaqulpj1piYfj428ncGr1jGwuQSDxU+EGC4pXgsWg2s8rJU3c/EwhOkfqeS8TY9xysBBDAzeRFGRw913S+R7OCxBGcbApk3w0ktyXaNGdR3goVRPaH2ulFJKKaV2y+7uznIcuPhi6RgtWyZdrGhUbsvK5IxW4jXLl0sgRU4OrE7K556k2yg1I8AYfERxieIBFivXYAw+I4mEjjEk+TxSwnUM/PNdbLlkKrO/X8Qbb0g3a9Uq2LYN+vVrGw+sroZvfnP7AA+lekoLLaWUUkoptVt2VjAtW7bz0btgUJL8jjkGNm+GlSvl9uST4bzzpLjatk2CNrKy5DU1NbDAm8wtAx/j7ZxLaHLSMYCHi8XgYTDWw7MGFw/HWBwsxhi8piYCC+bxw0VXc1rjv3BdGU9saJCCq7xczmmtXw+/+Y0UYkrtCWNt539/UO3V1dWRlZVFbW0tmV31xJVSSimlDnHbLx6WQqo7o3eeJ12s+noZMczLgxUrJDJ+zRo5PzVggIwhrl0r752XB+mpHmdteJSLS2eTFyvBb1sAsDi4xmKMxXEcrOfhGR+RGIQIEMbPeobxg5S/UZJWQEuLjBAGAlLQNTTIZ3zlK/CrX2lXS3XUk9pAC61d0EJLKaWUUmrXuiqY9iREorgYHnkE5s0DXzxVYNs2SThMnPlqaYFBW4qY4d1Ofsm/cL0w+HxyPstaPM/iWYcwPlxigCVEMj6ifE4B17pzWe4rwO+X68/IkNvMTDmndfrpMkKoYRgqoSe1gf7XRimllFJK7THHkXNOBQVyu6fFSTAId98Nl1wi73fyyTBwICQlyePWQl0d1A3LZ8EVT7Js1GQibiomEMAYg7WWKD5CJOEngiGGweIjgkuEApbwj9jlnBt9AceR9wuFpLOVmioFXVfR9Ep1lxZaSimllFJqv+Tzwfe+B6NHt3XKtm2THVvV1VIQjR8PnuPj8ZG3s2VgEJMSwAYCRI2fCH78hOMbt4SLh4PFJcZo1vBA7FquaPozXtTD75dCLidHirpEAmJnnicjjUuXym37HVxKJWi8u1JKKaWU2m8lQjPmz5eY96oq+Ro6VLpnSUnx4I3R+cQuvg3nkTuIFa/AxKL44muN2xhcYvFSCyyQTTW/if6cAv/nPB+4ji19g4wfL2EcXUXTd3Uebfx4CQXR81yqPS20lFJKKaXUfi0YhHHjZIxvyRJ45x0ptrZskUInsfMqLzgZvjSSbb+cReD/5hOINuFZg4MUVbLg2MRvHSL4AUimmQvsAvJiLRSOvYFYdpBly+R920fTFxdLSEdNjYwWpqVJSEdhoaQWXn+9FluqjRZaSimllFJqv5c4AzZiBFxwQVvwRlqaPN7YKGN8ecF86mfP5dlNp3DB8nvpu20dligOFg8Hg8ViiOAniUg8JAMG2I2c1PgKOUsizA4/TP8cH8cfD0VF0tUaOlQ6WTU1HZcsZ2bK98uWwXPPSUGo4RkKNHVwlzR1UCmllFJq/7SjMb4LL4Tnn4eafy/lx4u/x5CazzB4RHHx4RHBxUdMiiwjVZG14BkXz3FZPvI8Xp50O8Umv/V9c3Nl19fo0VJcdVZbK3vAZs5sW36sDj4HdergAw88wMiRI0lOTubYY4/l7bff3unz//Of/3DssceSnJzMqFGjePDBB/fRlSqllFJKqb0lMcZXWCiLkceNk9vCQvjjH+HII6F5TAF/PPoRNqaPJoZLMylEcHHxcIlh4wuOHePhuha/3xDwWgiWvMQVz1/OuZVzGX+YR//+8MEH8OmnsHGjFGWdpaXtODxDHZoOqELrqaee4oYbbuBXv/oVhYWFnHrqqZx77rmU7SB3s6SkhPPOO49TTz2VwsJCfvnLX3L99dczb968fXzlSimllFKqt3hexzG+zExw3bYxvpoaSQT88Y+h35cLmJV9D1ucbFJpxk8UP1HAwTFydst6YIzBOAYPgxuLMKRpNVPevZnTFtzItveLqa6WjtWrr8Lbb0vqYXuNjV2HZ6hD1wFVaP3+979n6tSpfP/73ycYDHL//fczbNgw/ud//qfL5z/44IPk5eVx//33EwwG+f73v8/3vvc9Zs2atY+vXCmllFJK9ZayMhkXHDas7axUgjFynqq4WLpM3/wmrD9mCv976gNs7j8O65MADM9xSAQSWgzW9WMjYcBgjUPYFj0WOQAAPK5JREFUl4obbuKw5Qs4e/n9HB0opl8/2bVVViYJiIliy1pYv16CMNqHZ6hD2wFTaIXDYT755BPOOuusDvefddZZvPfee12+5v3339/u+WeffTYff/wxkUiky9eEQiHq6uo6fCmllFJKqf1Hfb2M6SWCMDprP8bX2Cj7uLadOoXnL3uCVWPPJ+pPBmOwxoBxiBofRCNgIWZdHDx84WacWIS+4SpOaV7I16ofYlCuh88nHbXaWgnA2LYtHi+fLcmHGoShEg6Y/yrU1NQQi8XIzc3tcH9ubi5VVVVdvqaqqqrL50ejUWpqarp8zd13301WVlbr17Bhw3rnB1BKKaWUUr0iI0PG9Bobu368/Rhf++dWD8hn3qVPUpR/OY1pucTcJMDIMmNj8Fw/DlE864GNYfDwE6Z/uJKzNz7Gdzb9jgHZHgMHSvG2dq10tyZM0Gh3tb0DptBKMJ36w9ba7e7b1fO7uj/hlltuoba2tvWrvLx8D69YKaWUUkr1prw8SRcsL98+mKLzGF/n51rHx3sn3cTqMefQkJpL1PiJBdIkfTASiS84NvGwDAeLATzSvTquWn8Xv6+fyjeOLOIrX5H3vfZauPnm7Yssz5O4+aVL5dbz9s3vRu0/Dpg9WtnZ2biuu133atOmTdt1rRIGDhzY5fN9Ph/9+/fv8jWBQIBAINA7F62UUkoppXqd48DFF0vxtGyZnMlKLA9ev377Mb7Oz432DfLaETdQUJvMl0P/INVrIBaVSsi0LjQ2GLz4PbJ7K4UmTt88j4IFRfzfcbdRlzuZ8eO3HxfsHDsfCMDAgXDKKXDUUVL86Yjhwe+A+Y84KSmJY489loULF3a4f+HChZx00kldvubEE0/c7vmvvvoqxx13HH6/f69dq1JKKaWU2ruCQRnXO+YYSQNcuVJuuxrj6+q5xTbI0qtnU/er3xIdPoao44/3rywWp91fkg0+YkTxEcOHtZBZvYbTXruVArOUoUM7Xlfn2Pn+/WXE8Omn4Wc/g5/+FO65R56nDm4H1MLip556iiuvvJIHH3yQE088kYcffpg///nPFBUVMXz4cG655RYqKip4/PHHAYl3P+KII/jBD37ANddcw/vvv891113Hk08+ySWXXNKtz9SFxUoppZRS+y/Pk3NS9fVyHmtn3aIdPXfl/CKqf3I7x2/8F240TMzx4VhPjqhg8XCI4sNHDIslRDI+oqxMKuD/vj6Xi35VQDAo73/PPVJkJWLmP/wQmpoker62Fvr1g0GDICdHz3UdiHpSGxwwo4MAl19+OZs3b2bmzJlUVlZyxBFH8NJLLzF8+HAAKisrO+zUGjlyJC+99BI33ngjf/rTnxg8eDBz5szpdpGllFJKKaX2b44DI0bs2XOTjsnnz6c9SeqHVxBc9zIWgxNtkRFC4yNqffgJYePnt3xE8BFlfHgJqU9dzrPr7sF7cApbt8L778OAAXIebPlyKbJyctpi6BsaYMgQ2LABnntOFi3rGOHB6YDqaH0RtKOllFJKKXVwS3Siqt8s4mefX02fbSX4oy24sQgx48OJhZBVxsT/fweHGBaDh0ut05e/jbuTN0dOZfFnDn36SMds82bpYCWO/3ue3PelL0kS4ubNMHNm9wtF9cXrSW2g9bNSSimllDqkJcI1Wkbn8/eRt7E5YwQYg+NFIRahdbMxibCMGF7riS7o51XzveKfc93KG5mQUozfD1VVsGkThMNtnxMOSyx8INBx11d7mlZ48DigRgeVUkoppZTaGxKBGfPnT+bBD0Zy5pJZTKycTyDahLVSUFnAwZPRQjwsDhH8WCBAMydULcDLaOGh2A34soNs3izFVnq6fEZdnZzPysqSPyd2fSV0TitMTpYI+Ysv1rNcByIttJRSSimllEKKmXHjoKwsn/rauTS/fAr+h+6F0nUYG8WJB2M48bNaEfz4icR3bkFGy0ZO5BVCzRFujj5MIOCjtlZCMEIh6Wbl5MC2bVBRAcceK4Ec0JZWWFMDw4a1xdUXFko0vQZnHHh0dFAppZRSSqm4RGBGwVEOOTdPxZ33T6pyCvBw8XDwcFs7WT6i+IjGlxob3GiY9IYqzt32FPdVXcHwhiJaWmDdOgnFCIXg008lBKO8HAoK5PM8TzpZNTWSVpiZCa4rt4n0wuee0zHCA40WWkoppZRSSnWhuBju/b8CZgUfYZ07mhgujaQQxcXBi5/VMvE/e9h4LmGAFs6KvMhfY9/ka74XAIhE5D0DAVma3KcPvPiifEZZmYwLDhvWlk6YYIw8f9kyeO89Pbt1INHRQaWUUkoppTrpMMo3oYDXUu7ha6/9kKxoDQ4xXDxiOLh0DPB247mEPhthVHgFv3d+xC8CsHrEFI48Us5dZWXJc5ctk07VeefJmay0tK6vpbkZFi+G3/4WUlL07NaBQjtaSimllFJKtdPVKF/1xCm8cckDbO43jpjxYwB/ktO6A0tObUkiYRQfMVxaSKavV82vm37BwOqlJCdLJ8uYtk5VcbEkDyYny5mszqqrpZNVVyf7ucaNg+xsObs1Z468Xu2ftNBSSimllFKqnR2N8q0aP4UXLn+C5aPOJ+ImA1IxGWOwGAxefIjQwcUjxYRwiTE0vIZfl02l34alWCthGBs3yjhhc7MkD44fL+e22m+4tVYKqS1bYPRoKcz07NaBQwstpZRSSiml2qmv3/EoX/WAfJ677EnezL2cLUm5hEgiZg0GSwwfnpNEkom0nuGS2yjjwkuY+vKlpD09lzff8HjrLXj9dVi9WrpWF18snaplyySlMBqVwmvNGll6HAx2LPrad8TKyvbd70Z1nxZaSimllFJKtZORseNRPoB1FT7+n/8mPupzDg2pueD68PARxo/jhbHWysLj+GJjiYT3yK4v4QcrpnFn5VQmBIqIRqWr9c9/yvtefz0ccwxs3gwrV0oBlpkJJ50ksfCd7Wjpsdo/aBiGUkoppZRS7eTlyShfYaGM6LXvJHkefPIJkB5k9ak30OfTZI5e/DiBWF1rLIaHwcXD2sRiY7AYPFxSbQOnb5lHXn0Rjwy9jciXJlNdLSOAN98Mv/iFdKjq66Wz9dBDkJra9XU2Nm6/9FjtP7SjpZRSSimlVDuO0/UoX20tfPwxhMOybHhLbpCF58zm1bPvpSk1B9cBsDhYYlb+mm0Ax8ifovglr9CD4bE1TN92K/l2aYcRwNY9XgXSyQoGtz+7BfL9+vXyeGLpsdq/aKGllFJKKaVUJ8Hg9qN8mzfD2LEwalRbcWONw+IJ17BgysNs7TcaY1wJxnBdHEcKJ2MMMSeA3/XwGUuKaSbZhMndtoIpL0xlZOPSDiOAnie7soqK4IQToH//jgXftm2waJF02o4//ov6DaldMdZ2ro9Ve3V1dWRlZVFbW0tmZuYXfTlKKaWUUmof8ry2Ub6MDPn+9tul29X+r4bV1dCyaCnXL/keh4U/xzo+/IRwjQV/Ei0RQ8BraT2/5RkfjhfFOi41WaN54qh7uOTRKTQ3S7T88uVy/io5WcIwQNIHN22SxEKA3FyJfNedWvtOT2oDPaOllFJKKaXUDiRG+RI8r+P5LYCSEjm3FQoVUDngEf7f1isZGF2PjRr8NoyLIckLYa3FIGN/Bg+DBevRv3Yt3//0R2z9F8wpmiJLkodJ2EVjo4wO9u8PX/4yvPKKXNO4cZCeLo8XFspzrr9ei639iY4OKqWUUkop1U3tz2998AEsXAivvgrr1km3aYlXwPxj72Jbv5G4LjhelFg4gmNkiMxCWyKhcYlaHyFfKunN1aTd8QtSVi9tXZLceWfW009LoXf88ZCVpTu19ndaaCmllFJKKdUDwSCcfz5s3SoFVlMTBAJS/CQnw9ONk3nsy4+xbPwlhJLSMYm4dye+4Nh6eNbBAq5rSTUtGC9G1uY1/GjxVNzipWzb1haAYYwUVKtWyWe0T0FMPK47tfY/WmgppZRSSinVA54Hn30mgRinnSY7rkaPlpCMoUOl8HpzUz4vXDiXl796HxvTRmIdF8eAayyOz8X1G3yuxed4uF4UPA9jowypWcK3n7uUzH/O5d23Paqr5TN9Ptm55bpdX5Pu1Nr/aKGllFJKKaVUD5SVSVhFXp4UWenpUgAZ09Z9qqmBbXUO7xw2lT+d9k8iwQJwXYzjYHwujuPgGIMBvGiMmG1bbDwkUsK0ymlc9/FUNv27iOpqSRv0+yEW6/qadKfW/kcLLaWUUkoppXqgvl66R2lpMsqXnS3R64lRv6QkKYxaWmTXVeZJBSQ9/oi0vVwXUlLksBdgYzE8azB4OHiAxeKQ5jVwTuM8bl17NZlv/4vaWhgzBjZsgKoqOowW6k6t/ZMWWkoppZRSSvVARoZ0jxobpYM1fjykpsYj3lvkKxELn50NF10EzlEFcM89ckdTk8wBxmJg2v46nlhxHMFHDBesZWikhK8vn0luTREDB8KaNfDii5I++NZbsHat7Nhq/Rz92/1+Q/+jUEoppZRSqgfy8qS4Ki+XblJODkycCIMGSQ21YYMUYqec0ilyfcoUeOABOcwlm4zx4oeuDIAxWMdPEhFcYiTZFvxeiDGRYi5bfgehxignniidLWMkHOODD+RcmEa77390j5ZSSimllFI9kIh4Ly+XbtLQodC3LxQUwMqVUoRdey2ccUYXHaYpU2DkSPje9+Dzz8Hx4YVlp5ZnfDhEcUwMzzo4QMCEcGyUSdULyC6+gjdPu50RX8qntlY6Z4mu2bhxX8RvQu2MsTYx3am60pPtz0oppZRS6tBRXAzz50swRkuLdLGCQRnh22V3aelSuPJKIqXrCdWHcb0QDuASxWJwsNj4yS2HGAaI+FOpzgny1pduY9W4yYCcDdu8GWbO7LhYWe0dPakNtKOllFJKKaXUbggGpZNUViYBGRkZMlbYrXNSBQWU/eAuYrfdQQ4r8BHFADGceJFFvOCK4eGCcbDG0HfrGk5/41a29RlOdW4BaWlQUaGx7vsjPaOllFJKKaXUbnIc6SQVFMhtd8MoPA/+vnUy/+/ox1h62CVEfKmYeOKgdLLiSYSOIwe4DPgiLfiiYbI3r2DKC1PJrlrKhg3Q3CydLc/biz+o6jEttJRSSimllNoLPA9KS2VKsLS0YyFUVgYffgiLmvL5L/9cHuh7Kw1OJhYHA7hYLC64BsdYWWiMh+NFcWIRciuXcMHjl9LvubmsK/F46CEJNSwu/oJ+WLUdHR1USimllFKql3V1fmv8eAnRCAZhyRLJwkhKgj59HBaMu4mhqzZw0uYXyI5uJMlEiFkXBwuOh+N5WEzrF9aS21zKr0I3c+6Qz1jkXMcnhUHKyzWBcH+hhZZSSimllFK9qLgY5syBmhoYNkwWGzc2QmGhJBX++Mfw9tttS4+tBRyHl4ZdRwotHLN5IdmxjTSbFNKijbjWwwBR68cfC2OBKC4hJ50U08TR5QvITGohbeINvFUd5Lnn5OxYT3ZqJfZ+9fismdohLbSUUkoppZTqJZ4nnayaGjj8cNl3BZCZKd8vWwb33QeffgrhsIwUJidLwdWUHWTesBuojSRz7pZ/kEEDLjGMMcSMi8+LAGBxcB1IM004XoT0xipGrVlIxJdCyYm/p7jYoays+ymEu+q+qd2jhZZSSimllFK9pKxMCpZhw9qKrARjIDUVXn8dQiFZcFxdLQVXYi9WKCfIy2Y27/mP5NbAveQ2rwNP4t0dAzHrYLCAh7Hy5UbDZDRUcvTix6hNHcRj2TdRX9+9dtSuum86hrj7tCGolFJKKaVUL6mvbxsJ7MxaKcTCYelwpaXJsuPMTPD7pcCprARrHF4dOpW/XfhPqgYU4Dk+oo4fD4M1slXLWHBsDIsDxmCsRyBcx2nv3MUPP51Kv8qiXV5r5+5bZia4blv3raYGnntO0wx3lxZaSimllFJK9ZKMDBm9a2zc/rHaWqiqgqwsed7GjXIOatgwGDlSRv3S06XYGT4cyjMLuH3oI6z1jaPFC9Bsk/FsPO3dxrDWYIwnFZy1gMEfbWLi+nkMuvVq+Ne/dnqtu+q+DR0qHa+yst767RxatNBSSimllFKql+Tlyfmm8vJ47dNOSwts2SIdrfp62LZNCp2SEohEICWl7YxUZiZ89BG831DAwyPuojJlJI6x+IjElxjLX+OlvjI4NkYMHx4+fD5w1qyBW2+VbPkd2Fn3DeT+lhZdhry7tNBSSimllFKqlziOhEhkZ0vwRW0tRKNyu2yZFC6xGPTvD6NGSXerrg7WrpXzWrm5cPrpsGYNNDVBTg58OnAy9wYf46P+5xAhCQNgZGRQot49KbxcB9exOOFmbDgMK1bA1KmtxVbnvV5paTvuvoHcn5ws3TfVcxqGoZRSSimlVC8KBiVEIpHkV1EBgYAULQMGyHmspCS5L9E12rQJfD649FI44QR4913pVoVC8twVbj7X93+S30eu4NSGl/GMIYUWjOfhGR8x48ONhrBYPM9gmyL4iOIsWQKXX07Zj+7h77VTOiQLHnYY9Osn3bf2CYkgn71+PUyYIF061XNaaCmllFJKKdXLgkHZZZXYTVVbCw89JEmDRUXSvcrMlCLKGCmyACZNkq7WqFFSZG3ZIq/3+SB3iI8Pjr6d4FvryNxcgudanGiEcMyHLxYC6W8BYD0PiyUW9TBr1pL+8x9hjoLsSVNakwWXLJHzYK4r3bYhQ9q6b5s3S4F10UW6T2t3aaGllFJKKaXUXuA4bbusli6VwmncOAm8WL5cUv0SRdTQodLhys2VUb0BA2S8MNHVCgRkzNAz+fxf822c8c4dpIdWYKNRHGtxkANhFim3HGJEcYl5LpFYCmnhaqau+gXPHDWS6syCDnu9hg6VscL//EcKO5BO12GHfSG/toOGFlpKKaWUUkrtZe3TCHNy5AxXbW1bEQVS5GRktAVqFBZ2PdK3MGkyfHskl1fMwn1hPoFYE9YasOABDl5rWIbB4ou1YIjRd/MaprwwlRemzKU6t6A1WXDtWin+Bg2CI46APn2k+Fu/XnZs6S6t3aONQKWUUkoppfayzmmExkhBk5srnaqKCilm8vJ2HaiRnQ0nX5vP+9+by0Nj7mNzxkiscbGAg8XDBQwOFoMnZ7XwwEbJrVrCJf97KaPenEvtVo/kZOmurV0LY8fKyGL//nJNuktrzxhrOwdPqvbq6urIysqitraWzMzML/pylFJKKaXUAaq4WDpENTXSSUqclVq/Xoqnzp2j4uK2QI1EgEUwCAUF8Nln8PHH8MkncKRZyt0bv8eYls9w8Ijiw8SLLAO48Th42bQljzQ6GbyQ8S0eTbqORQ1BcnPlzFh2thSEOTlyDYnzWjNnto1BHsp6Uhvo6KBSSimllFL7QFdphMnJkux30UXbj+d1DtTIyJDC7I9/lGJt+HB5j/dWF3CV9wiPczkjWUsTKaTQgosXL7IMBg8wRHFpMekke02cVreAMC1s892A0ydIcjJUVkpxNXGiFFtpafIZukur57TQUkoppZRSah/pqnhKjAt2pX2ghufBPfdIkZUY62tokPtXJRdwS+Qe/mB/SDY1uMRaz2qZeC9L/gwptgmHCNm2ijPNQkI2hd9V/56Rox1yciQRcfly6W7pLq3dp2e0lFJKKaWU2ocSxVNBgdx2Nz69rEwKoGHD5Pvly+W815gx0LcvvOyfwg95gBJGxUcFDbH4ea0YTnx40MOJn9lKIsxAW8mV0cf4TvXvCDV7GCMjhDU1sG2bjDUmzo7tSueFyIf6uS7taCmllFJKKXUAqK+Xs1ppaTLeV1MjoRWJ1MKtW+HF8BTKzUj+Yr9HPp8TwYfBw8ZjMhzazmyBpBJmUM+08N2MWr6BV0dex/qMIM3NckZs9Oju7dLq6jzZ+PES6nGoJhZqR0sppZRSSqkDQPuI+FBIkgj9fulq1dTIc/x+WJNawH8lPcJKxhEmQAvJJNLvXGLxPVvSbvJwieAjQIizQgu4pPx+MtYXE43C0Ud3L9o9EfJRWCjjhuPGyW1hodxfXLzXfiX7NS20lFJKKaWUOgC0j4hPSpJdV5GIFF1NTdJ1SkuTYqy8TwH3pN9FVfJIXGPxE8GJd7IcA2CI4MdHGAeLjyg53kZOj7zCTyK/47KvRfnNb3ZdZHmedLIS58YyM8F1aV2IfCjHw2uhpZRSSiml1AGg/X6tigpZMrxtm3S4mpulwBoyRIqw5mZ4t89k7g0+xvt9ziFCEgbiI4QOMeOSRBiDjacSghsNk9FYxRmbnuKWpVfgW1G0y2tqf26s/WJloHUhcnGxPO9Qo4WWUkoppZRSB4hERPyECVJwhcOwZYuc08rJkeIrJUXGDFNT4dNQPjfmPMnbmZPxAqnYpADWgmM9MIYoSfiIxc9xWRzHkOSFyHrvZbj6avjXv3YactH+3FhX0tLk8UMxHl7DMJRSSimllDqAtI+IX7IE3n4bXntNFguDJBmOGyedrZYWKCvzUXL27Zz5wTooKcE2g4mEwXHxRcNYL36CyxhMwMXELNYYYqvW0PCTW/nLP4ezOFrA1q3SVRs/Hq66CvLzO54b62p/76EcD68dLaWUUkoppQ4wiYj4Cy+Ee++FWbPgxBNlVK+gAPr1k9G9zZtlsfGX/ysf57bbcEaMwDUWJxbF8aIYa3EAxxic5AAmFsWzEG1sIVQXJql0BWf8YypVry1l1SpYsQKefhq+9z341786nhuztuM1WtuzePiDjbG2869EtVdXV0dWVha1tbVkdlWmK6WUUkoptR/oKmI9GJR49tZQi6IiuP12eOklbCyGjcbkfr8PYy02FiMaMxjrEcWHSxQPl7WM5jeZ91A8Zgp+vxRQQ4bAI49I0TdnjgRfDB0q44KNjfKc7OzuJRceKHpSG2ihtQtaaCmllFJKqQOF58lIYX29jOvl5XWxAysaZetl18Ibr5PcUIPPixDDxXEsWA8nfghLlhzHsDh4uNSYHH6d/SdWB6fQ0gJVVXDZZfBf/yXnt955R+4LhXZQ5B0EelIb6BktpZRSSimlDhKJkcKdKV7l4+mkmzixr59jvIVkNm8k7KSQFGmUkIz4amOXGFFcoriESKG/reamzb/gji0jqehbgDGwYAGsWSNR84EADBwIp5wCRx21gyKvnW4VhQcwLbSUUkoppZQ6RHgePPssfFgXxJ5wA0nLkzl+5T9ICTdgrYwRevGFxrF4nIOLJUALDjGGe2v41Zqp3DZkLp/XFRAKyajgiBEyLlhaCg0NcNhhOy+auhpzHD9e4usPlg6Yjg7ugo4OKqWUUkqpA1n7ztGSJXDffVLcOA74XY9vhh/lysp76Vu3DocoBovFxQIGSyyxZ4sYFrD4WOeM4A/JP+c/I6/mjK869Okjn2UtLFsm8fM33STntDp3rIqL2850DRvWdqarvHz/P9OlZ7R6kRZaSimllFLqQFVUBI89Jp2jujrpOG3dKkmEfftCJAK1tXBYeCm3l3+P8eHPcJAgDAcbHyKUIivR4TIYPAyNJoN/D/kWGyZfx9pAkFBIxgcBSkqkq7VxY8eO1YUXwvPPQ2EhHH54xyXH7Yu0m2/eP8cI9YyWUkoppZRSh7h//QtmzpRiJympbbmw58GmTVL8pKXJouPPygv4ScojPMblDA6vpYUUkmnBxcMl1jpOaDHEcGkgnTTbxClbFrDo+RbeyLiBVb5g61mtykpoboYjjmjrWBUWSuHX0ACjR3csskC+HzpUOl5lZbs+a7a/2w/rRKWUUkoppdSeKCqCO++EDRtg0CDpXsViUswYI4VPdbV0kYyBrCz43BTwm8x7qPVnk0oTPiIdOllgAAfHQFr88YymKo7atJDLtz1Edj+PlBQJx9i6FXJzZYmx68rt4YfLZ65dC6mpXV93WpoUg/X1++o3tfdooaWUUkoppdRBxPNkXLCqSjpEyclyH8hZKV98pm3rVhkn9Dzw++X21eQpPPvVB6jLHoVxHMCAceWUlnHwuRbX9XCJYvDwEyY7Vsm5mx7jayW/A88jGpWu1oYNbUuMrZURxbQ0ud2woetrb2yU683I2Nu/pb3vgCm0tm7dypVXXklWVhZZWVlceeWVbNu2baevueqqqzDGdPiaNGnSvrlgpZRSSimlvgBlZXImKxCQkUGQ4spxpJhKS5P7fT4pbDZvlg7S4MESy/7vtCk8cs5TVA04Es/1EzN+MAbjOkStIRYlPk7oYDEYLOnU892Ku/n+shsZb4sZPFjCLmprpYv1zjvw73/LWGBdHbz1lowvtmethGcEgxKccaA7YAqtb37zmyxevJiXX36Zl19+mcWLF3PllVfu8nXnnHMOlZWVrV8vvfTSPrhapZRSSimlvhj19VJQJSdL2AVI0ZWaKsuEHUfGBfv1k51Xp54KI0fCt78Nv/0tHHMMLDMF3F/wCBsyxmEDAWxyMrEYWI8OZ7YMEMUlgg8/Ib7avIAbzP0cboqJRqWr9uGHcmYrNVVGCPv1g3AYXn5ZxgijUSnIli2T1MGLLto/gzB66oAIwyguLubll1/mgw8+YOLEiQD8+c9/5sQTT2TFihWMGzduh68NBAIMHDhwX12qUkoppZRSX6iMDClmtm2T8cCcHCmssrOl0GpokAIsO1sKn4oKSSH82tekmxQMJuLgC0j56C5SHryD8JLl+KxUbR4OJh7+HsFPgDBgcImS7W3ky82v4JZGWD7sYcrKfDQ1yTWAdLdGjpREwrffhqVLJTQjJUXSBi+6aP+Ndu+pA6LQev/998nKymotsgAmTZpEVlYW77333k4LrTfffJMBAwbQp08fvvzlL/Pf//3fDBgwYIfPD4VChEKh1u/r6up654dQSimllFJqH8jLk2Jl0yYprKqrpaBKSZHiat066XBlZcGWLdsXOI7TLvGvYDIVw0ZSfs3tHLP+X7heGBsvtKIYkggDEMXBxZJEmP7RKs6vf4q0dfX8T87txAbkEwrJyGBqqsS85+TAV78qBd3UqXJfYs/WweKAKLSqqqq6LI4GDBhAVVXVDl937rnn8vWvf53hw4dTUlLCjBkzOOOMM/jkk08IJEL+O7n77ru54447eu3alVJKKaWU2pccBy6+WBYAgxRVdXVSVIVCMG4c/OAHcMIJHRcJ78iWQfncf/yT3OS7gnElL+NhSLYS/W7jXS0puCwWg8UQIMTpzS8zqGIdD0du4/3+kxk4EIYMkfTDsjJJIwyFJBXxQI9y78oXWmjdfvvtuyxqFi1aBIDpHLQPWGu7vD/h8ssvb/3zEUccwXHHHcfw4cN58cUX+drXvtbla2655RZ+9rOftX5fV1fHsGHDdnqNSimllFJK7U+CQbj+epg/XwIotmyRYioYhO9+F/Lzu/9eGRmQlOrj1ZNvJ1C1jtymEgD8hInithZZwuBZhxguHoZhkRJ+uHkmvrEjWUY+H30k44yRiFxPair8/e8SznGwjAwmfKGF1o9//GO+8Y1v7PQ5I0aM4LPPPmPjxo3bPVZdXU1ubm63P2/QoEEMHz6cVatW7fA5gUBgh90upZRSSimlDhTBoHSv5LxV97pX7XmevLa2VnZi/XtJPmX9b2Nq8x2MtcvxEW3tYVnAwRDFj58wxhgc24I1MKKlmDPevYMFOU/QEvW1BnU0NEis/JIlMGeOFIaJYivx2btz3fuLL7TQys7OJjs7e5fPO/HEE6mtreWjjz7ihBNOAODDDz+ktraWk046qduft3nzZsrLyxk0aNBuX7NSSimllFIHig7nrXqguFi6YcuXywLhlhZJBVwSmUzl6JFMrbid05tfwiEWj3g3RHHiRRbEjINjPQKEMEQ5J7IAu+kK7ku9nXXp+YTDkJ4uHa1oVM6RPfccjB0Lb74JCxbI6KPrytmy8eNlHPJA6noZaxNrxPZv5557Lhs2bOChhx4C4Nprr2X48OEsWLCg9Tnjx4/n7rvv5uKLL6ahoYHbb7+dSy65hEGDBlFaWsovf/lLysrK+P/t3XtwVPX9//HXObvZ3EiWW4RgEgLKJRhALoMFRZQKVCoVq5XRVsBLa0YtIuPtW4tAy6+OMPr1ixeqLYrTIjKoOHZUlFJB8VIkBkshXOSOBDFYQgiXbPZ8fn98SDRAgY2bvejzMZPRc/bs7nuZz4R98T7nfcrLy5V1hndBO3DggILBoKqqqpSdnd0snw0AAABIFOXltsNUWSnl59v7bu3eLb35pu00BYNSilOnqRW/0sAjS9U6XCl/uFaeJJ8jhZ0U+UytZCTjuHJMWJJ0SBna4BbpsawH9UGrK9S2rQ1Shw9L/fvb92vTxt5z6/BhG8Tat7fdrEOH7CCPb3a94iGSbJA0Dbh58+apZ8+eGj58uIYPH65evXrpL3/5S6NjNmzYoKqqKkmSz+fTmjVrdOWVV6pr164aN26cunbtqg8//PCMQxYAAADwfeJ5tpNVWSn16GGnFfp89hqqli2lVq1s4LnoEr82XXmPdp/3Ix1q0U6e65cnn46agBwvJBl7UqFjwgrLp5BS5MnROdqsB2t/q4tbr2m4cXJdnR3WsWaNvZGxZLtwLVtKX3whrV1rpxRWVtqul+fF788nEkkxdVCSWrdurb/+9a+nPOabzbn09HS99dZbzV0WAAAA8J2xY4c9XTA/3957q15qqr2eynXttVVpadLR9kX6Z9ZEhVLS1POT55XuVSskI0dquKGxJ7dhTEaajijkGXU8skH3bbxZM7rN0Xp/T/l80tatdgJhdrYNWD6f/cnJsacVbtgg9expu207diTHlMKk6WgBAAAAaF7V1fZ6rMzMxvuDQdvJOnLETgysv+1sZU6R3hr+v3q61f/ooLJkjgUrV0aefJIcuTJy5cmVJ7/q5DMhnXPwU01de41+uH2OWmR4+vLLr2+snJLy9fs6jg1flZW283XkiK0xGRC0AAAAAEiyE/7S0qSamsb7HccOpEhJsY/V1trgU1UlfVzq6k/Be7S80zh9FWivkFJk5KpOPoXlKnysuyU58o7dZ8uRUfsj23T33vs0Yftd6uaV67zz7OuHQo3fu/70wqoqW1uyXAVE0AIAAAAgyQ6e6N7dTvw7fmRe27b25sLdu9ugtXGjtG+fnRTY6RxX20eUaOs5w7U/0F4hBVSjTBk58sleVFX3jRsb18lVjbKUqkO6rOZvmuQ+pmJ/udq2tYHqm+9dW2tPI9y3zw7CKCiI3Z/Ht5E012gBAAAAaF6ua8eo79xpx7nn5dnTCGtqpF27pM6dpTvusPvq73HledLUqdK29CK5l06Ur0WaMj6Zpxbhg/IpfGz0u08Bx7aqjHHlc6WW/kNywiGl1+3RgANLFHo/XbuGPaqqKldffmlPGUxJsQErNdUGrNGjk+d+WgQtAAAAAA2KiuwY9fr7aH3+uT1lr29fG3SOH6/uebbLVVYmZfUo0sKB/6us3b10U+UM5R7dLiksRzo2JMOV6xh5xpPjeXKNJ9XWKhiu0NCaudq/Ilc1fe7R9p2u9uyxYS4jQxo2TPrVr5LrPloELQAAAACNFBVJ3brZCX/1nauCgpN3k47vgqWluVqcdbM2Bgfofz67Sd1q/y3P75cX9mQcOybDMZLCYclnX9DnGKWHqnXNpoeUE9qtxR1L1KqoSPn50qhR0tChydPJqpc0NyyOF25YDAAAAJxeebntgq1aJZWW2k7UoKw1mrrlBrWq2SWfVytfXa0c48k1YbmuHYoh122YeGFcV6G2uaq+YJiOlExU7tCihApYkWQDOloAAAAAvrX6Lti2bdL//Z+0aZN0dv+eenfTdF28fJpyKtfL5x27TkuubFvr2Dz32lrJceTU1Snw1Rdqs+ot6YWQdMkzkpuckSWB8iEAAACAZOa6dmBGSYnUsaPtcq1qf4UWjZqrdQU/UsgJ2ONSXDmua8cJ1tbaMYP1d0gOhaS9e6WXXrIXZpWXx/ETNR1BCwAAAEBU1Q/U6NPHTg1c8Z/zNOsH87XlvCukjAy5aan2QM+zASsQsNdsmWOnEmZm2ptnLV0qPfZYUoat5OzDAQAAAEhoJw7U8Kugeqrcm7dLW7fag47dJMvU1spIkuNKRnIOHpRTVydVVkpLlkjp6dKjjybVRIzkqRQAAABAUnFdqbBQ6tnT/tfteZ704IN2wxiprk5eqE5eWKoLuwqHjepCnsJHQ/KMbBCrqJDmzpVmzrQdsCRB0AIAAAAQO1dcYYPTj36kcEqqQmFHYSObyhxHPuNJRgp5rg1bxtiW2EMPSXfdlTSnERK0AAAAAMSUV3Setvy/+VrZaYz+k9JOXkqqXBOWYzwZx5HnC8hnwvI8yfj8djJhKCQtXmxHGiZB2OIaLQAAAAAx8/X9tvyq2n+PfpmaoiFH3lJrZ48cx5EnV36vVnKkkJMq1/PkpKXZjpfn2Yu+Xn3VXgCWwNdsJW5lAAAAAL5TysulWbOksjKpRQupomWRFnWcqPfThipsfHLDIbmmTnIchd0UOfJkHJ+Umir5j/WI2rSxL7RjR3w/zGkQtAAAAAA0O8+znazKSqlHD6l1a3tG4I7MIj3Z6xktazFSR910hX2pkhxJjsK+gExWln1yaqqdPhgMSkeO2Ou2EhhBCwAAAECz27FDWr9eys+3t84KBqW2baWqKslz/fpL4VRtCRQp5KYqlJKuQ/5shdNayOeFbDcrEJBycuz/p6VJWVnx/kinRNACAAAA0Oyqq20jKjPTbjuO1L27lJEhffmltCX9PD3Z5kHtzSxUOCylhQ8qw3fUXp+VkSG1amWvy/r8c3uTroKC+H6g0yBoAQAAAGh2WVm2EVVT8/W+nBzpgguk3FzpwAHpTd8VmtVnrj7r/VP5ctrIn+a3yaygwJ5v+OWXtg02enRCD8KQmDoIAAAAIAYKCmwHq6zMZibHsftzcux8i1WrpC5dpDvvPE+FBX+Wu+wf0t/+Ju3cKfl89uC+fW3IKiqK2+c4UwQtAAAAAM3OdaWrrrK5ad06KS/PNqtqaqRdu6SOHaWSEqlzZ0lypcsuk4YOtRd3VVfbllhBQcJ3suoRtAAAAADERFGRNGGCnT64fr293Cot7RSNKteVCgvjUOm3R9ACAAAAEDNFRXamRZI2qs4YQQsAAABATCVxo+qMfcdyIwAAAADEHx0tAAAAAAnJ85L3FEOCFgAAAIC4OT5M5eXZKYSffiqtWCHt2SMdPWqHZnTvbicXJsF0d4IWAAAAgPgoL/96AuGRIzZQHT4s1dVJ27fb/+blST17ShkZ9h5cO3fayYWJHraSpPEGAAAA4LukvFyaNcuGp7Zt7U2Lt26V1q6199kyRsrNlf7zH3sz46NH7Y2OKyulV1+1nbBERtACAAAAEFOeZztZlZU2PGVlSRs32g7W2WfbrlYoJKWmSjk50qFDtusl2Q5Xebk93TCREbQAAAAAxNSOHTY45edLjiNVVdnQFQzaEObz2Q7W0aP28exs+3hVlZSZaU8zrK6O96c4Na7RAgAAABBT1dU2LGVm2u2jR203KyXFnjLo99vtujr7eCBgn1MfvNLSbBcskdHRAgAAABBTWVk2LNXU2O3UVBuu6k8XTE2VwmHb2ZKk2lr7eCBgJxIWFdlR74mMoAUAAAAgpgoK7Kj2nTttBysYtAMxqqrsdiAgtWhhu1iHD0v799vtzz+3x40enfj300rw8gAAAAB817iuvR9W27Z2wuCBA1KXLrZrtWWLPaXwooukVq3sfbRCIXtsv37JMdpdkhxjjIl3EYnswIEDCgaDqqqqUnZ2drzLAQAAAL4zTnYfrSNH7GmF9acQ5uZKF14o9e5tO2Hx7GRFkg0YhgEAAAAgLoqKpG7d7BTC6mp77VZenr0Oq3473uGqqQhaAAAAAOLGdaXCwsb7jt9ORkmYDQEAAAAgsRG0AAAAACDKCFoAAAAAEGUELQAAAACIMoIWAAAAAEQZQQsAAAAAooygBQAAAABRRtACAAAAgCgjaAEAAABAlBG0AAAAACDKCFoAAAAAEGUELQAAAACIMoIWAAAAAESZP94FJDpjjCTpwIEDca4EAAAAQDzVZ4L6jHAqBK3TqK6uliTl5+fHuRIAAAAAiaC6ulrBYPCUxzjmTOLY95jnedq9e7eysrLkOE5cazlw4IDy8/O1c+dOZWdnx7UWJAfWDCLFmkGkWDOIFGsGkUqkNWOMUXV1tTp06CDXPfVVWHS0TsN1XeXl5cW7jEays7PjvsiQXFgziBRrBpFizSBSrBlEKlHWzOk6WfUYhgEAAAAAUUbQAgAAAIAoI2glkdTUVE2ZMkWpqanxLgVJgjWDSLFmECnWDCLFmkGkknXNMAwDAAAAAKKMjhYAAAAARBlBCwAAAACijKAFAAAAAFFG0AIAAACAKCNoJZinnnpKnTp1Ulpamvr166f33nvvlMcvX75c/fr1U1pamjp37qw//vGPMaoUiSKSNfPKK69o2LBhysnJUXZ2tgYOHKi33norhtUiEUT6e6be+++/L7/fr/PPP795C0TCiXTNHD16VA888IA6duyo1NRUnXPOOXr22WdjVC0SQaRrZt68eerdu7cyMjKUm5urG2+8Ufv27YtRtYind999V6NGjVKHDh3kOI5effXV0z4nWb7/ErQSyIIFCzRx4kQ98MADKisr0+DBg3X55Zdrx44dJz1+69atGjlypAYPHqyysjL95je/0YQJE/Tyyy/HuHLES6Rr5t1339WwYcP0xhtvqLS0VJdeeqlGjRqlsrKyGFeOeIl0zdSrqqrS2LFj9cMf/jBGlSJRNGXNXHvttVq6dKnmzJmjDRs2aP78+erevXsMq0Y8RbpmVqxYobFjx+rmm2/W2rVrtXDhQn388ce65ZZbYlw54qGmpka9e/fWE088cUbHJ9X3X4OEMWDAAFNSUtJoX/fu3c39999/0uPvvfde071790b7br31VvODH/yg2WpEYol0zZxMjx49zLRp06JdGhJUU9fMmDFjzG9/+1szZcoU07t372asEIkm0jXz5ptvmmAwaPbt2xeL8pCAIl0zM2fONJ07d260b9asWSYvL6/ZakRikmQWLVp0ymOS6fsvHa0EUVtbq9LSUg0fPrzR/uHDh+uDDz446XM+/PDDE44fMWKEVq1apVAo1Gy1IjE0Zc0cz/M8VVdXq3Xr1s1RIhJMU9fMc889p82bN2vKlCnNXSISTFPWzGuvvab+/ftrxowZOvvss9W1a1fdfffdOnz4cCxKRpw1Zc0MGjRIu3bt0htvvCFjjL744gu99NJL+vGPfxyLkpFkkun7rz/eBcCqrKxUOBxWu3btGu1v166d9uzZc9Ln7Nmz56TH19XVqbKyUrm5uc1WL+KvKWvmeI888ohqamp07bXXNkeJSDBNWTObNm3S/fffr/fee09+P39lfN80Zc1s2bJFK1asUFpamhYtWqTKykrddttt+uqrr7hO63ugKWtm0KBBmjdvnsaMGaMjR46orq5OP/nJT/T444/HomQkmWT6/ktHK8E4jtNo2xhzwr7THX+y/fjuinTN1Js/f76mTp2qBQsW6Kyzzmqu8pCAznTNhMNhXX/99Zo2bZq6du0aq/KQgCL5PeN5nhzH0bx58zRgwACNHDlSjz76qObOnUtX63skkjWzbt06TZgwQQ8++KBKS0u1ePFibd26VSUlJbEoFUkoWb7/8s+TCaJt27by+Xwn/GvP3r17T0jt9dq3b3/S4/1+v9q0adNstSIxNGXN1FuwYIFuvvlmLVy4UJdddllzlokEEumaqa6u1qpVq1RWVqY77rhDkv0SbYyR3+/X22+/raFDh8akdsRHU37P5Obm6uyzz1YwGGzYV1RUJGOMdu3apS5dujRrzYivpqyZhx56SBdeeKHuueceSVKvXr2UmZmpwYMHa/r06QnVoUD8JdP3XzpaCSIQCKhfv35asmRJo/1LlizRoEGDTvqcgQMHnnD822+/rf79+yslJaXZakViaMqakWwna/z48XrhhRc4//17JtI1k52drTVr1mj16tUNPyUlJerWrZtWr16tCy64IFalI06a8nvmwgsv1O7du3Xw4MGGfRs3bpTrusrLy2vWehF/TVkzhw4dkus2/krq8/kkfd2pAOol1fffOA3hwEm8+OKLJiUlxcyZM8esW7fOTJw40WRmZppt27YZY4y5//77zQ033NBw/JYtW0xGRoa56667zLp168ycOXNMSkqKeemll+L1ERBjka6ZF154wfj9fvPkk0+aioqKhp/9+/fH6yMgxiJdM8dj6uD3T6Rrprq62uTl5ZlrrrnGrF271ixfvtx06dLF3HLLLfH6CIixSNfMc889Z/x+v3nqqafM5s2bzYoVK0z//v3NgAED4vUREEPV1dWmrKzMlJWVGUnm0UcfNWVlZWb79u3GmOT+/kvQSjBPPvmk6dixowkEAqZv375m+fLlDY+NGzfODBkypNHxy5YtM3369DGBQMAUFhaa2bNnx7hixFska2bIkCFG0gk/48aNi33hiJtIf898E0Hr+ynSNVNeXm4uu+wyk56ebvLy8sykSZPMoUOHYlw14inSNTNr1izTo0cPk56ebnJzc83Pf/5zs2vXrhhXjXh45513TvndJJm//zrG0JMFAAAAgGjiGi0AAAAAiDKCFgAAAABEGUELAAAAAKKMoAUAAAAAUUbQAgAAAIAoI2gBAAAAQJQRtAAAAAAgyghaAAAAABBlBC0AAAAAiDKCFgAg5saPH6/Ro0fH/H3nzp2rli1bntFxjuOc8PPnP/85KnVs27ZNjuNo9erVUXk9AEDi8ce7AAAAElF2drY2bNjQaF8wGIxTNf9dbW2tAoFAvMsAAByHjhYAIO4uueQSTZgwQffee69at26t9u3ba+rUqY2OcRxHs2fP1uWXX6709HR16tRJCxcubHh82bJlchxH+/fvb9i3evVqOY6jbdu2admyZbrxxhtVVVXV0KE6/j2Of7/27ds3+klPT5ckrVu3TiNHjlSLFi3Url073XDDDaqsrGx47uLFi3XRRRepZcuWatOmja644gpt3ry54fFOnTpJkvr06SPHcXTJJZc0/DlMnDixUR2jR4/W+PHjG7YLCws1ffp0jR8/XsFgUL/85S8lSR988IEuvvhipaenKz8/XxMmTFBNTc3p/ugBAM2EoAUASAjPP/+8MjMz9c9//lMzZszQ7373Oy1ZsqTRMZMnT9bVV1+tTz/9VL/4xS903XXXqby8/Ixef9CgQXrssceUnZ2tiooKVVRU6O677464zoqKCg0ZMkTnn3++Vq1apcWLF+uLL77Qtdde23BMTU2NJk2apI8//lhLly6V67q66qqr5HmeJGnlypWSpL///e+qqKjQK6+8ElENM2fOVHFxsUpLSzV58mStWbNGI0aM0E9/+lP961//0oIFC7RixQrdcccdEX8+AEB0cOogACAh9OrVS1OmTJEkdenSRU888YSWLl2qYcOGNRzzs5/9TLfccosk6fe//72WLFmixx9/XE899dRpXz8QCCgYDDZ0qk6nqqpKLVq0aNhu0aKF9uzZo9mzZ6tv3776wx/+0PDYs88+q/z8fG3cuFFdu3bV1Vdf3ei15syZo7POOkvr1q1TcXGxcnJyJElt2rQ5o1qON3To0EYhcezYsbr++usbumFdunTRrFmzNGTIEM2ePVtpaWkRvwcA4NshaAEAEkKvXr0abefm5mrv3r2N9g0cOPCE7eYaKJGVlaVPPvmkYdt17UkgpaWleueddxqFsHqbN29W165dtXnzZk2ePFkfffSRKisrGzpZO3bsUHFx8beurX///o22S0tL9dlnn2nevHkN+4wx8jxPW7duVVFR0bd+TwBAZAhaAICEkJKS0mjbcZyGgHIqjuNI+joIGWMaHguFQk2ux3VdnXvuuSfs9zxPo0aN0sMPP3zCY7m5uZKkUaNGKT8/X3/605/UoUMHeZ6n4uJi1dbWnvY9v1n/f/sMmZmZJ9R06623asKECSccW1BQcMr3BAA0D4IWACBpfPTRRxo7dmyj7T59+khSw+l4FRUVatWqlSSd0O0KBAIKh8Pfqoa+ffvq5ZdfVmFhofz+E/8a3bdvn8rLy/X0009r8ODBkqQVK1acUIekE2rJyclRRUVFw3Y4HNa///1vXXrppaetae3atScNhgCA+GAYBgAgaSxcuFDPPvusNm7cqClTpmjlypUNAx/OPfdc5efna+rUqdq4caNef/11PfLII42eX1hYqIMHD2rp0qWqrKzUoUOHIq7h9ttv11dffaXrrrtOK1eu1JYtW/T222/rpptuUjgcVqtWrdSmTRs988wz+uyzz/SPf/xDkyZNavQaZ511ltLT0xsGaVRVVUmy1169/vrrev3117V+/XrddtttjaYo/jf33XefPvzwQ91+++1avXq1Nm3apNdee02//vWvI/58AIDoIGgBAJLGtGnT9OKLL6pXr156/vnnNW/ePPXo0UOSPfVw/vz5Wr9+vXr37q2HH35Y06dPb/T8QYMGqaSkRGPGjFFOTo5mzJgRcQ0dOnTQ+++/r3A4rBEjRqi4uFh33nmngsGgXNeV67p68cUXVVpaquLiYt11112aOXNmo9fw+/2aNWuWnn76aXXo0EFXXnmlJOmmm27SuHHjNHbsWA0ZMkSdOnU6bTdLste3LV++XJs2bdLgwYPVp08fTZ48ueFURgBA7Dnm+JPBAQBIQI7jaNGiRRo9enS8SwEA4LToaAEAAABAlBG0AAAAACDKmDoIAEgKnOkOAEgmdLQAAAAAIMoIWgAAAAAQZQQtAAAAAIgyghYAAAAARBlBCwAAAACijKAFAAAAAFFG0AIAAACAKCNoAQAAAECU/X/SP38to/bIcgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming the model and x_test are already defined and the model is trained\n",
    "\n",
    "# Generate new data points for a smooth curve\n",
    "x_range = np.linspace(0, 1, 500)  # Generate 500 points between 0 and 1\n",
    "x_range = np.column_stack([x_range, x_range**2, x_range**3, np.sin(x_range), np.cos(x_range)])  # Example transformations\n",
    "\n",
    "# True function (assuming you know the true relationship)\n",
    "def true_function(x):\n",
    "    return 2*x[:, 0] - 3*x[:, 1] + 0.5*x[:, 2] - 1.5*x[:, 3] + 2*x[:, 4] + np.random.normal(0, 0.1, x.shape[0])\n",
    "\n",
    "# Generate true values\n",
    "y_true = true_function(x_range)\n",
    "\n",
    "# Predict using the neural network\n",
    "y_pred = model_pruned.predict(x_range)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_range[:, 0], y_true, color='blue', alpha=0.5, label='Actual Function')\n",
    "plt.scatter(x_range[:, 0], y_pred, color='red', alpha=0.5, label='NN Predicted Function')\n",
    "plt.title('Comparison of Actual Function vs Neural Network Function')\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Output Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUNING WITH ONLY TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 10)                60        \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181\n",
      "Trainable params: 181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/32 [..............................] - ETA: 27s - loss: 2.0171WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_end` time: 0.0046s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_train_batch_end` time: 0.0046s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 5ms/step - loss: 1.5811 - val_loss: 1.4159\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3369 - val_loss: 1.2593\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1920 - val_loss: 1.0965\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0455 - val_loss: 0.9558\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8963 - val_loss: 0.8055\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7472 - val_loss: 0.6564\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 0.5125\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4554 - val_loss: 0.3843\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3335 - val_loss: 0.2767\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2274 - val_loss: 0.1859\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1477 - val_loss: 0.1283\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1012 - val_loss: 0.0904\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0645 - val_loss: 0.0531\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0470 - val_loss: 0.0444\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0383\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.0316\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.0293\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0262\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0241\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0206\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0190\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0250\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0203\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.0177\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.0236\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0166\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.0147\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.0141\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.0144\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.0130\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.0124\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0511 - val_loss: 0.0392\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0195\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.0142\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.0296\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0238\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.0201\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.0194\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.0165\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.0444\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0186\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0150\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0161 - val_loss: 0.0140\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0135\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0133\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0184 - val_loss: 0.0217\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.0157\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.0135\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0131\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 10 but corresponding boolean dimension is 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, layers\u001b[38;5;241m.\u001b[39mDense):\n\u001b[0;32m     63\u001b[0m     weights, biases \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[1;32m---> 64\u001b[0m     weights, biases \u001b[38;5;241m=\u001b[39m \u001b[43mremove_zero_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     new_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mactivation, input_shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[0;32m     67\u001b[0m     new_model\u001b[38;5;241m.\u001b[39madd(new_layer)\n",
      "Cell \u001b[1;32mIn[67], line 55\u001b[0m, in \u001b[0;36mremove_zero_rows\u001b[1;34m(weights, biases)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_zero_rows\u001b[39m(weights, biases):\n\u001b[0;32m     54\u001b[0m     non_zero_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39mall(weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights[non_zero_rows], \u001b[43mbiases\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_zero_rows\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 10 but corresponding boolean dimension is 5"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Create a simple neural network model for regression\n",
    "model = models.Sequential([\n",
    "    layers.Dense(10, activation='relu', input_shape=(5,)),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Generate some synthetic data for training\n",
    "np.random.seed(0)\n",
    "x_train = np.random.rand(1000, 5)\n",
    "y_train = 2*x_train[:, 0] - 3*x_train[:, 1] + 0.5*x_train[:, 2] - 1.5*x_train[:, 3] + 2*x_train[:, 4] + np.random.normal(0, 0.1, 1000)\n",
    "\n",
    "x_test = np.random.rand(200, 5)\n",
    "y_test = 2*x_test[:, 0] - 3*x_test[:, 1] + 0.5*x_test[:, 2] - 1.5*x_test[:, 3] + 2*x_test[:, 4] + np.random.normal(0, 0.1, 200)\n",
    "\n",
    "# Apply pruning to the model\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=0.5,\n",
    "        begin_step=0,\n",
    "        end_step=2000\n",
    "    )\n",
    "}\n",
    "\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "pruned_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define the callbacks for pruning\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='./logs')  # Optional, for logging\n",
    "]\n",
    "\n",
    "# Train the pruned model\n",
    "pruned_model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "\n",
    "# Strip the pruning wrappers\n",
    "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Function to remove zero rows from dense layer weights\n",
    "def remove_zero_rows(weights, biases):\n",
    "    non_zero_rows = ~np.all(weights == 0, axis=1)\n",
    "    return weights[non_zero_rows], biases[non_zero_rows]\n",
    "\n",
    "# Rebuild the model with modified weights\n",
    "new_model = models.Sequential()\n",
    "input_shape = (5,)  # Initial input shape\n",
    "\n",
    "for layer in pruned_model.layers:\n",
    "    if isinstance(layer, layers.Dense):\n",
    "        weights, biases = layer.get_weights()\n",
    "        weights, biases = remove_zero_rows(weights, biases)\n",
    "        \n",
    "        new_layer = layers.Dense(weights.shape[1], activation=layer.activation, input_shape=input_shape)\n",
    "        new_model.add(new_layer)\n",
    "        new_layer.set_weights([weights, biases])\n",
    "        \n",
    "        input_shape = (weights.shape[1],)  # Update input shape for the next layer\n",
    "    else:\n",
    "        new_model.add(layer)\n",
    "\n",
    "new_model.summary()\n",
    "\n",
    "# Compile the new model\n",
    "new_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Convert the optimized model to TensorFlow Lite with sparse tensor support\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Enable sparse tensor support\n",
    "converter._experimental_sparsify_weights = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('optimized_pruned_regression.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "with open('optimized_pruned_regression.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, 'optimized_pruned_regression'))\n",
    "\n",
    "print(\"Model pruning and conversion to TFLite with sparse tensor support completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
